{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from train import save_training_results_to_df, train_epoch, evaluate, translate, my_check_disk_space, save_model_locally\n",
    "# from preprocess import get_dataloaders2, split_parallel_corpus_chunked, TranslationIterableDataset, get_dataloader_from_files, train_tokenizer\n",
    "from preprocess import load_tokenizer, get_dataloader_from_files\n",
    "from test_model import load_tokenizer, test_model, translate_batch, calculate_bleu, save_results_to_df, translate\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models.signature import infer_signature\n",
    "# from model import Seq2SeqTransformer\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import nltk\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del batch, output, loss\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class Seq2SeqTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "#                  num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_encoder_layers,\n",
    "#             num_decoder_layers=num_decoder_layers,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        \n",
    "#         self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "    \n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "#         # src: [batch_size, src_len]\n",
    "#         # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "#         # Create target mask for decoder\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "#         # Embedding and positional encoding\n",
    "#         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "        \n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "#         # print(\"tgt max index:\", tgt.max())\n",
    "#         print(\"tgt shape:\", tgt.shape)\n",
    "#         print(\"tgt max index:\", tgt.max())  # before embedding()\n",
    "#         print(\"embedding vocab size:\", self.embedding.num_embeddings)\n",
    "\n",
    "        \n",
    "#         # Transformer forward pass\n",
    "#         # output = self.transformer(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "#         #new approach ignores padded areas that confused prediction before\n",
    "#         src_key_padding_mask = (src == self.embedding.padding_idx)\n",
    "#         tgt_key_padding_mask = (tgt == self.embedding.padding_idx)\n",
    "\n",
    "#         output = self.transformer(\n",
    "#             src, tgt,\n",
    "#             src_mask=src_mask,\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             src_key_padding_mask=src_key_padding_mask,\n",
    "#             tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "#             memory_key_padding_mask=src_key_padding_mask\n",
    "#         )\n",
    "\n",
    "        \n",
    "#         # Project to vocabulary size\n",
    "#         output = self.output_layer(output)\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     def encode(self, src):\n",
    "#         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         return self.transformer.encoder(src)\n",
    "    \n",
    "#     def decode(self, tgt, memory, tgt_mask=None):\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "#         output = self.transformer.decoder(tgt, memory, tgt_mask)\n",
    "#         output = self.output_layer(output)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class Seq2SeqTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "#                  num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_encoder_layers,\n",
    "#             num_decoder_layers=num_decoder_layers,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        \n",
    "#         self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "    \n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "#         # src: [batch_size, src_len]\n",
    "#         # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "#         # Create target mask for decoder\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "#         # Create source padding mask\n",
    "#         src_padding_mask = (src == 0).to(src.device)  # True for padding tokens\n",
    "#         tgt_padding_mask = (tgt == 0).to(tgt.device)  # True for padding tokens\n",
    "        \n",
    "#         # Embedding and positional encoding\n",
    "#         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "        \n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "#         print(\"tgt max index:\", tgt.max())\n",
    "#         print(\"tgt shape:\", tgt.shape)\n",
    "#         print(\"embedding vocab size:\", self.embedding.num_embeddings)\n",
    "\n",
    "        \n",
    "#         # Transformer forward pass\n",
    "#         output = self.transformer(\n",
    "#             src, \n",
    "#             tgt,\n",
    "#             src_mask=None,  # No source mask needed\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             src_key_padding_mask=src_padding_mask,\n",
    "#             tgt_key_padding_mask=tgt_padding_mask,\n",
    "#             memory_key_padding_mask=src_padding_mask\n",
    "#         )\n",
    "        \n",
    "#         # Project to vocabulary size\n",
    "#         output = self.output_layer(output)\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     def encode(self, src):\n",
    "#         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         return self.transformer.encoder(src)\n",
    "    \n",
    "#     def decode(self, tgt, memory, tgt_mask=None):\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "#         output = self.transformer.decoder(tgt, memory, tgt_mask)\n",
    "#         output = self.output_layer(output)\n",
    "#         return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class Seq2SeqTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "#                  num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_encoder_layers,\n",
    "#             num_decoder_layers=num_decoder_layers,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        \n",
    "#         self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "    \n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "#         # src: [batch_size, src_len]\n",
    "#         # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "#         try:\n",
    "#             # Create target mask for decoder\n",
    "#             if tgt_mask is None:\n",
    "#                 tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "            \n",
    "#             # Create source padding mask\n",
    "#             src_padding_mask = (src == 0).to(src.device)  # True for padding tokens\n",
    "#             tgt_padding_mask = (tgt == 0).to(tgt.device)  # True for padding tokens\n",
    "            \n",
    "#             # Embedding and positional encoding\n",
    "#             src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#             src = self.pos_encoder(src)\n",
    "            \n",
    "#             tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#             tgt = self.pos_encoder(tgt)\n",
    "            \n",
    "#             # Transformer forward pass\n",
    "#             output = self.transformer(\n",
    "#                 src, \n",
    "#                 tgt,\n",
    "#                 src_mask=None,  # No source mask needed\n",
    "#                 tgt_mask=tgt_mask,\n",
    "#                 src_key_padding_mask=src_padding_mask,\n",
    "#                 tgt_key_padding_mask=tgt_padding_mask,\n",
    "#                 memory_key_padding_mask=src_padding_mask\n",
    "#             )\n",
    "            \n",
    "#             # Project to vocabulary size\n",
    "#             output = self.output_layer(output)\n",
    "            \n",
    "#             return output\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in forward pass: {str(e)}\")\n",
    "#             print(f\"Input shapes - src: {src.shape}, tgt: {tgt.shape}\")\n",
    "#             print(f\"Device - src: {src.device}, tgt: {tgt.device}\")\n",
    "#             raise\n",
    "    \n",
    "#     def encode(self, src):\n",
    "#         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         return self.transformer.encoder(src)\n",
    "    \n",
    "#     def decode(self, tgt, memory, tgt_mask=None):\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "#         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "#         tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "#         output = self.transformer.decoder(tgt, memory, tgt_mask)\n",
    "#         output = self.output_layer(output)\n",
    "#         return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(\"Model embedding vocab size:\", self.embedding.num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        print(\"Embedding matrix shape:\", self.embedding.weight.shape)\n",
    "\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    # def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "    #     # src: [batch_size, src_len]\n",
    "    #     # tgt: [batch_size, tgt_len]\n",
    "        \n",
    "    #     try:\n",
    "    #         # Create target mask for decoder\n",
    "    #         if tgt_mask is None:\n",
    "    #             tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "            \n",
    "    #         # Create source padding mask\n",
    "    #         src_padding_mask = (src == 0).to(src.device)  # True for padding tokens\n",
    "    #         tgt_padding_mask = (tgt == 0).to(tgt.device)  # True for padding tokens\n",
    "            \n",
    "    #         # Embedding and positional encoding\n",
    "    #         src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "    #         src = self.pos_encoder(src)\n",
    "            \n",
    "    #         tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "    #         tgt = self.pos_encoder(tgt)\n",
    "            \n",
    "    #         # Transformer forward pass\n",
    "    #         output = self.transformer(\n",
    "    #             src, \n",
    "    #             tgt,\n",
    "    #             src_mask=None,  # No source mask needed\n",
    "    #             tgt_mask=tgt_mask,\n",
    "    #             src_key_padding_mask=src_padding_mask,\n",
    "    #             tgt_key_padding_mask=tgt_padding_mask,\n",
    "    #             memory_key_padding_mask=src_padding_mask\n",
    "    #         )\n",
    "            \n",
    "    #         # Project to vocabulary size\n",
    "    #         output = self.output_layer(output)\n",
    "            \n",
    "    #         return output\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error in forward pass: {str(e)}\")\n",
    "    #         print(f\"Input shapes - src: {src.shape}, tgt: {tgt.shape}\")\n",
    "    #         print(f\"Device - src: {src.device}, tgt: {tgt.device}\")\n",
    "    #         raise\n",
    "    \n",
    "    # def encode(self, src):\n",
    "    #     src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "    #     src = self.pos_encoder(src)\n",
    "    #     return self.transformer.encoder(src)\n",
    "    \n",
    "    # def decode(self, tgt, memory, tgt_mask=None):\n",
    "    #     if tgt_mask is None:\n",
    "    #         tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "    #     tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "    #     tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "    #     output = self.transformer.decoder(tgt, memory, tgt_mask)\n",
    "    #     output = self.output_layer(output)\n",
    "    #     return output \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "    # Create subsequent mask for decoder if not provided\n",
    "        if tgt_mask is None:\n",
    "            tgt_seq_len = tgt.size(1)\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device).bool()\n",
    "\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == 0)  # assuming pad_id = 0\n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == 0)\n",
    "\n",
    "        # Embed and positionally encode\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        # Transformer\n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask.bool(),\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask.bool()\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        return self.output_layer(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     #     # Set up MLflow\n",
    "#     # mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "#     # mlflow.set_experiment(\"german-english-translator\")\n",
    "    \n",
    "#     # Set device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # Get dataloaders and tokenizer\n",
    "#     train_loader, val_loader, test_loader, tokenizer = get_dataloaders(\n",
    "#         'de-en/europarl-v7.de-en.de',\n",
    "#         'de-en/europarl-v7.de-en.en',\n",
    "#         batch_size=32\n",
    "#     )\n",
    "#     # print(\"pad_token_id:\", tokenizer.pad_id() if hasattr(tokenizer, \"pad_id\") else \"n/a\")\n",
    "#     # print(\"unk_token_id:\", tokenizer.unk_id() if hasattr(tokenizer, \"unk_id\") else \"n/a\")\n",
    "#     pad_id = tokenizer.pad_id()\n",
    "    \n",
    "#     # Model parameters\n",
    "#     model_params = {\n",
    "#         \"vocab_size\": tokenizer.get_piece_size(),\n",
    "#         \"d_model\": 512,\n",
    "#         \"nhead\": 8,\n",
    "#         \"num_encoder_layers\": 6,\n",
    "#         \"num_decoder_layers\": 6,\n",
    "#         \"dim_feedforward\": 2048,\n",
    "#         \"dropout\": 0.1\n",
    "#     }\n",
    "    \n",
    "#     # Training parameters\n",
    "#     training_params = {\n",
    "#         \"learning_rate\": 1e-4,\n",
    "#         \"betas\" : (0.9, 0.98), # typical for tranformer training\n",
    "#         \"weight_decay\":0.01, # L2 reg\n",
    "#         \"num_epochs\": 10,\n",
    "#         \"batch_size\": 32\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         # with mlflow.start_run() as run:\n",
    "#         #     print(\"✅ Entered MLflow run\")\n",
    "#         #     print(\"Run ID:\", run.info.run_id)\n",
    "#         #     run_id = run.info.run_id\n",
    "#         #     mlflow.log_param(\"debug_check\", 1)  # confirm it's logging\n",
    "#         #     # Log parameters\n",
    "#         #     mlflow.log_params({**model_params, **training_params})\n",
    "            \n",
    "#             # Initialize model\n",
    "#         model = Seq2SeqTransformer(**model_params).to(device)\n",
    "        \n",
    "#         # Initialize optimizer\n",
    "#         optimizer = AdamW(model.parameters(), lr=training_params[\"learning_rate\"],\\\n",
    "#                         betas = training_params[\"betas\"],\\\n",
    "#                             weight_decay = training_params[\"weight_decay\"])\n",
    "        \n",
    "#         # Training loop\n",
    "#         best_val_loss = float('inf')\n",
    "#         training_history = []\n",
    "        \n",
    "#         for epoch in range(training_params[\"num_epochs\"]):\n",
    "#             print(f\"\\nEpoch {epoch + 1}/{training_params['num_epochs']}\")\n",
    "            \n",
    "#             # Train\n",
    "#             train_loss = train_epoch(model, train_loader, optimizer, device, pad_id)\n",
    "            \n",
    "#             # Evaluate\n",
    "#             val_loss = evaluate(model, val_loader, device)\n",
    "            \n",
    "#             # Log metrics\n",
    "#             mlflow.log_metrics({\n",
    "#                 \"train_loss\": train_loss,\n",
    "#                 \"val_loss\": val_loss,\n",
    "#                 \"epoch\": epoch + 1\n",
    "#             }, step=epoch)\n",
    "            \n",
    "#             print(f\"Train Loss: {train_loss:.4f}\")\n",
    "#             print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "#             # Test translation\n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(\"SAMPLE PREDICTION\")\n",
    "#             print(\"=\"*50)\n",
    "#             test_text = \"Hallo, wie geht es dir?\"\n",
    "#             translation = translate(model, tokenizer, test_text, device)\n",
    "#             print(f\"German: {test_text}\")\n",
    "#             print(f\"English: {translation}\")\n",
    "#             print(\"=\"*50 + \"\\n\")\n",
    "            \n",
    "#             # Save epoch results\n",
    "#             epoch_results = {\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'val_loss': val_loss,\n",
    "#                 'model_path': f\"mlruns/{run.info.experiment_id}/{run.info.run_id}/artifacts/model_epoch_{epoch + 1}\",\n",
    "#                 'batch_size': training_params['batch_size'],\n",
    "#                 'learning_rate': training_params['learning_rate'],\n",
    "#                 'device': str(device),\n",
    "#                 'test_translation_source': test_text,\n",
    "#                 'test_translation_output': translation\n",
    "#             }\n",
    "#             training_history.append(epoch_results)\n",
    "            \n",
    "#             # Log model checkpoint\n",
    "#             checkpoint_path = f\"checkpoints/epoch_{epoch + 1}\"\n",
    "#             os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'val_loss': val_loss,\n",
    "#             }, f\"{checkpoint_path}.pt\")\n",
    "                \n",
    "#             # # Log model to MLflow\n",
    "#             # mlflow.pytorch.log_model(model, f\"model_epoch_{epoch + 1}\")\n",
    "            \n",
    "#             # # Log sample translation\n",
    "#             # mlflow.log_text(translation, f\"translations/epoch_{epoch + 1}.txt\")\n",
    "            \n",
    "#             # # Get signatures\n",
    "#             # example_input = torch.randint(0, tokenizer.get_piece_size(), (1, 12)).to(device)\n",
    "#             # example_target = torch.randint(0, tokenizer.get_piece_size(), (1, 11)).to(device)\n",
    "#             # input_example = (example_input, example_target)\n",
    "\n",
    "#             # # Run one inference pass to get output for signature\n",
    "#             # example_output = model(example_input, example_target)\n",
    "#             # signature = infer_signature(\n",
    "#             #     inputs={\"input_ids\": example_input.cpu().numpy(), \"target_input\": example_target.cpu().numpy()},\n",
    "#             #     outputs=example_output.detach().cpu().numpy()\n",
    "#             # )\n",
    "            \n",
    "#             # # Save final model\n",
    "#             # mlflow.pytorch.log_model(model, \"final_model\", input_example=input_example, signature=signature)\n",
    "            \n",
    "#             # Save all training results to DataFrame\n",
    "#             df = pd.DataFrame(training_history)\n",
    "#             timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "#             results_file = f\"training_results_{timestamp}.csv\"\n",
    "#             df.to_csv(results_file, index=False)\n",
    "#             print(f\"\\nDEBUG: Saved training results to {os.path.abspath(results_file)}\")\n",
    "            \n",
    "#             # Log the DataFrame as an artifact\n",
    "#             os.makedirs(\"training_results\", exist_ok=True)\n",
    "#             metrics_file = f\"training_results/metrics_{timestamp}.csv\"\n",
    "#             df.to_csv(metrics_file, index=False)\n",
    "#             print(f\"DEBUG: Saved metrics to {os.path.abspath(metrics_file)}\")\n",
    "#             mlflow.log_artifact(\"training_results\")\n",
    "#             print(\"DEBUG: Logged artifacts to MLflow\")\n",
    "            \n",
    "#             # Print the model path for the shell script to capture\n",
    "#             # model_path = f\"mlruns/{run.info.experiment_id}/{run.info.run_id}/artifacts/final_model\"\n",
    "#             # print(f\"\\nDEBUG: Final model path: {os.path.abspath(model_path)}\")\n",
    "#             # print(f\"MODEL_PATH={model_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"❌ MLflow run failed:\", e)\n",
    "#         print(\"DEBUG: Exception details:\", str(e))\n",
    "#         import traceback\n",
    "#         print(\"DEBUG: Full traceback:\")\n",
    "#         print(traceback.format_exc())\n",
    "\n",
    "# except Exception as e:\n",
    "#         print(\"❌ Main function failed:\", e)\n",
    "#         print(\"DEBUG: Exception details:\", str(e))\n",
    "#         import traceback\n",
    "#         print(\"DEBUG: Full traceback:\")\n",
    "#         print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training now...\")\n",
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# split_parallel_corpus_chunked(\n",
    "#     de_file='de-en/europarl-v7.de-en.de',\n",
    "#     en_file='de-en/europarl-v7.de-en.en',\n",
    "#     output_dir='de-en/split',\n",
    "#     train_ratio=0.7,\n",
    "#     val_ratio=0.2,\n",
    "#     test_ratio=0.1,\n",
    "#     lim=10000  # or None for full dataset\n",
    "# )\n",
    "\n",
    "\n",
    "# train_loader, val_loader, test_loader, tokenizer = get_dataloaders2(\n",
    "#     train_de='de-en/split/train.de',\n",
    "#     train_en='de-en/split/train.en',\n",
    "#     val_de='de-en/split/val.de',\n",
    "#     val_en='de-en/split/val.en',\n",
    "#     test_de='de-en/split/test.de',\n",
    "#     test_en='de-en/split/test.en',\n",
    "#     batch_size=8,\n",
    "#     vocab_train_src='de-en/europarl-v7.de-en.de',\n",
    "#     vocab_train_tgt='de-en/europarl-v7.de-en.en'\n",
    "# )\n",
    "\n",
    "\n",
    "# # Get dataloaders and tokenizer\n",
    "# # train_loader, val_loader, test_loader, tokenizer = get_dataloaders(\n",
    "# #     'de-en/europarl-v7.de-en.de',\n",
    "# #     'de-en/europarl-v7.de-en.en',\n",
    "# #     batch_size=32,\n",
    "# #     lim=10000\n",
    "# # )\n",
    "\n",
    "\n",
    "# # print(\"pad_token_id:\", tokenizer.pad_id() if hasattr(tokenizer, \"pad_id\") else \"n/a\")\n",
    "# # print(\"unk_token_id:\", tokenizer.unk_id() if hasattr(tokenizer, \"unk_id\") else \"n/a\")\n",
    "# pad_id = tokenizer.pad_id()\n",
    "\n",
    "# # Model parameters, halfing these since way less data now\n",
    "# model_params = {\n",
    "#     \"vocab_size\": tokenizer.get_piece_size(),\n",
    "#     \"d_model\": 256,\n",
    "#     \"nhead\": 4,\n",
    "#     \"num_encoder_layers\": 3,\n",
    "#     \"num_decoder_layers\": 3,\n",
    "#     \"dim_feedforward\": 1024,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# # Training parameters\n",
    "# training_params = {\n",
    "#     \"learning_rate\": 5e-5,\n",
    "#     \"betas\": (0.9, 0.98),\n",
    "#     \"weight_decay\": 0.1,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"warmup_steps\": 1000,\n",
    "#     \"gradient_clip_val\": 1.0,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Initialize model and optimizer\n",
    "#     model = Seq2SeqTransformer(**model_params).to(device)\n",
    "#     optimizer = AdamW(model.parameters(), \n",
    "#                     lr=training_params[\"learning_rate\"],\n",
    "#                     betas=training_params[\"betas\"],\n",
    "#                     weight_decay=training_params[\"weight_decay\"])\n",
    "    \n",
    "#     # Add learning rate scheduler\n",
    "#     def get_lr_scheduler(optimizer, warmup_steps):\n",
    "#         def lr_lambda(step):\n",
    "#             if step < warmup_steps:\n",
    "#                 return float(step) / float(max(1, warmup_steps))\n",
    "#             return 1.0\n",
    "#         return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "#     scheduler = get_lr_scheduler(optimizer, training_params[\"warmup_steps\"])\n",
    "#     global_step = 0\n",
    "    \n",
    "#     # Training loop\n",
    "#     best_val_loss = float('inf')\n",
    "#     training_history = []\n",
    "    \n",
    "#     for epoch in range(training_params[\"num_epochs\"]):\n",
    "#         print(f\"\\nEpoch {epoch + 1}/{training_params['num_epochs']}\")\n",
    "        \n",
    "#         # Train\n",
    "#         train_loss = train_epoch(model, train_loader, optimizer, device, pad_id)\n",
    "        \n",
    "#         # Evaluate\n",
    "#         val_loss = evaluate(model, val_loader, device)\n",
    "        \n",
    "#         # Save checkpoint locally (keeping only last 2)\n",
    "#         checkpoint_path = save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss)\n",
    "#         print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "#         # Save epoch results\n",
    "#         epoch_results = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'train_loss': train_loss,\n",
    "#             'val_loss': val_loss,\n",
    "#             'checkpoint_path': checkpoint_path,\n",
    "#             'batch_size': training_params['batch_size'],\n",
    "#             'learning_rate': training_params['learning_rate'],\n",
    "#             'device': str(device)\n",
    "#         }\n",
    "#         training_history.append(epoch_results)\n",
    "        \n",
    "#         # Save results to CSV\n",
    "#         df = pd.DataFrame(training_history)\n",
    "#         results_file = f\"training_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "#         df.to_csv(results_file, index=False)\n",
    "#         print(f\"Saved training results to {results_file}\")\n",
    "        \n",
    "#         # Update learning rate\n",
    "#         scheduler.step(global_step)\n",
    "#         global_step += 1\n",
    "        \n",
    "#         print(f\"Train Loss: {train_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "#         # Test translation\n",
    "#         test_text = \"Hallo, wie geht es dir?\"\n",
    "#         translation = translate(model, tokenizer, test_text, device)\n",
    "#         print(f\"\\nSample translation:\")\n",
    "#         print(f\"German: {test_text}\")\n",
    "#         print(f\"English: {translation}\")\n",
    "        \n",
    "#         # Check disk space before next epoch\n",
    "#         if not my_check_disk_space():\n",
    "#             print(\"Warning: Low disk space. Saving current state and stopping training.\")\n",
    "#             break\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during training: {str(e)}\")\n",
    "#     import traceback\n",
    "#     print(traceback.format_exc())\n",
    "#     # Save the model state even if training fails\n",
    "#     if 'model' in locals():\n",
    "#         save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss, path=\"error_checkpoints\")\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training now...\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# batch_size = 32\n",
    "# tokenizer = train_tokenizer(\"de-en/europarl-v7.de-en.de\", \"de-en/europarl-v7.de-en.en\")\n",
    "\n",
    "# split_parallel_corpus_chunked(\n",
    "#     de_file='de-en/europarl-v7.de-en.de',\n",
    "#     en_file='de-en/europarl-v7.de-en.en',\n",
    "#     output_dir='de-en/split',\n",
    "#     train_ratio=0.7,\n",
    "#     val_ratio=0.2,\n",
    "#     test_ratio=0.1,\n",
    "#     lim=10000  # or None for full dataset\n",
    "# )\n",
    "\n",
    "# train_loader = get_dataloader_from_files(\"train.de\", \"train.en\", tokenizer, batch_size)\n",
    "# val_loader = get_dataloader_from_files(\"val.de\", \"val.en\", tokenizer, batch_size)\n",
    "# test_loader = get_dataloader_from_files(\"test.de\", \"test.en\", tokenizer, batch_size)\n",
    "\n",
    "# pad_id = tokenizer.pad_id()\n",
    "\n",
    "# # Model parameters, halfing these since way less data now\n",
    "# model_params = {\n",
    "#     \"vocab_size\": tokenizer.get_piece_size(),\n",
    "#     \"d_model\": 256,\n",
    "#     \"nhead\": 4,\n",
    "#     \"num_encoder_layers\": 3,\n",
    "#     \"num_decoder_layers\": 3,\n",
    "#     \"dim_feedforward\": 1024,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# # Training parameters\n",
    "# training_params = {\n",
    "#     \"learning_rate\": 5e-5,\n",
    "#     \"betas\": (0.9, 0.98),\n",
    "#     \"weight_decay\": 0.1,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"warmup_steps\": 1000,\n",
    "#     \"gradient_clip_val\": 1.0,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Initialize model and optimizer\n",
    "#     model = Seq2SeqTransformer(**model_params).to(device)\n",
    "#     optimizer = AdamW(model.parameters(), \n",
    "#                     lr=training_params[\"learning_rate\"],\n",
    "#                     betas=training_params[\"betas\"],\n",
    "#                     weight_decay=training_params[\"weight_decay\"])\n",
    "    \n",
    "#     # Add learning rate scheduler\n",
    "#     def get_lr_scheduler(optimizer, warmup_steps):\n",
    "#         def lr_lambda(step):\n",
    "#             if step < warmup_steps:\n",
    "#                 return float(step) / float(max(1, warmup_steps))\n",
    "#             return 1.0\n",
    "#         return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "#     scheduler = get_lr_scheduler(optimizer, training_params[\"warmup_steps\"])\n",
    "#     global_step = 0\n",
    "    \n",
    "#     # Training loop\n",
    "#     best_val_loss = float('inf')\n",
    "#     training_history = []\n",
    "    \n",
    "#     for epoch in range(training_params[\"num_epochs\"]):\n",
    "#         print(f\"\\nEpoch {epoch + 1}/{training_params['num_epochs']}\")\n",
    "        \n",
    "#         # Train\n",
    "#         train_loss = train_epoch(model, train_loader, optimizer, device, pad_id)\n",
    "        \n",
    "#         # Evaluate\n",
    "#         val_loss = evaluate(model, val_loader, device)\n",
    "        \n",
    "#         # Save checkpoint locally (keeping only last 2)\n",
    "#         checkpoint_path = save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss)\n",
    "#         print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "#         # Save epoch results\n",
    "#         epoch_results = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'train_loss': train_loss,\n",
    "#             'val_loss': val_loss,\n",
    "#             'checkpoint_path': checkpoint_path,\n",
    "#             'batch_size': training_params['batch_size'],\n",
    "#             'learning_rate': training_params['learning_rate'],\n",
    "#             'device': str(device)\n",
    "#         }\n",
    "#         training_history.append(epoch_results)\n",
    "        \n",
    "#         # Save results to CSV\n",
    "#         df = pd.DataFrame(training_history)\n",
    "#         results_file = f\"training_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "#         df.to_csv(results_file, index=False)\n",
    "#         print(f\"Saved training results to {results_file}\")\n",
    "        \n",
    "#         # Update learning rate\n",
    "#         scheduler.step(global_step)\n",
    "#         global_step += 1\n",
    "        \n",
    "#         print(f\"Train Loss: {train_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "#         # Test translation\n",
    "#         test_text = \"Hallo, wie geht es dir?\"\n",
    "#         translation = translate(model, tokenizer, test_text, device)\n",
    "#         print(f\"\\nSample translation:\")\n",
    "#         print(f\"German: {test_text}\")\n",
    "#         print(f\"English: {translation}\")\n",
    "        \n",
    "#         # Check disk space before next epoch\n",
    "#         if not my_check_disk_space():\n",
    "#             print(\"Warning: Low disk space. Saving current state and stopping training.\")\n",
    "#             break\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during training: {str(e)}\")\n",
    "#     import traceback\n",
    "#     print(traceback.format_exc())\n",
    "#     # Save the model state even if training fails\n",
    "#     if 'model' in locals():\n",
    "#         save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss, path=\"error_checkpoints\")\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training now...\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# batch_size = 32\n",
    "\n",
    "# # ✅ 1. Run this ONCE — then comment it out for all future runs\n",
    "# if not os.path.exists(\"translation_tokenizer.model\"):\n",
    "#     train_tokenizer(\"de-en/europarl-v7.de-en.de\", \"de-en/europarl-v7.de-en.en\")\n",
    "\n",
    "# if not os.path.exists(\"de-en/split/train.de\"):\n",
    "#     split_parallel_corpus_chunked(\n",
    "#         de_file='de-en/europarl-v7.de-en.de',\n",
    "#         en_file='de-en/europarl-v7.de-en.en',\n",
    "#         output_dir='de-en/split',\n",
    "#         train_ratio=0.7,\n",
    "#         val_ratio=0.2,\n",
    "#         test_ratio=0.1,\n",
    "#         lim=10000  # ✅ limit before any tokenizing happens\n",
    "#     )\n",
    "\n",
    "# # ✅ 2. Load tokenizer from file (no data loaded into memory)\n",
    "# tokenizer = load_tokenizer()\n",
    "\n",
    "# # ✅ 3. Use only pre-split, line-limited files\n",
    "# train_loader = get_dataloader_from_files(\"de-en/split/train.de\", \"de-en/split/train.en\", tokenizer, batch_size)\n",
    "# val_loader = get_dataloader_from_files(\"de-en/split/val.de\", \"de-en/split/val.en\", tokenizer, batch_size)\n",
    "# test_loader = get_dataloader_from_files(\"de-en/split/test.de\", \"de-en/split/test.en\", tokenizer, batch_size)\n",
    "\n",
    "# pad_id = tokenizer.pad_id()\n",
    "\n",
    "\n",
    "# # Model parameters, halfing these since way less data now\n",
    "# model_params = {\n",
    "#     \"vocab_size\": tokenizer.get_piece_size(),\n",
    "#     \"d_model\": 256,\n",
    "#     \"nhead\": 4,\n",
    "#     \"num_encoder_layers\": 3,\n",
    "#     \"num_decoder_layers\": 3,\n",
    "#     \"dim_feedforward\": 1024,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# # Training parameters\n",
    "# training_params = {\n",
    "#     \"learning_rate\": 5e-5,\n",
    "#     \"betas\": (0.9, 0.98),\n",
    "#     \"weight_decay\": 0.1,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"warmup_steps\": 1000,\n",
    "#     \"gradient_clip_val\": 1.0,\n",
    "#     \"dropout\": 0.2\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Initialize model and optimizer\n",
    "#     model = Seq2SeqTransformer(**model_params).to(device)\n",
    "#     optimizer = AdamW(model.parameters(), \n",
    "#                     lr=training_params[\"learning_rate\"],\n",
    "#                     betas=training_params[\"betas\"],\n",
    "#                     weight_decay=training_params[\"weight_decay\"])\n",
    "    \n",
    "#     # Add learning rate scheduler\n",
    "#     def get_lr_scheduler(optimizer, warmup_steps):\n",
    "#         def lr_lambda(step):\n",
    "#             if step < warmup_steps:\n",
    "#                 return float(step) / float(max(1, warmup_steps))\n",
    "#             return 1.0\n",
    "#         return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "#     scheduler = get_lr_scheduler(optimizer, training_params[\"warmup_steps\"])\n",
    "#     global_step = 0\n",
    "    \n",
    "#     # Training loop\n",
    "#     best_val_loss = float('inf')\n",
    "#     training_history = []\n",
    "    \n",
    "#     for epoch in range(training_params[\"num_epochs\"]):\n",
    "#         print(f\"\\nEpoch {epoch + 1}/{training_params['num_epochs']}\")\n",
    "        \n",
    "#         # Train\n",
    "#         train_loss = train_epoch(model, train_loader, optimizer, device, pad_id)\n",
    "        \n",
    "#         # Evaluate\n",
    "#         val_loss = float('inf')\n",
    "#         try:\n",
    "#             val_loss = evaluate(model, val_loader, device)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Validation failed: {str(e)}\")\n",
    "        \n",
    "#         # Save checkpoint locally (keeping only last 2)\n",
    "#         # val_loss = val_loss if 'val_loss' in locals() else None\n",
    "#         checkpoint_path = save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss, path=\"error_checkpoints\")\n",
    "#         print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "#         # Save epoch results\n",
    "#         epoch_results = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'train_loss': train_loss,\n",
    "#             'val_loss': val_loss,\n",
    "#             'checkpoint_path': checkpoint_path,\n",
    "#             'batch_size': training_params['batch_size'],\n",
    "#             'learning_rate': training_params['learning_rate'],\n",
    "#             'device': str(device)\n",
    "#         }\n",
    "#         training_history.append(epoch_results)\n",
    "        \n",
    "#         # Save results to CSV\n",
    "#         df = pd.DataFrame(training_history)\n",
    "#         results_file = f\"training_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "#         df.to_csv(results_file, index=False)\n",
    "#         print(f\"Saved training results to {results_file}\")\n",
    "        \n",
    "#         # Update learning rate\n",
    "#         scheduler.step()\n",
    "#         global_step += 1\n",
    "        \n",
    "#         print(f\"Train Loss: {train_loss:.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "#         # Test translation\n",
    "#         test_text = \"Hallo, wie geht es dir?\"\n",
    "#         translation = translate(model, tokenizer, test_text, device)\n",
    "#         print(f\"\\nSample translation:\")\n",
    "#         print(f\"German: {test_text}\")\n",
    "#         print(f\"English: {translation}\")\n",
    "        \n",
    "#         # Check disk space before next epoch\n",
    "#         if not my_check_disk_space():\n",
    "#             print(\"Warning: Low disk space. Saving current state and stopping training.\")\n",
    "#             break\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during training: {str(e)}\")\n",
    "#     import traceback\n",
    "#     print(traceback.format_exc())\n",
    "#     # Save the model state even if training fails\n",
    "#     if 'model' in locals():\n",
    "#         save_model_locally(model, optimizer, epoch + 1, train_loss, val_loss, path=\"error_checkpoints\")\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training new tokenizer...\n",
      "Train: 8000, Val: 1000, Test: 1000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokenizer_input.txt\n",
      "  input_format: \n",
      "  model_prefix: translation_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [DE]\n",
      "  user_defined_symbols: [EN]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: tokenizer_input.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 15959 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [DE]\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: [EN]\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=2571543\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=105\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 15959 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 15959\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 39803\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=58837 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13648 size=20 all=2738 active=1977 piece=or\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8415 size=40 all=3738 active=2977 piece=▁h\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5705 size=60 all=4539 active=3778 piece=▁is\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4250 size=80 all=5659 active=4898 piece=ve\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3361 size=100 all=6794 active=6033 piece=lich\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3340 min_freq=281\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2883 size=120 all=7649 active=1813 piece=isch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2397 size=140 all=8254 active=2418 piece=de\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1927 size=160 all=8973 active=3137 piece=▁von\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1707 size=180 all=9707 active=3871 piece=uld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1493 size=200 all=10157 active=4321 piece=▁have\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1478 min_freq=251\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1351 size=220 all=10771 active=1615 piece=rä\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1222 size=240 all=11484 active=2328 piece=über\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1116 size=260 all=12106 active=2950 piece=ungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1015 size=280 all=12858 active=3702 piece=aus\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=943 size=300 all=13608 active=4452 piece=art\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=937 min_freq=214\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=862 size=320 all=14197 active=1530 piece=00\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=799 size=340 all=14782 active=2115 piece=heit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=752 size=360 all=15237 active=2570 piece=▁haben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=700 size=380 all=15792 active=3125 piece=tern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=651 size=400 all=16312 active=3645 piece=ise\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=651 min_freq=192\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=624 size=420 all=16691 active=1333 piece=▁part\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=583 size=440 all=17113 active=1755 piece=ical\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=552 size=460 all=17635 active=2277 piece=▁from\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=526 size=480 all=18115 active=2757 piece=vel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=497 size=500 all=18578 active=3220 piece=▁polit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=495 min_freq=169\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=520 all=18965 active=1381 piece=ction\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=450 size=540 all=19374 active=1790 piece=ance\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=425 size=560 all=19775 active=2191 piece=mer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=410 size=580 all=20048 active=2464 piece=che\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=401 size=600 all=20424 active=2840 piece=▁int\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=401 min_freq=150\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=384 size=620 all=20672 active=1262 piece=du\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=640 all=20991 active=1581 piece=▁ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=357 size=660 all=21365 active=1955 piece=ließ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=344 size=680 all=21730 active=2320 piece=olitik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=332 size=700 all=22095 active=2685 piece=▁iss\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=331 min_freq=129\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=321 size=720 all=22372 active=1380 piece=ität\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=310 size=740 all=22837 active=1845 piece=ahl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=300 size=760 all=23071 active=2079 piece=ign\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=291 size=780 all=23468 active=2476 piece=alt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=800 all=23911 active=2919 piece=unkt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=284 min_freq=112\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=276 size=820 all=24198 active=1468 piece=▁say\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=266 size=840 all=24406 active=1676 piece=lt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=259 size=860 all=24640 active=1910 piece=▁proposal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=250 size=880 all=24826 active=2096 piece=rol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=241 size=900 all=25070 active=2340 piece=schließ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=241 min_freq=99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=920 all=25284 active=1446 piece=▁ins\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=940 all=25551 active=1713 piece=▁over\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=960 all=25725 active=1887 piece=▁mark\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=219 size=980 all=25947 active=2109 piece=ständ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=213 size=1000 all=26203 active=2365 piece=bens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=213 min_freq=89\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=209 size=1020 all=26439 active=1517 piece=▁point\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=203 size=1040 all=26609 active=1687 piece=▁ar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=1060 all=26901 active=1979 piece=ublic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=196 size=1080 all=27155 active=2233 piece=▁requ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=1100 all=27306 active=2384 piece=fe\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=192 min_freq=82\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=1120 all=27592 active=1610 piece=rukt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=184 size=1140 all=27850 active=1868 piece=▁weil\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=1160 all=28102 active=2120 piece=▁2000\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=1180 all=28314 active=2332 piece=▁ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=1200 all=28436 active=2454 piece=elbst\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=175 min_freq=75\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=1220 all=28654 active=1628 piece=fall\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=168 size=1240 all=28839 active=1813 piece=▁dafür\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=163 size=1260 all=28946 active=1920 piece=EG\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=160 size=1280 all=29192 active=2166 piece=▁Mrs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=1300 all=29332 active=2306 piece=▁Gro\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=158 min_freq=70\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=156 size=1320 all=29433 active=1565 piece=▁erw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=1340 all=29548 active=1680 piece=ene\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=1360 all=29726 active=1858 piece=▁pla\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=1380 all=29838 active=1970 piece=▁future\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=1400 all=29995 active=2127 piece=ischer\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=145 min_freq=65\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=1420 all=30197 active=1665 piece=▁ihrer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=1440 all=30344 active=1812 piece=cond\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=1460 all=30473 active=1941 piece=ium\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=1480 all=30702 active=2170 piece=▁maj\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=1500 all=30836 active=2304 piece=ised\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=133 min_freq=61\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=131 size=1520 all=31021 active=1703 piece=▁democ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=1540 all=31198 active=1880 piece=▁ser\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=1560 all=31352 active=2034 piece=▁find\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=1580 all=31458 active=2140 piece=reichen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=1600 all=31630 active=2312 piece=aum\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=123 min_freq=57\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=1620 all=31739 active=1676 piece=▁measures\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=1640 all=31903 active=1840 piece=▁legal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=1660 all=31998 active=1935 piece=erk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=1680 all=32140 active=2077 piece=ents\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=1700 all=32291 active=2228 piece=▁require\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=115 min_freq=53\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=1720 all=32431 active=1752 piece=▁aller\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=1740 all=32544 active=1865 piece=▁world\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=1760 all=32699 active=2020 piece=▁few\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=1780 all=32844 active=2165 piece=▁Vertrag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=1800 all=32970 active=2291 piece=▁Au\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=106 min_freq=50\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=1820 all=33089 active=1764 piece=▁einmal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=1840 all=33193 active=1868 piece=elf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=1860 all=33382 active=2057 piece=rache\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=1880 all=33488 active=2163 piece=grund\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=1900 all=33655 active=2330 piece=▁Beit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=100 min_freq=47\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=1920 all=33819 active=1842 piece=▁stand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=1940 all=33951 active=1974 piece=▁Punkt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=1960 all=34102 active=2125 piece=government\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=1980 all=34211 active=2234 piece=governmental\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=94 size=2000 all=34345 active=2368 piece=▁cr\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=94 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=2020 all=34387 active=1754 piece=▁Fl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=2040 all=34577 active=1944 piece=▁Best\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=2060 all=34671 active=2038 piece=▁glau\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=2080 all=34758 active=2125 piece=druck\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=2100 all=34843 active=2210 piece=führung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=89 min_freq=43\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=2120 all=34986 active=1876 piece=▁nationalen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=2140 all=35095 active=1985 piece=flicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=2160 all=35247 active=2137 piece=▁rules\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=2180 all=35332 active=2222 piece=▁proced\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=2200 all=35485 active=2375 piece=▁bew\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=82 min_freq=41\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=2220 all=35657 active=1938 piece=ervor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=2240 all=35766 active=2047 piece=▁Umsetzung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=2260 all=35880 active=2161 piece=▁security\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=2280 all=35998 active=2279 piece=leicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=2300 all=36051 active=2332 piece=▁dep\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=78 min_freq=39\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=2320 all=36110 active=1852 piece=▁6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=2340 all=36217 active=1959 piece=cycling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=2360 all=36338 active=2080 piece=▁lack\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=2380 all=36412 active=2154 piece=fach\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=2400 all=36544 active=2286 piece=half\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=74 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=2420 all=36584 active=1867 piece=▁achieve\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=2440 all=36659 active=1942 piece=▁Pal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=2460 all=36763 active=2046 piece=▁authority\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=2480 all=36896 active=2179 piece=▁having\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=2500 all=37003 active=2286 piece=▁Haider\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=71 min_freq=35\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=2520 all=37104 active=1951 piece=▁gentle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=2540 all=37175 active=2022 piece=▁after\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=2560 all=37263 active=2110 piece=äten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=2580 all=37333 active=2180 piece=▁Vorschläge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=2600 all=37435 active=2282 piece=▁groups\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=67 min_freq=34\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=2620 all=37525 active=1962 piece=unkte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=2640 all=37585 active=2022 piece=urn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=2660 all=37728 active=2165 piece=unktion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=2680 all=37815 active=2252 piece=mitt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=2700 all=37918 active=2355 piece=rachtens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=33\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=2720 all=37974 active=1951 piece=allen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=2740 all=38070 active=2047 piece=/2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=2760 all=38234 active=2211 piece=völker\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=2780 all=38302 active=2279 piece=▁15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=2800 all=38406 active=2383 piece=gument\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=61 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=2820 all=38499 active=2005 piece=▁Lös\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=2840 all=38551 active=2057 piece=▁months\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=2860 all=38577 active=2083 piece=ppos\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=2880 all=38626 active=2132 piece=▁verschiedenen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=2900 all=38767 active=2273 piece=▁erinn\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=58 min_freq=30\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=2920 all=38817 active=1986 piece=▁Instrument\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=2940 all=38925 active=2094 piece=▁inner\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=2960 all=38926 active=2095 piece=▁vorgesehen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=2980 all=39018 active=2187 piece=schutz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=3000 all=39155 active=2324 piece=▁Als\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=55 min_freq=29\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=3020 all=39247 active=2048 piece=▁interests\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=3040 all=39341 active=2142 piece=▁week\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=3060 all=39411 active=2212 piece=esse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=3080 all=39484 active=2285 piece=▁Schutz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=3100 all=39634 active=2435 piece=▁bod\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=52 min_freq=28\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=3120 all=39714 active=2060 piece=▁intend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=3140 all=39732 active=2078 piece=imum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=3160 all=39798 active=2144 piece=▁Diskuss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=3180 all=39936 active=2282 piece=gent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=3200 all=40058 active=2404 piece=isches\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=50 min_freq=27\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=3220 all=40118 active=2042 piece=▁procedure\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=3240 all=40287 active=2211 piece=gaben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=3260 all=40367 active=2291 piece=kontrolle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=3280 all=40483 active=2407 piece=▁Koh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=3300 all=40518 active=2442 piece=▁experien\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=48 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=3320 all=40588 active=2093 piece=▁cru\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=3340 all=40626 active=2131 piece=olitiken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=3360 all=40657 active=2162 piece=▁ze\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=3380 all=40759 active=2264 piece=▁build\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=3400 all=40804 active=2309 piece=▁Beschäftigungs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=45 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=3420 all=40903 active=2133 piece=▁getan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=3440 all=40929 active=2159 piece=▁objective\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=3460 all=41007 active=2237 piece=▁Our\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=3480 all=41102 active=2332 piece=▁unemp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=3500 all=41140 active=2370 piece=▁Programms\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=44 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=3520 all=41194 active=2111 piece=rang\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=3540 all=41289 active=2206 piece=▁share\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=3560 all=41337 active=2254 piece=▁congratul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=3580 all=41421 active=2338 piece=▁pri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=3600 all=41499 active=2416 piece=▁wobei\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=42 min_freq=24\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=3620 all=41542 active=2118 piece=▁unserem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=3640 all=41564 active=2140 piece=asch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=3660 all=41673 active=2249 piece=finden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=3680 all=41704 active=2280 piece=▁recogn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=3700 all=41748 active=2324 piece=▁criminal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=23\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=3720 all=41799 active=2138 piece=äss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=3740 all=41949 active=2288 piece=ience\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=3760 all=41997 active=2336 piece=iveness\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=3780 all=42027 active=2366 piece=▁Verfügung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=3800 all=42109 active=2448 piece=fern\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=3820 all=42206 active=2189 piece=▁Städ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=3840 all=42250 active=2233 piece=▁behand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=3860 all=42290 active=2273 piece=▁2000-2006\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=3880 all=42432 active=2415 piece=temp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=3900 all=42551 active=2534 piece=uclear\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=3920 all=42589 active=2163 piece=▁Integr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=3940 all=42613 active=2187 piece=▁unemployment\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=3960 all=42751 active=2325 piece=▁Kar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=3980 all=42838 active=2412 piece=▁turn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=4000 all=42882 active=2456 piece=▁Treaties\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=37 min_freq=21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=4020 all=42929 active=2192 piece=äuf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=4040 all=43036 active=2299 piece=teuer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=4060 all=43105 active=2368 piece=▁Langen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=4080 all=43129 active=2392 piece=▁Verwaltung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=4100 all=43219 active=2482 piece=iken\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=35 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=4120 all=43313 active=2249 piece=▁affe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=4140 all=43370 active=2306 piece=▁wegen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=4160 all=43380 active=2316 piece=▁vernünft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=4180 all=43381 active=2317 piece=bod\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=4200 all=43530 active=2466 piece=▁ago\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=34 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=4220 all=43594 active=2241 piece=▁basic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=4240 all=43621 active=2268 piece=▁sorgen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=4260 all=43647 active=2294 piece=▁unaccept\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4280 all=43703 active=2350 piece=▁11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4300 all=43781 active=2428 piece=▁When\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=33 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4320 all=43846 active=2255 piece=▁Straf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4340 all=43891 active=2300 piece=▁stellt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4360 all=43917 active=2326 piece=▁Zweitens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=4380 all=43919 active=2328 piece=▁significant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=4400 all=43981 active=2390 piece=resp\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=4420 all=44060 active=2266 piece=▁Bezie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=4440 all=44077 active=2283 piece=▁immedi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=4460 all=44091 active=2297 piece=▁creating\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=4480 all=44085 active=2291 piece=▁establishing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=4500 all=44232 active=2438 piece=▁Rep\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=31 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=4520 all=44341 active=2316 piece=▁firm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=4540 all=44407 active=2382 piece=▁using\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=4560 all=44435 active=2410 piece=▁geführt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=4580 all=44432 active=2407 piece=▁Transport\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4600 all=44422 active=2397 piece=ave\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=30 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4620 all=44520 active=2314 piece=▁occ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4640 all=44596 active=2390 piece=▁total\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4660 all=44642 active=2436 piece=ichtlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4680 all=44665 active=2459 piece=▁Natürlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=4700 all=44663 active=2457 piece=▁gewährleisten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=30 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4720 all=44758 active=2329 piece=▁UN\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4740 all=44870 active=2441 piece=fälle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4760 all=44973 active=2544 piece=▁home\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4780 all=45023 active=2594 piece=▁fraud\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4800 all=45047 active=2618 piece=▁review\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=29 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4820 all=45085 active=2288 piece=▁everyone\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=4840 all=45091 active=2294 piece=▁subsidiarity\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4860 all=45198 active=2401 piece=tain\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4880 all=45301 active=2504 piece=änden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4900 all=45359 active=2562 piece=▁assoc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4920 all=45396 active=2304 piece=▁events\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4940 all=45420 active=2328 piece=▁gleichen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=4960 all=45429 active=2337 piece=▁wesentlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=4980 all=45463 active=2371 piece=cher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5000 all=45577 active=2485 piece=griff\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=27 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5020 all=45625 active=2321 piece=▁talk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5040 all=45658 active=2354 piece=▁highl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5060 all=45684 active=2380 piece=zuführen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5080 all=45695 active=2391 piece=▁Vitorino\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=5100 all=45720 active=2416 piece=▁contribute\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=27 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5120 all=45706 active=2272 piece=mü\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5140 all=45866 active=2432 piece=▁unc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5160 all=45916 active=2482 piece=▁sort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5180 all=45927 active=2493 piece=▁fragen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5200 all=45953 active=2519 piece=▁Bedenken\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=5220 all=45962 active=2307 piece=▁conference\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5240 all=46012 active=2357 piece=▁21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5260 all=46106 active=2451 piece=wahl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5280 all=46181 active=2526 piece=▁Brit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5300 all=46235 active=2580 piece=▁benöt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=25 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5320 all=46277 active=2349 piece=▁konnte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5340 all=46287 active=2359 piece=▁competen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=5360 all=46280 active=2352 piece=▁effectively\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5380 all=46340 active=2412 piece=ober\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5400 all=46432 active=2504 piece=elten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5420 all=46509 active=2391 piece=▁poll\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5440 all=46543 active=2425 piece=▁jedes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5460 all=46550 active=2432 piece=▁braucht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5480 all=46544 active=2426 piece=▁referred\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=5500 all=46556 active=2438 piece=organisation\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5520 all=46624 active=2391 piece=üng\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5540 all=46731 active=2498 piece=▁fal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5560 all=46799 active=2566 piece=▁Flex\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5580 all=46823 active=2590 piece=▁Costa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5600 all=46863 active=2630 piece=zeichne\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=23 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5620 all=46892 active=2366 piece=▁origin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5640 all=46924 active=2398 piece=▁reading\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5660 all=46941 active=2415 piece=▁betrachte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=5680 all=46938 active=2412 piece=▁konzentrieren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5700 all=47032 active=2506 piece=just\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5720 all=47136 active=2448 piece=inden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5740 all=47215 active=2527 piece=▁verm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5760 all=47254 active=2566 piece=▁übern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5780 all=47296 active=2608 piece=▁strict\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5800 all=47322 active=2634 piece=▁Haushalt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=5820 all=47326 active=2369 piece=▁zuständig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5840 all=47342 active=2385 piece=ubl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5860 all=47448 active=2491 piece=zess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5880 all=47497 active=2540 piece=▁2002\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5900 all=47544 active=2587 piece=idents\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=21 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5920 all=47559 active=2388 piece=General\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5940 all=47605 active=2434 piece=▁impact\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5960 all=47616 active=2445 piece=▁element\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=5980 all=47620 active=2449 piece=▁investig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=6000 all=47617 active=2446 piece=▁Ausbildung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=21 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=6020 all=47612 active=2376 piece=▁Fremdenverkehr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6040 all=47722 active=2486 piece=▁kl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6060 all=47826 active=2590 piece=▁zul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6080 all=47887 active=2651 piece=▁exem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6100 all=47932 active=2696 piece=ördert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6120 all=47944 active=2407 piece=fordern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6140 all=47971 active=2434 piece=▁ausges\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6160 all=47997 active=2460 piece=▁approve\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6180 all=48004 active=2467 piece=setzungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6200 all=48004 active=2467 piece=▁derartige\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=6220 all=47992 active=2389 piece=▁sovereignty\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6240 all=48045 active=2442 piece=cke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6260 all=48195 active=2592 piece=ilit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6280 all=48239 active=2636 piece=▁Dann\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6300 all=48274 active=2671 piece=cedure\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6320 all=48296 active=2435 piece=▁sogen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6340 all=48357 active=2496 piece=ältigen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6360 all=48372 active=2511 piece=▁hoffen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6380 all=48380 active=2519 piece=▁letzter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6400 all=48392 active=2531 piece=▁heritage\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6420 all=48383 active=2411 piece=▁vermeiden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=6440 all=48372 active=2400 piece=▁intervention\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6460 all=48416 active=2444 piece=akis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6480 all=48527 active=2555 piece=▁Dut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6500 all=48597 active=2625 piece=ormen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6520 all=48641 active=2472 piece=gerung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6540 all=48716 active=2547 piece=▁happy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6560 all=48771 active=2602 piece=ulative\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6580 all=48809 active=2640 piece=anderung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6600 all=48814 active=2645 piece=▁intends\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6620 all=48821 active=2448 piece=▁größeren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6640 all=48821 active=2448 piece=▁entscheid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=6660 all=48818 active=2445 piece=▁beschränken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6680 all=48807 active=2434 piece=oe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6700 all=48915 active=2542 piece=Vert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6720 all=49008 active=2535 piece=▁Ins\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6740 all=49070 active=2597 piece=reibt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6760 all=49135 active=2662 piece=▁quot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6780 all=49222 active=2749 piece=▁Daten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6800 all=49252 active=2779 piece=ktionen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6820 all=49273 active=2479 piece=▁untern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6840 all=49284 active=2490 piece=▁lokaler\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6860 all=49290 active=2496 piece=▁advisers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6880 all=49290 active=2496 piece=▁Vertrages\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6900 all=49275 active=2481 piece=▁Energieträ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6920 all=49263 active=2450 piece=▁credibility\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=6940 all=49251 active=2438 piece=▁Kohäsionsfonds\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=6960 all=49324 active=2511 piece=riè\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=6980 all=49405 active=2592 piece=ragt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7000 all=49471 active=2658 piece=▁nöt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7020 all=49551 active=2551 piece=uting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7040 all=49584 active=2584 piece=▁lies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7060 all=49643 active=2643 piece=▁China\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7080 all=49652 active=2652 piece=ategien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7100 all=49690 active=2690 piece=▁facing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7120 all=49696 active=2491 piece=änderung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7140 all=49695 active=2490 piece=▁respekt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7160 all=49714 active=2509 piece=▁financed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7180 all=49704 active=2499 piece=▁combating\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7200 all=49690 active=2485 piece=▁challenges\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7220 all=49672 active=2467 piece=▁Altfahrzeuge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=7240 all=49658 active=2453 piece=▁Stahlindustrie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7260 all=49773 active=2568 piece=▁23\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7280 all=49848 active=2643 piece=▁Bol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7300 all=49894 active=2689 piece=isher\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7320 all=49942 active=2538 piece=▁insp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7340 all=49998 active=2594 piece=jahres\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7360 all=50045 active=2641 piece=▁domin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7380 all=50070 active=2666 piece=▁Gewähr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7400 all=50078 active=2674 piece=▁lokale\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7420 all=50109 active=2535 piece=▁Kingdom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7440 all=50104 active=2530 piece=▁niemand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7460 all=50115 active=2541 piece=▁Lösungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7480 all=50104 active=2530 piece=▁secondly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7500 all=50110 active=2536 piece=▁disadvant\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7520 all=50113 active=2506 piece=▁bestätigen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=7540 all=50099 active=2492 piece=▁gefährlicher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7560 all=50119 active=2512 piece=MEs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7580 all=50193 active=2586 piece=feil\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7600 all=50278 active=2671 piece=▁PSE\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7620 all=50322 active=2558 piece=hilos\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7640 all=50355 active=2591 piece=▁abst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7660 all=50383 active=2619 piece=inding\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7680 all=50471 active=2707 piece=▁aktiv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7700 all=50476 active=2712 piece=fertigt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7720 all=50491 active=2535 piece=▁chance\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7740 all=50501 active=2545 piece=▁racism\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7760 all=50535 active=2579 piece=▁Akteure\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7780 all=50542 active=2586 piece=▁gezeigt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7800 all=50538 active=2582 piece=▁consolid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7820 all=50525 active=2512 piece=▁Botschaft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7840 all=50516 active=2503 piece=▁Bolkestein\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7860 all=50501 active=2488 piece=▁distributed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=7880 all=50488 active=2475 piece=▁notification\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=7900 all=50479 active=2466 piece=IS\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=7920 all=50555 active=2595 piece=etze\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=7940 all=50621 active=2661 piece=▁GDP\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=7960 all=50695 active=2735 piece=inter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=7980 all=50761 active=2801 piece=▁merg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8000 all=50805 active=2845 piece=▁Dalai\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8020 all=50822 active=2558 piece=▁lives\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8040 all=50864 active=2600 piece=▁Fehler\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8060 all=50873 active=2609 piece=▁leicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8080 all=50907 active=2643 piece=▁Finnish\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8100 all=50918 active=2654 piece=▁glauben\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8120 all=50923 active=2551 piece=▁Eurojust\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8140 all=50919 active=2547 piece=▁verfolgt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8160 all=50911 active=2539 piece=▁ergriffen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8180 all=50899 active=2527 piece=▁strategies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=8200 all=50887 active=2515 piece=▁Bereitschaft\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8220 all=50891 active=2549 piece=AG\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8240 all=50983 active=2641 piece=▁Ah\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8260 all=51032 active=2690 piece=oura\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8280 all=51075 active=2733 piece=▁Lis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8300 all=51137 active=2795 piece=reize\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8320 all=51178 active=2596 piece=▁ante\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8340 all=51198 active=2616 piece=atever\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8360 all=51250 active=2668 piece=uklear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8380 all=51270 active=2688 piece=▁Tagen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8400 all=51287 active=2705 piece=▁sixth\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8420 all=51340 active=2618 piece=▁Gerade\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8440 all=51352 active=2630 piece=▁compla\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8460 all=51365 active=2643 piece=▁stattf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8480 all=51378 active=2656 piece=▁Ordnung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8500 all=51378 active=2656 piece=▁genuine\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8520 all=51389 active=2580 piece=▁Behinder\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8540 all=51387 active=2578 piece=▁fighting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8560 all=51377 active=2568 piece=▁zugrunde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8580 all=51368 active=2559 piece=▁ernsthaft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8600 all=51358 active=2549 piece=▁betrachten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8620 all=51349 active=2559 piece=▁Schwerpunkt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8640 all=51333 active=2543 piece=▁Griechenland\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=8660 all=51317 active=2527 piece=▁bevorstehenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8680 all=51348 active=2558 piece=det\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8700 all=51422 active=2632 piece=▁48\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8720 all=51505 active=2653 piece=uses\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8740 all=51551 active=2699 piece=▁tut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8760 all=51600 active=2748 piece=itrak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8780 all=51649 active=2797 piece=▁aver\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8800 all=51671 active=2819 piece=arding\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8820 all=51749 active=2660 piece=▁Dafür\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8840 all=51751 active=2662 piece=▁price\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8860 all=51769 active=2680 piece=gebiete\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8880 all=51802 active=2713 piece=▁angeht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8900 all=51805 active=2716 piece=▁island\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8920 all=51817 active=2603 piece=oubtedly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8940 all=51816 active=2602 piece=▁average\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8960 all=51813 active=2599 piece=▁highest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=8980 all=51810 active=2596 piece=vorschlag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9000 all=51811 active=2597 piece=▁deswegen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9020 all=51795 active=2575 piece=▁Kernkraft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9040 all=51791 active=2571 piece=▁geltenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9060 all=51793 active=2573 piece=▁Umstruktur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9080 all=51779 active=2559 piece=▁reconsider\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9100 all=51769 active=2549 piece=▁eingeleitet\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9120 all=51751 active=2571 piece=▁Betrügereien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=9140 all=51732 active=2552 piece=▁Christdemokrat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9160 all=51759 active=2579 piece=ikh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9180 all=51815 active=2635 piece=▁Sc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9200 all=51871 active=2691 piece=isms\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9220 all=51938 active=2659 piece=▁Fäh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9240 all=51974 active=2695 piece=Gebie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9260 all=52039 active=2760 piece=uling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9280 all=52084 active=2805 piece=▁Mann\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9300 all=52104 active=2825 piece=▁inev\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9320 all=52158 active=2658 piece=itizen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9340 all=52169 active=2669 piece=▁Maast\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9360 all=52184 active=2684 piece=▁fried\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9380 all=52190 active=2690 piece=▁towns\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9400 all=52214 active=2714 piece=▁Hatzid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9420 all=52215 active=2611 piece=▁geleis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9440 all=52219 active=2615 piece=▁seiten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9460 all=52230 active=2626 piece=rounding\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9480 all=52226 active=2622 piece=▁anderes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9500 all=52216 active=2612 piece=▁operate\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9520 all=52211 active=2606 piece=glichkeit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9540 all=52215 active=2610 piece=▁decisive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9560 all=52203 active=2598 piece=▁repeated\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9580 all=52209 active=2604 piece=▁Reformpro\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9600 all=52198 active=2593 piece=▁immediate\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9620 all=52182 active=2594 piece=katastrophe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9640 all=52172 active=2584 piece=▁gefährlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9660 all=52159 active=2571 piece=▁Jahrtausend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9680 all=52143 active=2555 piece=▁undertaking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9700 all=52125 active=2537 piece=▁unabhängigen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=9720 all=52107 active=2589 piece=▁ausgezeichneten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9740 all=52164 active=2646 piece=mpt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9760 all=52239 active=2721 piece=ancy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9780 all=52302 active=2784 piece=schl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9800 all=52372 active=2854 piece=▁Mot\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9820 all=52414 active=2659 piece=going\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9840 all=52478 active=2723 piece=raxis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9860 all=52500 active=2745 piece=▁Such\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9880 all=52522 active=2767 piece=▁hull\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9900 all=52551 active=2796 piece=digten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9920 all=52590 active=2665 piece=▁Phase\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9940 all=52602 active=2677 piece=▁dient\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9960 all=52608 active=2683 piece=▁spend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=9980 all=52642 active=2717 piece=samkeit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10000 all=52662 active=2737 piece=▁Surely\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10020 all=52656 active=2628 piece=▁fehlen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10040 all=52656 active=2628 piece=▁sought\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10060 all=52643 active=2615 piece=bewerber\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10080 all=52681 active=2653 piece=▁Budgets\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10100 all=52680 active=2652 piece=▁genannt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10120 all=52671 active=2625 piece=greifende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10140 all=52693 active=2647 piece=▁Sjöstedt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10160 all=52685 active=2639 piece=▁erweiter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10180 all=52680 active=2634 piece=▁regulate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10200 all=52678 active=2632 piece=▁Abkommens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10220 all=52668 active=2624 piece=▁einfachen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10240 all=52656 active=2612 piece=▁unmöglich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10260 all=52645 active=2601 piece=▁britischen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10280 all=52630 active=2586 piece=▁recognises\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10300 all=52617 active=2573 piece=▁Mitgliedern\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10320 all=52603 active=2617 piece=▁hervorragen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10340 all=52590 active=2604 piece=▁Kooperations\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10360 all=52573 active=2587 piece=▁unzureichend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=10380 all=52556 active=2570 piece=▁Constitutional\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10400 all=52599 active=2613 piece=96/\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10420 all=52661 active=2690 piece=rel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10440 all=52729 active=2758 piece=disc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10460 all=52773 active=2802 piece=udem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10480 all=52792 active=2821 piece=▁Okt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10500 all=52823 active=2852 piece=5-002\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10520 all=52876 active=2691 piece=known\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10540 all=52911 active=2726 piece=▁Bill\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10560 all=52928 active=2743 piece=▁Well\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10580 all=52944 active=2759 piece=▁size\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10600 all=52996 active=2811 piece=istern\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10620 all=53037 active=2688 piece=waffen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10640 all=53057 active=2708 piece=▁Marin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10660 all=53058 active=2709 piece=▁displ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10680 all=53072 active=2723 piece=▁sozio\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10700 all=53084 active=2735 piece=ivation\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10720 all=53110 active=2678 piece=▁Formen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10740 all=53102 active=2670 piece=▁attend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10760 all=53106 active=2674 piece=▁looked\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10780 all=53099 active=2667 piece=▁umfaßt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10800 all=53126 active=2694 piece=ordinary\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10820 all=53145 active=2673 piece=▁Jacques\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10840 all=53143 active=2671 piece=▁beziehe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10860 all=53136 active=2664 piece=▁hierfür\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10880 all=53137 active=2665 piece=▁rechnen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10900 all=53149 active=2677 piece=▁Betriebs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10920 all=53141 active=2645 piece=▁bereiten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10940 all=53129 active=2633 piece=▁infringe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10960 all=53115 active=2619 piece=▁strictly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=10980 all=53123 active=2627 piece=▁Mobilität\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11000 all=53119 active=2623 piece=▁erläutert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11020 all=53100 active=2637 piece=▁wichtiges\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11040 all=53096 active=2633 piece=▁Vorschlags\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11060 all=53082 active=2619 piece=▁legitimate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11080 all=53067 active=2604 piece=▁Königreichs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11100 all=53053 active=2590 piece=▁verzeichnen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11120 all=53039 active=2639 piece=▁productivity\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11140 all=53021 active=2621 piece=▁investigation\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=11160 all=53003 active=2603 piece=▁Schlußfolgerung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11180 all=53058 active=2658 piece=kan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11200 all=53121 active=2721 piece=aupt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11220 all=53206 active=2739 piece=kauf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11240 all=53260 active=2793 piece=öten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11260 all=53276 active=2809 piece=▁arg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11280 all=53296 active=2829 piece=Richt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11300 all=53348 active=2881 piece=hrter\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11320 all=53414 active=2731 piece=runde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11340 all=53451 active=2768 piece=▁Herz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11360 all=53465 active=2782 piece=▁bomb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11380 all=53482 active=2799 piece=▁tagt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11400 all=53512 active=2829 piece=irable\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11420 all=53558 active=2720 piece=▁Bedro\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11440 all=53568 active=2730 piece=▁Menge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11460 all=53578 active=2740 piece=▁decre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11480 all=53585 active=2747 piece=▁perce\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11500 all=53598 active=2760 piece=Bericht\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11520 all=53636 active=2717 piece=trieben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11540 all=53641 active=2722 piece=▁Privat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11560 all=53633 active=2714 piece=▁annehm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11580 all=53633 active=2714 piece=▁klären\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11600 all=53621 active=2702 piece=▁sieben\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11620 all=53628 active=2689 piece=ifiziert\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11640 all=53639 active=2700 piece=▁Pfeiler\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11660 all=53640 active=2701 piece=▁collect\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11680 all=53631 active=2692 piece=▁gezielt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11700 all=53620 active=2681 piece=▁perform\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11720 all=53613 active=2672 piece=▁warning\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11740 all=53607 active=2666 piece=▁Beratung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11760 all=53599 active=2658 piece=▁Vielzahl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11780 all=53589 active=2648 piece=▁evidence\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11800 all=53574 active=2633 piece=▁mobility\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11820 all=53565 active=2670 piece=▁versteht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11840 all=53564 active=2669 piece=▁Gerichten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11860 all=53554 active=2659 piece=▁eingehend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11880 all=53539 active=2644 piece=▁outermost\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11900 all=53529 active=2634 piece=▁Argumenten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11920 all=53513 active=2661 piece=▁ausgeführt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11940 all=53493 active=2641 piece=▁philosophy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11960 all=53480 active=2628 piece=▁Ergebnissen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=11980 all=53465 active=2613 piece=▁keinesfalls\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=12000 all=53445 active=2593 piece=▁Futtermittel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=12020 all=53425 active=2653 piece=▁ökonomischen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=12040 all=53405 active=2633 piece=▁verwirklichen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=12060 all=53385 active=2613 piece=▁Sitzungsperiode\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12080 all=53397 active=2625 piece=gy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12100 all=53442 active=2670 piece=arg\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12120 all=53482 active=2707 piece=obb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12140 all=53571 active=2796 piece=äme\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12160 all=53587 active=2812 piece=▁UC\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12180 all=53627 active=2852 piece=east\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12200 all=53671 active=2896 piece=ller\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12220 all=53726 active=2736 piece=tung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12240 all=53755 active=2765 piece=▁Ari\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12260 all=53766 active=2776 piece=▁Ruß\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12280 all=53786 active=2796 piece=▁zit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12300 all=53810 active=2820 piece=hagen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12320 all=53853 active=2732 piece=orary\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12340 all=53905 active=2784 piece=usive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12360 all=53936 active=2815 piece=▁Effe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12380 all=53955 active=2834 piece=▁Stör\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12400 all=53972 active=2851 piece=▁gone\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12420 all=53980 active=2707 piece=(1998)\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12440 all=54001 active=2728 piece=faktor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12460 all=54046 active=2773 piece=städte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12480 all=54053 active=2780 piece=▁Flugh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12500 all=54049 active=2776 piece=▁Sport\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12520 all=54050 active=2704 piece=▁essen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12540 all=54048 active=2702 piece=▁posed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12560 all=54046 active=2700 piece=▁zumal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12580 all=54071 active=2725 piece=reicher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12600 all=54112 active=2766 piece=zogenen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12620 all=54116 active=2707 piece=▁Heimat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12640 all=54111 active=2702 piece=▁Serben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12660 all=54112 active=2703 piece=▁centre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12680 all=54116 active=2707 piece=▁fertig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12700 all=54119 active=2710 piece=▁racial\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12720 all=54120 active=2707 piece=▁verste\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12740 all=54134 active=2721 piece=ikations\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12760 all=54160 active=2747 piece=▁Kapitel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12780 all=54155 active=2742 piece=▁Wissens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12800 all=54146 active=2733 piece=▁factors\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12820 all=54133 active=2695 piece=▁municip\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12840 all=54131 active=2693 piece=▁showing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12860 all=54128 active=2690 piece=leistungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12880 all=54143 active=2705 piece=▁Laufbahn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12900 all=54137 active=2699 piece=▁aufbauen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12920 all=54126 active=2696 piece=▁gebildet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12940 all=54115 active=2685 piece=▁schließt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12960 all=54114 active=2684 piece=▁ähnliche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=12980 all=54111 active=2681 piece=▁Certainly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13000 all=54099 active=2669 piece=▁Rechtfert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13020 all=54084 active=2689 piece=▁betreiben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13040 all=54067 active=2672 piece=▁gefährden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13060 all=54049 active=2654 piece=▁undermine\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13080 all=54040 active=2645 piece=▁Dokumenten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13100 all=54024 active=2629 piece=▁appelliere\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13120 all=54011 active=2689 piece=▁inevitably\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13140 all=53991 active=2669 piece=▁tremendous\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13160 all=53973 active=2651 piece=▁Instruments\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13180 all=53956 active=2634 piece=▁earthquakes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13200 all=53941 active=2619 piece=▁potentielle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13220 all=53924 active=2681 piece=▁Insbesondere\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13240 all=53907 active=2664 piece=▁fortzusetzen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13260 all=53889 active=2646 piece=▁Österreicher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13280 all=53872 active=2629 piece=▁eingebrachten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13300 all=53854 active=2611 piece=▁Wiederaufnahme\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=13320 all=53835 active=2674 piece=▁Mitgliedsländer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13340 all=53852 active=2691 piece=ez\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13360 all=53896 active=2735 piece=dds\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13380 all=53954 active=2793 piece=▁4.\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13400 all=53970 active=2809 piece=5-02\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13420 all=54010 active=2737 piece=held\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13440 all=54053 active=2780 piece=ores\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13460 all=54097 active=2824 piece=wind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13480 all=54128 active=2855 piece=▁Frü\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13500 all=54143 active=2870 piece=▁ast\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13520 all=54162 active=2725 piece=▁ört\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13540 all=54195 active=2758 piece=höhen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13560 all=54241 active=2804 piece=ptive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13580 all=54267 active=2830 piece=ässer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13600 all=54260 active=2823 piece=▁Mart\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13620 all=54268 active=2720 piece=▁conj\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13640 all=54272 active=2724 piece=▁heik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13660 all=54277 active=2729 piece=▁ster\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13680 all=54292 active=2744 piece=banken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13700 all=54332 active=2784 piece=jenige\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13720 all=54363 active=2746 piece=ruhest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13740 all=54390 active=2773 piece=▁Amtes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13760 all=54384 active=2767 piece=▁Korru\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13780 all=54387 active=2770 piece=▁birgt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13800 all=54389 active=2772 piece=▁hohes\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13820 all=54393 active=2724 piece=▁shape\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13840 all=54398 active=2729 piece=Article\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13860 all=54417 active=2748 piece=laubnis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13880 all=54446 active=2777 piece=▁Before\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13900 all=54445 active=2776 piece=▁KULTUR\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13920 all=54442 active=2720 piece=▁adhere\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13940 all=54451 active=2729 piece=▁einzum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13960 all=54438 active=2716 piece=▁lowest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=13980 all=54428 active=2706 piece=▁tragic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14000 all=54431 active=2709 piece=ensystem\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14020 all=54449 active=2738 piece=▁Ansehen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14040 all=54438 active=2727 piece=▁Konkurr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14060 all=54429 active=2718 piece=▁Umfangs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14080 all=54415 active=2704 piece=▁billigt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14100 all=54408 active=2697 piece=▁erfreut\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14120 all=54395 active=2708 piece=▁immense\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14140 all=54385 active=2698 piece=▁prinzip\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14160 all=54377 active=2690 piece=▁thirdly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14180 all=54371 active=2684 piece=elessness\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14200 all=54377 active=2690 piece=▁Decision\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14220 all=54373 active=2715 piece=▁Selbstge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14240 all=54359 active=2701 piece=▁combined\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14260 all=54345 active=2687 piece=▁findings\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14280 all=54330 active=2672 piece=▁modernen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14300 all=54320 active=2662 piece=▁struktur\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14320 all=54319 active=2711 piece=initiative\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14340 all=54314 active=2706 piece=▁Erzeugung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14360 all=54305 active=2697 piece=▁Vorbehalt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14380 all=54289 active=2681 piece=▁conflicts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14400 all=54280 active=2672 piece=▁höchstens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14420 all=54264 active=2698 piece=▁penalties\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14440 all=54251 active=2685 piece=▁schneller\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14460 all=54241 active=2675 piece=▁zerstören\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14480 all=54232 active=2666 piece=▁Innovation\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14500 all=54221 active=2655 piece=▁ansprechen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14520 all=54211 active=2702 piece=▁dependency\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14540 all=54199 active=2690 piece=▁incentives\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14560 all=54180 active=2671 piece=▁referendum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14580 all=54163 active=2654 piece=▁vorherigen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14600 all=54148 active=2639 piece=▁Anmerkungen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14620 all=54132 active=2692 piece=▁Wortmeldung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14640 all=54115 active=2675 piece=▁einzuräumen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14660 all=54095 active=2655 piece=▁reformieren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14680 all=54078 active=2638 piece=▁Auffassungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14700 all=54061 active=2621 piece=▁Unionsbürger\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14720 all=54042 active=2685 piece=▁explanations\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14740 all=54025 active=2668 piece=▁verschwinden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14760 all=54007 active=2650 piece=▁Verteidigungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14780 all=53989 active=2632 piece=▁weitestgehend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14800 all=53970 active=2613 piece=▁interregionale\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=14820 all=53950 active=2679 piece=▁bereitzustellen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14840 all=53974 active=2703 piece=FL\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14860 all=54001 active=2730 piece=so\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14880 all=54036 active=2765 piece=Der\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14900 all=54064 active=2793 piece=enk\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14920 all=54117 active=2751 piece=olk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14940 all=54158 active=2792 piece=äle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14960 all=54167 active=2801 piece=▁83\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=14980 all=54182 active=2816 piece=1997\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15000 all=54205 active=2839 piece=buch\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15020 all=54244 active=2745 piece=iano\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15040 all=54284 active=2785 piece=owis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15060 all=54308 active=2809 piece=urse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15080 all=54323 active=2824 piece=▁250\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15100 all=54324 active=2825 piece=▁Cux\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15120 all=54328 active=2720 piece=▁Mun\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15140 all=54336 active=2728 piece=▁Wür\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15160 all=54345 active=2737 piece=▁leb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15180 all=54363 active=2755 piece=Freie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15200 all=54384 active=2776 piece=ayann\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15220 all=54413 active=2747 piece=föder\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15240 all=54440 active=2774 piece=limin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15260 all=54465 active=2799 piece=suche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15280 all=54494 active=2828 piece=▁Bele\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15300 all=54496 active=2830 piece=▁Krit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15320 all=54503 active=2731 piece=▁Rett\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15340 all=54506 active=2734 piece=▁aufs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15360 all=54510 active=2738 piece=▁gift\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15380 all=54515 active=2743 piece=▁race\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15400 all=54522 active=2750 piece=▁würd\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15420 all=54542 active=2743 piece=antien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15440 all=54569 active=2770 piece=gerade\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15460 all=54600 active=2801 piece=iteten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15480 all=54629 active=2830 piece=terung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15500 all=54669 active=2870 piece=ührend\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15520 all=54663 active=2727 piece=▁EAGFL\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15540 all=54659 active=2723 piece=▁Kapaz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15560 all=54659 active=2723 piece=▁Reise\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15580 all=54650 active=2714 piece=▁Waren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15600 all=54653 active=2717 piece=▁embar\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15620 all=54656 active=2733 piece=▁imply\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15640 all=54656 active=2733 piece=▁pilot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15660 all=54654 active=2731 piece=▁strug\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15680 all=54655 active=2732 piece=4/2000)\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15700 all=54679 active=2756 piece=gründen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15720 all=54709 active=2762 piece=schwung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15740 all=54727 active=2780 piece=▁Ariane\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15760 all=54725 active=2778 piece=▁Gegner\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15780 all=54715 active=2768 piece=▁Mengen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15800 all=54707 active=2760 piece=▁Sprung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15820 all=54701 active=2729 piece=▁arrive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15840 all=54703 active=2731 piece=▁ecolog\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15860 all=54696 active=2724 piece=▁grünes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15880 all=54688 active=2716 piece=▁merger\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15900 all=54685 active=2713 piece=▁stoßen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15920 all=54684 active=2734 piece=▁wächst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15940 all=54693 active=2743 piece=gewichte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15960 all=54713 active=2763 piece=streicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=15980 all=54702 active=2752 piece=▁Eurodac\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16000 all=54697 active=2747 piece=▁Kräften\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16020 all=54690 active=2728 piece=▁Umstand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16040 all=54680 active=2718 piece=▁backing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16060 all=54670 active=2708 piece=▁divided\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16080 all=54658 active=2696 piece=▁exploit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16100 all=54641 active=2679 piece=▁hundred\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16120 all=54631 active=2723 piece=▁popular\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16140 all=54614 active=2706 piece=▁tragedy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16160 all=54608 active=2700 piece=behaltlos\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16180 all=54623 active=2715 piece=ränkungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16200 all=54631 active=2723 piece=▁Bereichs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16220 all=54617 active=2718 piece=▁Industry\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16240 all=54605 active=2706 piece=▁Schatten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16260 all=54592 active=2693 piece=▁anfangen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16280 all=54587 active=2688 piece=▁compiled\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16300 all=54572 active=2673 piece=▁distingu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16320 all=54556 active=2711 piece=▁frontier\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16340 all=54547 active=2702 piece=▁mehrmals\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16360 all=54529 active=2684 piece=▁regarded\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16380 all=54512 active=2667 piece=▁standing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16400 all=54507 active=2662 piece=▁vorgeben\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16420 all=54504 active=2723 piece=haltigkeit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16440 all=54502 active=2721 piece=▁Eisenbahn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16460 all=54493 active=2712 piece=▁Quaestors\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16480 all=54482 active=2701 piece=▁accepting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16500 all=54463 active=2682 piece=▁delegates\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16520 all=54455 active=2716 piece=▁fragility\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16540 all=54439 active=2700 piece=▁kostspiel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16560 all=54425 active=2686 piece=▁schnellen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16580 all=54410 active=2671 piece=▁verzerren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16600 all=54391 active=2652 piece=▁übernimmt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16620 all=54375 active=2704 piece=▁Amerikaner\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16640 all=54360 active=2689 piece=▁Memorandum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16660 all=54345 active=2674 piece=▁angefangen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16680 all=54330 active=2659 piece=▁besonderer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16700 all=54311 active=2640 piece=▁entgegenzu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16720 all=54298 active=2703 piece=▁indirectly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16740 all=54282 active=2687 piece=▁prescribed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16760 all=54263 active=2668 piece=▁umstritten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16780 all=54249 active=2654 piece=▁zweckmäßig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16800 all=54236 active=2641 piece=▁Altfahrzeug\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16820 all=54221 active=2696 piece=▁Materialien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16840 all=54202 active=2677 piece=▁amtierender\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16860 all=54185 active=2660 piece=▁dargestellt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16880 all=54165 active=2640 piece=▁hervorgehen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16900 all=54146 active=2621 piece=▁rückwirkend\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16920 all=54133 active=2695 piece=unktionalität\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16940 all=54118 active=2680 piece=▁Mitarbeitern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16960 all=54098 active=2660 piece=▁ausschließen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=16980 all=54080 active=2642 piece=▁gerichtliche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17000 all=54061 active=2623 piece=▁unterbrochen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17020 all=54048 active=2690 piece=▁Erscheinungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17040 all=54028 active=2670 piece=▁Untersuchungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17060 all=54008 active=2650 piece=▁entschuldigen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17080 all=53988 active=2630 piece=angelegenheiten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17100 all=53968 active=2610 piece=▁comprehensible\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=17120 all=53948 active=2679 piece=▁Gesamtfangmenge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17140 all=53934 active=2665 piece=Er\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17160 all=53960 active=2691 piece=aca\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17180 all=53994 active=2725 piece=iki\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17200 all=54031 active=2762 piece=▁(1\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17220 all=54045 active=2714 piece=,000\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17240 all=54072 active=2741 piece=chit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17260 all=54098 active=2767 piece=irbt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17280 all=54120 active=2789 piece=reht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17300 all=54143 active=2812 piece=zähl\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17320 all=54175 active=2737 piece=ofar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17340 all=54184 active=2746 piece=▁22,\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17360 all=54182 active=2744 piece=▁Kum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17380 all=54197 active=2759 piece=▁bla\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17400 all=54206 active=2768 piece=▁rus\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17420 all=54219 active=2723 piece=▁ord\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17440 all=54233 active=2737 piece=ampen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17460 all=54258 active=2762 piece=dende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17480 all=54274 active=2778 piece=iefly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17500 all=54293 active=2797 piece=lehrt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17520 all=54323 active=2743 piece=▁Ans\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17540 all=54339 active=2759 piece=lösen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17560 all=54360 active=2780 piece=reckt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17580 all=54392 active=2812 piece=uität\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17600 all=54413 active=2833 piece=▁81(3\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17620 all=54437 active=2743 piece=quem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17640 all=54457 active=2763 piece=▁Büro\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17660 all=54462 active=2768 piece=▁Kauk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17680 all=54465 active=2771 piece=▁Symp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17700 all=54466 active=2772 piece=▁dens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17720 all=54487 active=2743 piece=▁ewig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17740 all=54487 active=2743 piece=▁mild\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17760 all=54489 active=2745 piece=▁soft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17780 all=54495 active=2751 piece=Israel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17800 all=54509 active=2765 piece=attete\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17820 all=54530 active=2745 piece=fuhren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17840 all=54550 active=2765 piece=illage\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17860 all=54570 active=2785 piece=meldes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17880 all=54581 active=2796 piece=region\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17900 all=54609 active=2824 piece=wärtig\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17920 all=54612 active=2733 piece=▁Among\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17940 all=54608 active=2729 piece=▁Flamm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17960 all=54602 active=2723 piece=▁Neues\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=17980 all=54599 active=2720 piece=▁Women\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18000 all=54591 active=2712 piece=▁bread\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18020 all=54592 active=2730 piece=▁false\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18040 all=54584 active=2722 piece=▁hulls\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18060 all=54594 active=2732 piece=▁marks\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18080 all=54592 active=2730 piece=▁scene\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18100 all=54583 active=2721 piece=▁unbef\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18120 all=54586 active=2732 piece=▁womit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18140 all=54595 active=2741 piece=beugung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18160 all=54603 active=2749 piece=hörlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18180 all=54621 active=2767 piece=nehmung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18200 all=54633 active=2779 piece=stitute\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18220 all=54657 active=2755 piece=zählung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18240 all=54652 active=2750 piece=▁Firmen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18260 all=54652 active=2750 piece=▁Kammer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18280 all=54643 active=2741 piece=▁Result\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18300 all=54634 active=2732 piece=▁Zeilen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18320 all=54634 active=2732 piece=▁begins\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18340 all=54621 active=2719 piece=▁deplor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18360 all=54611 active=2709 piece=▁fossil\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18380 all=54598 active=2696 piece=▁hypoth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18400 all=54605 active=2703 piece=▁murder\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18420 all=54596 active=2720 piece=▁relies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18440 all=54589 active=2713 piece=▁strebt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18460 all=54587 active=2711 piece=▁vessel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18480 all=54575 active=2699 piece=aneously\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18500 all=54598 active=2722 piece=inquency\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18520 all=54612 active=2743 piece=stockung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18540 all=54624 active=2755 piece=▁António\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18560 all=54615 active=2746 piece=▁Fischer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18580 all=54613 active=2744 piece=▁Kulture\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18600 all=54604 active=2735 piece=▁Russian\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18620 all=54595 active=2722 piece=▁Verlaub\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18640 all=54577 active=2704 piece=▁anzusch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18660 all=54569 active=2696 piece=▁blicken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18680 all=54551 active=2678 piece=▁diverse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18700 all=54536 active=2663 piece=▁fashion\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18720 all=54517 active=2708 piece=▁homogen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18740 all=54510 active=2701 piece=▁letztes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18760 all=54499 active=2690 piece=▁pretext\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18780 all=54479 active=2670 piece=▁specify\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18800 all=54471 active=2662 piece=▁unrecht\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18820 all=54470 active=2722 piece=▁worries\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18840 all=54470 active=2722 piece=enteilung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18860 all=54483 active=2735 piece=potential\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18880 all=54483 active=2735 piece=▁Caucasus\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18900 all=54472 active=2724 piece=▁Mischung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18920 all=54458 active=2710 piece=▁Trennung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18940 all=54449 active=2701 piece=▁angelegt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18960 all=54435 active=2687 piece=▁beschloß\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=18980 all=54422 active=2674 piece=▁definite\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19000 all=54403 active=2655 piece=▁entzieht\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19020 all=54386 active=2704 piece=▁gedenken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19040 all=54374 active=2692 piece=▁honoured\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19060 all=54362 active=2680 piece=▁limiting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19080 all=54348 active=2666 piece=▁postpone\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19100 all=54328 active=2646 piece=▁sichtbar\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19120 all=54310 active=2699 piece=▁unakzept\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19140 all=54299 active=2688 piece=▁zulässig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19160 all=54297 active=2686 piece=restricted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19180 all=54297 active=2686 piece=▁Beiträgen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19200 all=54282 active=2671 piece=▁Geometrie\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19220 all=54265 active=2698 piece=▁Positives\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19240 all=54248 active=2681 piece=▁Steuervor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19260 all=54234 active=2667 piece=▁angesehen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19280 all=54215 active=2648 piece=▁buildings\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19300 all=54203 active=2636 piece=▁ehemalige\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19320 all=54185 active=2693 piece=▁exclusive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19340 all=54166 active=2674 piece=▁guideline\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19360 all=54147 active=2655 piece=▁katastrop\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19380 all=54128 active=2636 piece=▁president\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19400 all=54108 active=2616 piece=▁sceptical\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19420 all=54091 active=2689 piece=▁südlichen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19440 all=54076 active=2674 piece=▁vollendet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19460 all=54063 active=2661 piece=freundliche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19480 all=54047 active=2645 piece=▁Behinderte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19500 all=54030 active=2628 piece=▁Konkurrenz\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19520 all=54010 active=2682 piece=▁Sichtweise\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19540 all=53992 active=2664 piece=▁accounting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19560 all=53973 active=2645 piece=▁begonnenen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19580 all=53954 active=2626 piece=▁concentric\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19600 all=53934 active=2606 piece=▁disturbing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19620 all=53916 active=2679 piece=▁genommenen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19640 all=53898 active=2661 piece=▁ländlicher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19660 all=53881 active=2644 piece=▁presidents\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19680 all=53864 active=2627 piece=▁spanischer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19700 all=53846 active=2609 piece=▁versichert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19720 all=53829 active=2676 piece=verwaltungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19740 all=53812 active=2659 piece=▁Finanzielle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19760 all=53795 active=2642 piece=▁Rückwirkung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19780 all=53779 active=2626 piece=▁arbeitenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19800 all=53760 active=2607 piece=▁concessions\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19820 all=53741 active=2669 piece=▁endeavoured\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19840 all=53721 active=2649 piece=▁hinnehmbare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19860 all=53701 active=2629 piece=▁observatory\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19880 all=53682 active=2610 piece=▁schädlicher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19900 all=53665 active=2593 piece=▁unterstellt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19920 all=53649 active=2668 piece=schlossenheit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19940 all=53632 active=2651 piece=▁Fortschritts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19960 all=53614 active=2633 piece=▁Steuerfragen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=19980 all=53596 active=2615 piece=▁apparatchiks\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20000 all=53579 active=2598 piece=▁eingegangene\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20020 all=53562 active=2662 piece=▁perpetrators\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20040 all=53542 active=2642 piece=▁unterrichtet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20060 all=53522 active=2622 piece=ungsaustauschs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20080 all=53504 active=2604 piece=▁Griechenlands\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20100 all=53487 active=2587 piece=▁Transnational\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20120 all=53468 active=2655 piece=▁certification\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20140 all=53448 active=2635 piece=▁interpretiert\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20160 all=53428 active=2615 piece=▁vorgebrachten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20180 all=53408 active=2595 piece=▁Mitgliedschaft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20200 all=53388 active=2575 piece=▁demonstrations\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20220 all=53368 active=2650 piece=▁transportieren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20240 all=53348 active=2630 piece=▁Fahrzeugbestand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20260 all=53328 active=2610 piece=▁dementsprechend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20280 all=53313 active=2595 piece=ec\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20300 all=53327 active=2609 piece=EPs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20320 all=53336 active=2675 piece=ynn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20340 all=53343 active=2682 piece=5/95\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20360 all=53347 active=2686 piece=digt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20380 all=53361 active=2700 piece=ogel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20400 all=53377 active=2716 piece=ölle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20420 all=53379 active=2669 piece=▁24.\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20440 all=53364 active=2654 piece=▁CFI\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20460 all=53353 active=2643 piece=▁VEU\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20480 all=53357 active=2647 piece=/60/9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20500 all=53360 active=2650 piece=afran\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20520 all=53371 active=2678 piece=arrel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20540 all=53381 active=2688 piece=ermis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20560 all=53395 active=2702 piece=latte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20580 all=53405 active=2712 piece=sense\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20600 all=53414 active=2721 piece=örung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20620 all=53417 active=2673 piece=▁Aspe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20640 all=53412 active=2668 piece=▁Goll\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20660 all=53412 active=2668 piece=▁Rojo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20680 all=53404 active=2660 piece=▁boyc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20700 all=53404 active=2660 piece=▁inex\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20720 all=53414 active=2679 piece=▁nuts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20740 all=53410 active=2675 piece=4-0018\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20760 all=53409 active=2674 piece=ackson\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20780 all=53419 active=2684 piece=elodie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20800 all=53432 active=2697 piece=iametr\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20820 all=53442 active=2679 piece=ilitar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20840 all=53455 active=2692 piece=loying\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20860 all=53470 active=2707 piece=roused\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20880 all=53479 active=2716 piece=würfen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20900 all=53473 active=2710 piece=▁Akten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20920 all=53481 active=2681 piece=5-0007\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20940 all=53482 active=2682 piece=▁Chevè\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20960 all=53472 active=2672 piece=▁Hätte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=20980 all=53458 active=2658 piece=▁Nazis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21000 all=53446 active=2646 piece=▁Stoff\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21020 all=53448 active=2674 piece=▁soph\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21040 all=53449 active=2675 piece=▁Uzbek\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21060 all=53447 active=2673 piece=▁anzup\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21080 all=53440 active=2666 piece=▁disgu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21100 all=53434 active=2660 piece=▁grüne\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21120 all=53435 active=2673 piece=ahndet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21140 all=53434 active=2672 piece=▁loose\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21160 all=53426 active=2664 piece=▁quiet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21180 all=53423 active=2661 piece=▁terat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21200 all=53414 active=2652 piece=Suffolk\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21220 all=53416 active=2673 piece=▁earm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21240 all=53417 active=2674 piece=arantee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21260 all=53428 active=2685 piece=etitive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21280 all=53438 active=2695 piece=ilegien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21300 all=53449 active=2706 piece=merkmal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21320 all=53454 active=2675 piece=▁army\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21340 all=53454 active=2675 piece=▁versö\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21360 all=53459 active=2680 piece=reating\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21380 all=53467 active=2688 piece=trading\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21400 all=53478 active=2699 piece=ächtige\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21420 all=53485 active=2679 piece=▁Gale\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21440 all=53485 active=2679 piece=▁petty\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21460 all=53488 active=2682 piece=▁Ankara\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21480 all=53481 active=2675 piece=▁Ermord\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21500 all=53471 active=2665 piece=▁Kernen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21520 all=53475 active=2677 piece=istin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21540 all=53478 active=2680 piece=geschra\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21560 all=53475 active=2677 piece=▁Mexiko\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21580 all=53464 active=2666 piece=▁Postul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21600 all=53459 active=2661 piece=▁Taking\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21620 all=53461 active=2675 piece=klären\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21640 all=53467 active=2681 piece=▁Tauern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21660 all=53457 active=2671 piece=▁apathy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21680 all=53445 active=2659 piece=▁devour\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21700 all=53432 active=2646 piece=▁forged\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21720 all=53441 active=2681 piece=▁Beleg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21740 all=53440 active=2680 piece=▁Innere\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21760 all=53432 active=2672 piece=▁hurdle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21780 all=53426 active=2666 piece=▁lovers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21800 all=53416 active=2656 piece=▁reiche\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21820 all=53424 active=2679 piece=peist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21840 all=53429 active=2684 piece=▁gutem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21860 all=53426 active=2681 piece=▁reopen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21880 all=53414 active=2669 piece=▁sounds\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21900 all=53411 active=2666 piece=▁uneven\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21920 all=53419 active=2678 piece=▁exod\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21940 all=53422 active=2681 piece=züchter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21960 all=53412 active=2671 piece=▁vorzug\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=21980 all=53399 active=2658 piece=Struktur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22000 all=53411 active=2670 piece=ceivable\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22020 all=53420 active=2679 piece=obles\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22040 all=53420 active=2679 piece=▁Lanka\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22060 all=53411 active=2670 piece=▁marker\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22080 all=53414 active=2673 piece=gehenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22100 all=53421 active=2680 piece=liegende\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22120 all=53429 active=2678 piece=worth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22140 all=53426 active=2675 piece=▁umtre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22160 all=53426 active=2675 piece=ragender\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22180 all=53432 active=2681 piece=ssystems\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22200 all=53435 active=2684 piece=widrigen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22220 all=53453 active=2688 piece=thaus\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22240 all=53456 active=2691 piece=▁weilt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22260 all=53452 active=2687 piece=▁94/728/\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22280 all=53437 active=2672 piece=▁Brunnen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22300 all=53425 active=2660 piece=▁Finance\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22320 all=53434 active=2681 piece=▁seiz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22340 all=53438 active=2685 piece=tretens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22360 all=53428 active=2675 piece=▁Gefälle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22380 all=53414 active=2661 piece=▁Looking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22400 all=53407 active=2654 piece=▁Protest\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22420 all=53419 active=2682 piece=▁nerv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22440 all=53421 active=2684 piece=reifens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22460 all=53425 active=2688 piece=▁Journal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22480 all=53409 active=2672 piece=▁Studium\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22500 all=53400 active=2663 piece=▁Vorgabe\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22520 all=53410 active=2680 piece=eiting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22540 all=53412 active=2682 piece=Staudam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22560 all=53412 active=2682 piece=stellers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22580 all=53397 active=2667 piece=▁aroused\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22600 all=53385 active=2655 piece=▁benannt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22620 all=53396 active=2681 piece=räder\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22640 all=53393 active=2678 piece=▁ärmer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22660 all=53385 active=2670 piece=beratung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22680 all=53371 active=2656 piece=▁catches\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22700 all=53352 active=2637 piece=▁devolve\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22720 all=53355 active=2671 piece=▁332,\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22740 all=53349 active=2665 piece=enommen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22760 all=53348 active=2664 piece=▁durchle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22780 all=53339 active=2655 piece=▁expires\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22800 all=53324 active=2640 piece=▁gekürzt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22820 all=53333 active=2676 piece=▁hege\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22840 all=53332 active=2675 piece=▁Inform\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22860 all=53325 active=2668 piece=▁gespart\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22880 all=53312 active=2655 piece=▁infiltr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22900 all=53301 active=2644 piece=▁liaison\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22920 all=53303 active=2668 piece=▁ital\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22940 all=53300 active=2665 piece=▁Attent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22960 all=53288 active=2653 piece=▁mancher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=22980 all=53273 active=2638 piece=▁opacity\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23000 all=53261 active=2626 piece=▁ranging\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23020 all=53268 active=2671 piece=ourge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23040 all=53264 active=2667 piece=▁dankt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23060 all=53267 active=2670 piece=▁Compens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23080 all=53257 active=2660 piece=▁scourge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23100 all=53242 active=2645 piece=▁strikes\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23120 all=53248 active=2669 piece=eyhun\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23140 all=53251 active=2672 piece=flöcher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23160 all=53244 active=2665 piece=▁suppose\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23180 all=53231 active=2652 piece=▁ungünst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23200 all=53220 active=2641 piece=▁zuwenig\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23220 all=53217 active=2658 piece=▁disg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23240 all=53217 active=2658 piece=▁rührt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23260 all=53211 active=2652 piece=▁wieviel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23280 all=53201 active=2642 piece=Staudamms\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23300 all=53208 active=2649 piece=eidenheit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23320 all=53226 active=2678 piece=▁ally\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23340 all=53225 active=2677 piece=▁Sobald\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23360 all=53221 active=2673 piece=führenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23380 all=53225 active=2677 piece=istellung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23400 all=53222 active=2674 piece=tegration\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23420 all=53234 active=2672 piece=▁Star\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23440 all=53229 active=2667 piece=▁karib\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23460 all=53222 active=2660 piece=venience\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23480 all=53222 active=2660 piece=verhütung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23500 all=53229 active=2667 piece=▁Alicante\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23520 all=53226 active=2659 piece=▁Spur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23540 all=53228 active=2661 piece=▁Vandam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23560 all=53212 active=2645 piece=▁Ambition\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23580 all=53197 active=2630 piece=▁Bosnians\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23600 all=53182 active=2615 piece=▁Embargos\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23620 all=53196 active=2674 piece=▁Dipl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23640 all=53188 active=2666 piece=▁Casaca\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23660 all=53176 active=2654 piece=▁Entgegen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23680 all=53162 active=2640 piece=▁Hunderte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23700 all=53146 active=2624 piece=▁Lorraine\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23720 all=53160 active=2672 piece=▁Ehr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23740 all=53165 active=2677 piece=▁hieße\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23760 all=53149 active=2661 piece=chanismus\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23780 all=53134 active=2646 piece=▁PREUSSAG\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23800 all=53120 active=2632 piece=▁Seifenbl\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23820 all=53130 active=2665 piece=▁Haß\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23840 all=53132 active=2667 piece=▁Heran\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23860 all=53121 active=2656 piece=▁crystal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23880 all=53108 active=2643 piece=▁Unwetter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23900 all=53093 active=2628 piece=▁ablaufen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23920 all=53101 active=2663 piece=▁embe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23940 all=53095 active=2657 piece=▁élites\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23960 all=53081 active=2643 piece=▁allotted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=23980 all=53065 active=2627 piece=▁aufweist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24000 all=53045 active=2607 piece=▁besseres\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24020 all=53058 active=2666 piece=▁pas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24040 all=53060 active=2668 piece=ifiques\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24060 all=53054 active=2662 piece=▁bestände\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24080 all=53035 active=2643 piece=▁comprise\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24100 all=53024 active=2632 piece=▁electing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24120 all=53035 active=2663 piece=▁1917\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24140 all=53025 active=2653 piece=▁Halten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24160 all=53010 active=2638 piece=▁täglich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24180 all=52993 active=2621 piece=▁ereignet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24200 all=52976 active=2604 piece=▁falscher\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24220 all=52980 active=2653 piece=orten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24240 all=52995 active=2668 piece=▁beding\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24260 all=52984 active=2657 piece=▁festlegt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24280 all=52969 active=2642 piece=▁geradezu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24300 all=52952 active=2625 piece=▁headlong\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24320 all=52950 active=2646 piece=▁hearings\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24340 all=52937 active=2633 piece=▁invested\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24360 all=52922 active=2618 piece=▁mainland\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24380 all=52904 active=2600 piece=▁ordinary\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24400 all=52889 active=2585 piece=▁probable\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24420 all=52874 active=2630 piece=▁schwächt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24440 all=52858 active=2614 piece=▁stopping\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24460 all=52845 active=2601 piece=▁underpin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24480 all=52831 active=2587 piece=▁vertieft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24500 all=52816 active=2572 piece=▁Überdies\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24520 all=52798 active=2623 piece=-0808/99):\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24540 all=52789 active=2614 piece=demokraten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24560 all=52782 active=2607 piece=punktmäßig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24580 all=52780 active=2605 piece=▁Alexander\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24600 all=52764 active=2589 piece=▁Champagne\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24620 all=52746 active=2621 piece=▁Europarat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24640 all=52729 active=2604 piece=▁Höhepunkt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24660 all=52710 active=2585 piece=▁Lockerung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24680 all=52694 active=2569 piece=▁Political\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24700 all=52677 active=2552 piece=▁Steuerung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24720 all=52659 active=2616 piece=▁abstained\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24740 all=52639 active=2596 piece=▁anzuhören\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24760 all=52621 active=2578 piece=▁balancing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24780 all=52601 active=2558 piece=▁conferred\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24800 all=52581 active=2538 piece=▁disposing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24820 all=52569 active=2618 piece=▁entailing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24840 all=52551 active=2600 piece=▁ethnische\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24860 all=52533 active=2582 piece=▁gelungene\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24880 all=52513 active=2562 piece=▁hurricane\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24900 all=52495 active=2544 piece=▁jahrelang\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24920 all=52479 active=2609 piece=▁mißbräuch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24940 all=52465 active=2595 piece=▁polluters\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24960 all=52448 active=2578 piece=▁reinstate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=24980 all=52433 active=2563 piece=▁shoulders\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25000 all=52413 active=2543 piece=▁technisch\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25020 all=52395 active=2603 piece=▁unterhalb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25040 all=52379 active=2587 piece=▁verziehen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25060 all=52368 active=2576 piece=▁Übersicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25080 all=52349 active=2557 piece=bahnunglück\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25100 all=52341 active=2549 piece=rukturellen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25120 all=52327 active=2604 piece=▁1999/2124(\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25140 all=52308 active=2585 piece=▁Auswertung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25160 all=52290 active=2567 piece=▁Durchbruch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25180 all=52273 active=2550 piece=▁Industriel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25200 all=52255 active=2532 piece=▁Minireform\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25220 all=52236 active=2594 piece=▁Scheideweg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25240 all=52218 active=2576 piece=▁Ultimately\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25260 all=52204 active=2562 piece=▁abgelaufen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25280 all=52185 active=2543 piece=▁angedrohte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25300 all=52165 active=2523 piece=▁ausgedehnt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25320 all=52145 active=2589 piece=▁beschämend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25340 all=52126 active=2570 piece=▁controlled\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25360 all=52107 active=2551 piece=▁eingehende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25380 all=52088 active=2532 piece=▁extinguish\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25400 all=52069 active=2513 piece=▁gewünschte\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25420 all=52051 active=2586 piece=▁ingredient\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25440 all=52031 active=2566 piece=▁natürliche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25460 all=52011 active=2546 piece=▁profession\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25480 all=51992 active=2527 piece=▁schlüssige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25500 all=51974 active=2509 piece=▁tightening\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25520 all=51957 active=2582 piece=▁vergleicht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25540 all=51940 active=2565 piece=▁wichtigere\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25560 all=51922 active=2547 piece=5-0004/2000)\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25580 all=51904 active=2529 piece=gebeschlüsse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25600 all=51890 active=2515 piece=vereinbarung\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25620 all=51871 active=2576 piece=▁Chamberlain\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25640 all=51851 active=2556 piece=▁Hauptstädte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25660 all=51833 active=2538 piece=▁Lebensdauer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25680 all=51813 active=2518 piece=▁Resolutions\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25700 all=51793 active=2498 piece=▁Unterscheid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25720 all=51780 active=2575 piece=▁Zuwanderung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25740 all=51762 active=2557 piece=▁aufenthalts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25760 all=51742 active=2537 piece=▁challenging\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25780 all=51723 active=2518 piece=▁degradieren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25800 all=51703 active=2498 piece=▁eingenommen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25820 all=51684 active=2566 piece=▁erschließen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25840 all=51664 active=2546 piece=▁geschätzten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25860 all=51646 active=2528 piece=▁irgendeinen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25880 all=51626 active=2508 piece=▁mehrjährige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25900 all=51607 active=2489 piece=▁parteipolit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25920 all=51587 active=2561 piece=▁reconciling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25940 all=51568 active=2542 piece=▁structuring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25960 all=51550 active=2524 piece=▁unzulässige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=25980 all=51531 active=2505 piece=▁wherewithal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26000 all=51513 active=2487 piece=geschlossenen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26020 all=51493 active=2556 piece=▁Befriedigung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26040 all=51475 active=2538 piece=▁Drogenhandel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26060 all=51457 active=2520 piece=▁Hauptaufgabe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26080 all=51437 active=2500 piece=▁Redebeiträge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26100 all=51418 active=2481 piece=▁Ungleichheit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26120 all=51398 active=2551 piece=▁angeprangert\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26140 all=51378 active=2531 piece=▁begünstigten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26160 all=51359 active=2512 piece=▁dissatisfied\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26180 all=51339 active=2492 piece=▁gigantischer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26200 all=51319 active=2472 piece=▁indifference\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26220 all=51300 active=2547 piece=▁medizinische\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26240 all=51280 active=2527 piece=▁präsentieren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26260 all=51260 active=2507 piece=▁statistische\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26280 all=51242 active=2489 piece=▁unreasonable\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26300 all=51222 active=2469 piece=▁vorzubringen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26320 all=51202 active=2542 piece=constitutional\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26340 all=51182 active=2522 piece=▁Energieformen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26360 all=51162 active=2502 piece=▁Gewinnspannen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26380 all=51142 active=2482 piece=▁Meinungsunter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26400 all=51122 active=2462 piece=▁Umweltschäden\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26420 all=51103 active=2538 piece=▁anzuschließen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26440 all=51083 active=2518 piece=▁disappointing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26460 all=51063 active=2498 piece=▁hinausgehende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26480 all=51044 active=2479 piece=▁nachgeordnete\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26500 all=51024 active=2459 piece=▁sophisticated\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26520 all=51005 active=2533 piece=▁unterbrochene\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26540 all=50986 active=2514 piece=▁zuwiderlaufen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26560 all=50966 active=2494 piece=▁Diamantopoulou\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26580 all=50946 active=2474 piece=▁Kernprinzipien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26600 all=50926 active=2454 piece=▁Rechtsextremen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26620 all=50906 active=2527 piece=▁Unterstützungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26640 all=50886 active=2507 piece=▁beträchtliches\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26660 all=50866 active=2487 piece=▁humanistischen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26680 all=50846 active=2467 piece=▁redistributing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26700 all=50826 active=2447 piece=▁untergeordnete\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26720 all=50806 active=2522 piece=ungskriminalität\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26740 all=50786 active=2502 piece=▁Großbritanniens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26760 all=50766 active=2482 piece=▁Rechtsordnungen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26780 all=50746 active=2462 piece=▁Umweltsanierung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26800 all=50726 active=2442 piece=▁fertigzustellen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=26820 all=50706 active=2517 piece=▁überwältigenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26840 all=50706 active=2517 piece=öteb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26860 all=50699 active=2510 piece=analp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26880 all=50704 active=2515 piece=oiren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26900 all=50693 active=2504 piece=▁Chef\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26920 all=50680 active=2522 piece=259/99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26940 all=50680 active=2522 piece=gneten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26960 all=50681 active=2523 piece=sfremd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=26980 all=50677 active=2519 piece=▁1923.\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27000 all=50661 active=2503 piece=▁Fauna\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27020 all=50654 active=2527 piece=▁Macao\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27040 all=50639 active=2512 piece=▁blown\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27060 all=50625 active=2498 piece=▁inexp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27080 all=50616 active=2489 piece=10.1999\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27100 all=50610 active=2483 piece=emakers\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27120 all=50601 active=2521 piece=▁fußen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27140 all=50599 active=2519 piece=iebiger\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27160 all=50601 active=2521 piece=ropping\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27180 all=50599 active=2519 piece=wellers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27200 all=50591 active=2511 piece=▁ECOFIN\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27220 all=50585 active=2524 piece=▁Invol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27240 all=50581 active=2520 piece=▁Fläche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27260 all=50568 active=2507 piece=▁Promot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27280 all=50561 active=2500 piece=▁anpaßt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27300 all=50549 active=2488 piece=▁deskri\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27320 all=50547 active=2525 piece=▁1.27\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27340 all=50538 active=2516 piece=43/1999\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27360 all=50530 active=2508 piece=▁flirts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27380 all=50517 active=2495 piece=▁lethal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27400 all=50505 active=2483 piece=▁outmod\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27420 all=50495 active=2515 piece=▁squash\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27440 all=50479 active=2499 piece=-0041/99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27460 all=50472 active=2492 piece=beigesch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27480 all=50471 active=2491 piece=gremiums\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27500 all=50470 active=2490 piece=oclastic\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27520 all=50467 active=2520 piece=uarantee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27540 all=50470 active=2523 piece=▁400,000\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27560 all=50453 active=2506 piece=▁Ehrlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27580 all=50438 active=2491 piece=▁Kostend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27600 all=50423 active=2476 piece=▁Raument\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27620 all=50417 active=2515 piece=▁Rinderw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27640 all=50409 active=2507 piece=▁Verleug\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27660 all=50397 active=2495 piece=▁beeilen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27680 all=50382 active=2480 piece=▁deletes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27700 all=50365 active=2463 piece=▁gepreßt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27720 all=50358 active=2512 piece=▁Flecht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27740 all=50347 active=2501 piece=▁Rentens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27760 all=50334 active=2488 piece=▁konzert\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27780 all=50321 active=2475 piece=▁prävent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27800 all=50304 active=2458 piece=▁shifted\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27820 all=50294 active=2506 piece=uttala\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27840 all=50286 active=2498 piece=▁sauber\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27860 all=50272 active=2484 piece=▁sponsor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27880 all=50255 active=2467 piece=▁vacuums\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27900 all=50237 active=2449 piece=▁ärmerer\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27920 all=50240 active=2515 piece=rowded\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27940 all=50232 active=2507 piece=5-0259/99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27960 all=50225 active=2500 piece=confusing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=27980 all=50224 active=2499 piece=hvermögen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28000 all=50225 active=2500 piece=offenheit\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28020 all=50224 active=2510 piece=schafters\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28040 all=50213 active=2499 piece=▁Bombenan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28060 all=50198 active=2484 piece=▁Fusionsw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28080 all=50184 active=2470 piece=▁Militärb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28100 all=50166 active=2452 piece=▁Rotstift\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28120 all=50154 active=2497 piece=perdstown\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28140 all=50140 active=2483 piece=▁Todesurs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28160 all=50125 active=2468 piece=▁anschloß\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28180 all=50105 active=2448 piece=▁betrüben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28200 all=50088 active=2431 piece=▁direktem\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28220 all=50079 active=2496 piece=assioned\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28240 all=50069 active=2486 piece=fachleute\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28260 all=50055 active=2472 piece=▁embodied\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28280 all=50035 active=2452 piece=▁festeren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28300 all=50017 active=2434 piece=▁gesagten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28320 all=50013 active=2497 piece=ibanon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28340 all=50013 active=2497 piece=zeihung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28360 all=50005 active=2489 piece=▁erhöhte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28380 all=49993 active=2477 piece=▁disquiet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28400 all=49979 active=2463 piece=▁inasmuch\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28420 all=49973 active=2493 piece=höfliche\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28440 all=49959 active=2479 piece=▁letztmal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28460 all=49944 active=2464 piece=▁nervösen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28480 all=49927 active=2447 piece=▁pondered\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28500 all=49909 active=2429 piece=▁ruiniert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28520 all=49896 active=2483 piece=dications\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28540 all=49882 active=2469 piece=▁seminars\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28560 all=49864 active=2451 piece=▁tauschen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28580 all=49850 active=2437 piece=▁vermuten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28600 all=49831 active=2418 piece=Scoreboard\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28620 all=49824 active=2485 piece=endrunde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28640 all=49814 active=2475 piece=▁Abfassen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28660 all=49810 active=2471 piece=bewußteren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28680 all=49802 active=2463 piece=gewöhnlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28700 all=49792 active=2453 piece=onoclastic\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28720 all=49788 active=2485 piece=alistin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28740 all=49783 active=2480 piece=eitsgrad\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28760 all=49770 active=2467 piece=comprehens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28780 all=49770 active=2467 piece=upportable\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28800 all=49759 active=2456 piece=▁Auswüchse\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28820 all=49758 active=2487 piece=▁Dagmar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28840 all=49746 active=2475 piece=▁Bausteine\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28860 all=49731 active=2460 piece=▁Emotionen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28880 all=49714 active=2443 piece=▁Gründungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28900 all=49697 active=2426 piece=▁Kontrover\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28920 all=49685 active=2472 piece=▁Letzteres\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28940 all=49667 active=2454 piece=▁Quästorin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28960 all=49649 active=2436 piece=▁Tragödien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=28980 all=49632 active=2419 piece=▁ablehnend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29000 all=49612 active=2399 piece=▁attaining\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29020 all=49602 active=2471 piece=derbring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29040 all=49596 active=2465 piece=ministern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29060 all=49581 active=2450 piece=▁Billionen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29080 all=49564 active=2433 piece=▁beschämen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29100 all=49546 active=2415 piece=▁conjuring\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29120 all=49548 active=2480 piece=▁fatig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29140 all=49540 active=2472 piece=▁küstenn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29160 all=49530 active=2462 piece=▁cynegetic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29180 all=49511 active=2443 piece=▁downright\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29200 all=49491 active=2423 piece=▁embracing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29220 all=49484 active=2468 piece=dokumente\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29240 all=49477 active=2461 piece=▁advocated\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29260 all=49457 active=2441 piece=▁floodtide\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29280 all=49439 active=2423 piece=▁getrieben\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29300 all=49420 active=2404 piece=▁kleineres\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29320 all=49406 active=2457 piece=▁Kleine\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29340 all=49395 active=2446 piece=▁abläuft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29360 all=49380 active=2431 piece=▁sinister\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29380 all=49361 active=2412 piece=▁licensing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29400 all=49344 active=2395 piece=▁nuklearer\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29420 all=49338 active=2462 piece=▁Latin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29440 all=49327 active=2451 piece=▁Wahltag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29460 all=49314 active=2438 piece=▁beratende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29480 all=49294 active=2418 piece=▁prevailed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29500 all=49276 active=2400 piece=▁schlüssig\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29520 all=49262 active=2450 piece=▁altering\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29540 all=49249 active=2437 piece=▁schuldige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29560 all=49230 active=2418 piece=▁subsidiär\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29580 all=49214 active=2402 piece=▁unlautere\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29600 all=49198 active=2386 piece=▁vertraten\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29620 all=49193 active=2455 piece=zögert\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29640 all=49182 active=2444 piece=iertheit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29660 all=49169 active=2431 piece=▁Londoner\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29680 all=49154 active=2416 piece=▁verwüstet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29700 all=49139 active=2401 piece=▁überholen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29720 all=49130 active=2448 piece=▁gegriffen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29740 all=49113 active=2431 piece=Fördergebie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29760 all=49108 active=2426 piece=bestreitbar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29780 all=49100 active=2418 piece=ifikationen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29800 all=49092 active=2410 piece=osovarische\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29820 all=49084 active=2446 piece=▁Endlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29840 all=49068 active=2430 piece=▁unabsetz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29860 all=49050 active=2412 piece=raproduktiv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29880 all=49042 active=2404 piece=ungsmonopol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29900 all=49033 active=2395 piece=▁Absolutely\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29920 all=49033 active=2452 piece=uniary\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29940 all=49023 active=2442 piece=verluste\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29960 all=49009 active=2428 piece=▁sticking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=29980 all=48992 active=2411 piece=▁Asienkrise\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30000 all=48973 active=2392 piece=▁Billigflag\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30020 all=48964 active=2439 piece=▁notes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30040 all=48958 active=2433 piece=▁anhängt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30060 all=48945 active=2420 piece=▁revolved\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30080 all=48932 active=2407 piece=▁Calvinists\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30100 all=48914 active=2389 piece=▁Erfassungs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30120 all=48911 active=2443 piece=lassungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30140 all=48900 active=2432 piece=▁mehrerer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30160 all=48887 active=2419 piece=▁Baugruppen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30180 all=48871 active=2403 piece=▁Gefährlich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30200 all=48855 active=2387 piece=▁Kernfaktor\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30220 all=48853 active=2441 piece=content\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30240 all=48848 active=2436 piece=▁harness\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30260 all=48833 active=2421 piece=▁harangued\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30280 all=48815 active=2403 piece=▁Leidtragen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30300 all=48800 active=2388 piece=▁Optimismus\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30320 all=48796 active=2436 piece=hension\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30340 all=48789 active=2429 piece=▁Großkonz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30360 all=48775 active=2415 piece=▁hinwegzuf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30380 all=48757 active=2397 piece=▁Reedereien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30400 all=48740 active=2380 piece=▁Selbstmord\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30420 all=48738 active=2435 piece=▁hired\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30440 all=48726 active=2423 piece=rosionsan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30460 all=48716 active=2413 piece=▁Madagascar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30480 all=48699 active=2396 piece=▁Todesopfer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30500 all=48681 active=2378 piece=▁Vorgängern\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30520 all=48678 active=2432 piece=▁bigot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30540 all=48674 active=2428 piece=▁Holzweg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30560 all=48660 active=2414 piece=▁Statuten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30580 all=48645 active=2399 piece=verwertungs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30600 all=48627 active=2381 piece=▁abzusenken\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30620 all=48626 active=2431 piece=▁expuls\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30640 all=48616 active=2421 piece=▁vermengt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30660 all=48598 active=2403 piece=▁Besonderer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30680 all=48578 active=2383 piece=▁aufgewühlt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30700 all=48558 active=2363 piece=▁beeinflußt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30720 all=48553 active=2423 piece=viates\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30740 all=48544 active=2414 piece=ectivity\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30760 all=48534 active=2404 piece=▁fatalist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30780 all=48520 active=2390 piece=▁Startpunkt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30800 all=48501 active=2371 piece=▁bridgehead\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30820 all=48498 active=2422 piece=▁Jassir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30840 all=48487 active=2411 piece=geladenen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30860 all=48472 active=2396 piece=▁demonstra\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30880 all=48457 active=2381 piece=▁championed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30900 all=48438 active=2362 piece=▁discovered\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30920 all=48436 active=2420 piece=▁shudder\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30940 all=48425 active=2409 piece=▁ermöglich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30960 all=48410 active=2394 piece=▁eingereist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=30980 all=48390 active=2374 piece=▁erschreckt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31000 all=48371 active=2355 piece=▁fossilised\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31020 all=48365 active=2413 piece=uplicit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31040 all=48357 active=2405 piece=▁rejoice\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31060 all=48343 active=2391 piece=▁expeditiv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31080 all=48326 active=2374 piece=▁geachteten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31100 all=48306 active=2354 piece=▁healthcare\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31120 all=48299 active=2409 piece=Trojan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31140 all=48289 active=2399 piece=▁cheque\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31160 all=48282 active=2392 piece=▁Klartext\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31180 all=48270 active=2380 piece=ungswirkung\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31200 all=48252 active=2362 piece=▁indistinct\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31220 all=48243 active=2404 piece=▁sahen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31240 all=48235 active=2396 piece=unktuelle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31260 all=48219 active=2380 piece=hinnehmbare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31280 all=48201 active=2362 piece=▁katholisch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31300 all=48182 active=2343 piece=▁minimising\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31320 all=48181 active=2409 piece=▁53⁄4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31340 all=48175 active=2403 piece=ukrativ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31360 all=48170 active=2398 piece=▁prozess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31380 all=48155 active=2383 piece=▁Kernpunkt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31400 all=48137 active=2365 piece=▁multiplied\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31420 all=48132 active=2402 piece=▁luxury\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31440 all=48121 active=2391 piece=linquished\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31460 all=48103 active=2373 piece=▁nochmalige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31480 all=48083 active=2353 piece=▁powerhouse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31500 all=48064 active=2334 piece=▁registered\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31520 all=48053 active=2393 piece=▁grâce\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31540 all=48044 active=2384 piece=lementen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31560 all=48036 active=2376 piece=▁entschäd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31580 all=48021 active=2361 piece=▁afternoons\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31600 all=48002 active=2342 piece=▁schlimmste\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31620 all=48001 active=2400 piece=tragsb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31640 all=47995 active=2394 piece=▁inquis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31660 all=47980 active=2379 piece=▁bindende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31680 all=47965 active=2364 piece=▁repression\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31700 all=47945 active=2344 piece=▁suspicions\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31720 all=47939 active=2392 piece=▁Höher\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31740 all=47931 active=2384 piece=▁Aromast\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31760 all=47920 active=2373 piece=▁Verschlag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31780 all=47903 active=2356 piece=▁symbolisch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31800 all=47885 active=2338 piece=▁underscore\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31820 all=47880 active=2390 piece=strösen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31840 all=47866 active=2376 piece=▁missions\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31860 all=47852 active=2362 piece=▁Anwesenden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=31880 all=47833 active=2343 piece=▁universell\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: translation_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: translation_tokenizer.vocab\n",
      "Tokenizer training complete!\n",
      "Loading tokenizer...\n",
      "\n",
      "Tokenizer Info:\n",
      "Vocabulary size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "\n",
      "Creating dataloaders...\n",
      "\n",
      "Training set sample:\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Input (DE): [DE] Veränderungen sollten nicht zu schnell eingeführt werden oder zu radikal sein, weil sonst die öffentliche Meinung eine Ratifizierung künftiger EU-Verträge erheblich erschweren könnte.\n",
      "Target (EN): [EN] Changes should not be brought about too quickly and cannot be too sweeping, otherwise public opinion will make ratification of any future European Union treaty very difficult indeed.\n",
      "\n",
      "Validation set sample:\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size:Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Input (DE): [DE] Wenn Sie den Text aufmerksam lesen, so können Sie erkennen, daß dieser Vorschlag für eine Kapitalsteuer ein ganz normales Instrument darstellt, mit dem versucht werden soll, die internationalen Anleger zu einem verantwortlichen Handeln auf den Finanzmärkten zu verpflichten.\n",
      "Target (EN): [EN] If you look carefully, this proposed tax on capital is just one of several instruments aimed at forcing international investors to behave responsibly on the financial markets.\n",
      "\n",
      "Test set sample:\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [EN] ID: 5\n",
      "\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Input (DE): [DE] Auch vor dem Hintergrund dieser Maßnahmenliste, die im übrigen nicht einmal vollständig ist, kann ich hier und heute vor der Ratspräsidentschaft und den Abgeordneten erneut und mit großer Überzeugung versichern: Wenn 1999 das Jahr der Konsolidierung des Wirkens der Union in diesem maßgeblichen Bereich war, so hoffe ich, daß 1999 auch der Aufbruch in eine neuen Phase war, deren Ziel die Beschleunigung der Schaffung eines Raums der Freiheit, der Sicherheit und des Rechts ist.\n",
      "Target (EN): [EN] This list of actions, which is not exhaustive, should, here today, in the presence of the Council Presidency and of the Members of this House, help to make it clear that, while 1999 was a year of consolidation for EU action in this key area, it also - I very much hope - represented the start of a new phase, marked by the desire to speed up the establishment of an area of freedom, security and justice.\n",
      "\n",
      "Epoch 1/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "/opt/anaconda3/envs/de-en-translator/lib/python3.9/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 1/250 [00:09<39:23,  9.49s/it, loss=10.6207, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:14<27:25,  6.64s/it, loss=10.2647, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:16<18:44,  4.55s/it, loss=10.0091, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:22<21:40,  5.29s/it, loss=9.8367, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<16:51,  4.13s/it, loss=9.6960, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:29<17:05,  4.20s/it, loss=9.5892, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:31<14:09,  3.50s/it, loss=9.5023, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:33<13:10,  3.27s/it, loss=9.4270, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:38<15:09,  3.78s/it, loss=9.3528, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:40<12:56,  3.24s/it, loss=9.2751, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:44<13:46,  3.46s/it, loss=9.2032, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:48<14:06,  3.56s/it, loss=9.1468, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:50<12:41,  3.21s/it, loss=9.0872, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:52<10:58,  2.79s/it, loss=9.0299, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:54<09:32,  2.44s/it, loss=8.9733, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:56<08:40,  2.23s/it, loss=8.9193, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [00:58<09:06,  2.35s/it, loss=8.8813, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:02<10:19,  2.67s/it, loss=8.8313, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:06<12:38,  3.29s/it, loss=8.7868, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:13<16:14,  4.24s/it, loss=8.7441, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:15<13:56,  3.65s/it, loss=8.6989, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:18<13:23,  3.52s/it, loss=8.6639, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:21<11:47,  3.12s/it, loss=8.6198, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:23<11:11,  2.97s/it, loss=8.5841, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:28<13:06,  3.49s/it, loss=8.5460, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:35<17:22,  4.65s/it, loss=8.5108, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:38<14:53,  4.01s/it, loss=8.4705, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:44<17:03,  4.61s/it, loss=8.4367, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:47<15:31,  4.21s/it, loss=8.4017, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:50<14:16,  3.89s/it, loss=8.3634, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:54<14:12,  3.89s/it, loss=8.3304, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [02:01<17:53,  4.92s/it, loss=8.2909, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [02:04<15:34,  4.30s/it, loss=8.2578, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:07<13:39,  3.79s/it, loss=8.2232, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:10<12:59,  3.62s/it, loss=8.1934, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:13<11:41,  3.28s/it, loss=8.1597, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:15<11:15,  3.17s/it, loss=8.1261, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:18<10:16,  2.91s/it, loss=8.0908, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:22<11:33,  3.29s/it, loss=8.0554, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:25<10:57,  3.13s/it, loss=8.0256, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:27<10:24,  2.99s/it, loss=7.9943, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:32<12:24,  3.58s/it, loss=7.9604, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:37<13:49,  4.01s/it, loss=7.9336, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:40<11:56,  3.48s/it, loss=7.9020, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:43<11:21,  3.32s/it, loss=7.8748, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:45<10:16,  3.02s/it, loss=7.8483, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:47<09:50,  2.91s/it, loss=7.8202, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [02:53<12:44,  3.79s/it, loss=7.7887, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [02:56<11:47,  3.52s/it, loss=7.7602, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [03:00<11:52,  3.56s/it, loss=7.7364, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [03:06<14:12,  4.29s/it, loss=7.7117, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:08<12:19,  3.73s/it, loss=7.6843, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:11<11:28,  3.49s/it, loss=7.6581, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:18<14:33,  4.46s/it, loss=7.6346, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:22<14:10,  4.36s/it, loss=7.6127, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:26<14:00,  4.33s/it, loss=7.5887, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:29<11:51,  3.68s/it, loss=7.5622, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:33<12:56,  4.04s/it, loss=7.5398, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:36<11:01,  3.46s/it, loss=7.5175, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:40<11:53,  3.75s/it, loss=7.4944, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:43<11:23,  3.61s/it, loss=7.4712, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:46<10:18,  3.29s/it, loss=7.4486, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:48<09:15,  2.97s/it, loss=7.4248, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:54<11:41,  3.77s/it, loss=7.4030, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:56<10:36,  3.44s/it, loss=7.3810, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [04:01<11:38,  3.80s/it, loss=7.3599, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:04<10:49,  3.55s/it, loss=7.3385, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:07<10:00,  3.30s/it, loss=7.3192, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:10<09:56,  3.29s/it, loss=7.2996, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:12<08:56,  2.98s/it, loss=7.2808, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:16<09:44,  3.27s/it, loss=7.2608, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:18<08:46,  2.96s/it, loss=7.2417, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:20<07:48,  2.65s/it, loss=7.2220, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:22<07:09,  2.44s/it, loss=7.2051, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:24<06:41,  2.29s/it, loss=7.1868, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:27<07:21,  2.54s/it, loss=7.1701, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:30<07:16,  2.53s/it, loss=7.1546, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:34<08:40,  3.02s/it, loss=7.1388, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:36<08:11,  2.87s/it, loss=7.1238, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:39<07:55,  2.80s/it, loss=7.1092, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:42<08:02,  2.85s/it, loss=7.0943, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:47<09:33,  3.41s/it, loss=7.0788, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:51<10:03,  3.61s/it, loss=7.0633, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:55<10:04,  3.64s/it, loss=7.0481, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [04:57<09:14,  3.36s/it, loss=7.0326, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [05:00<08:22,  3.07s/it, loss=7.0152, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [05:01<07:18,  2.69s/it, loss=7.0007, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [05:04<06:59,  2.59s/it, loss=6.9867, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:11<10:32,  3.93s/it, loss=6.9746, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:13<08:47,  3.30s/it, loss=6.9603, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:16<08:28,  3.20s/it, loss=6.9470, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:18<08:04,  3.07s/it, loss=6.9341, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:22<08:04,  3.08s/it, loss=6.9231, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:24<07:32,  2.90s/it, loss=6.9086, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:27<07:22,  2.85s/it, loss=6.8980, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:29<06:37,  2.58s/it, loss=6.8833, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:31<06:25,  2.52s/it, loss=6.8702, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:36<07:56,  3.13s/it, loss=6.8610, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:38<07:16,  2.89s/it, loss=6.8457, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:45<10:37,  4.25s/it, loss=6.8363, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:47<08:49,  3.55s/it, loss=6.8242, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:50<07:52,  3.19s/it, loss=6.8118, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:52<07:26,  3.04s/it, loss=6.8001, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:55<07:09,  2.94s/it, loss=6.7906, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [05:57<06:31,  2.70s/it, loss=6.7803, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [06:01<07:01,  2.93s/it, loss=6.7692, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [06:03<06:51,  2.88s/it, loss=6.7592, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:07<07:11,  3.04s/it, loss=6.7487, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:09<06:39,  2.84s/it, loss=6.7403, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:15<08:21,  3.58s/it, loss=6.7295, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:17<07:41,  3.32s/it, loss=6.7206, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:20<07:01,  3.06s/it, loss=6.7100, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:27<09:38,  4.22s/it, loss=6.7008, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:29<08:16,  3.65s/it, loss=6.6913, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:31<07:06,  3.16s/it, loss=6.6813, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:35<07:21,  3.29s/it, loss=6.6723, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:41<09:18,  4.20s/it, loss=6.6643, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:48<11:09,  5.07s/it, loss=6.6557, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:52<10:17,  4.71s/it, loss=6.6474, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:54<08:18,  3.84s/it, loss=6.6387, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:56<07:25,  3.45s/it, loss=6.6308, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [06:59<07:03,  3.31s/it, loss=6.6242, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [07:03<07:03,  3.33s/it, loss=6.6155, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:04<06:02,  2.88s/it, loss=6.6054, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:07<05:59,  2.87s/it, loss=6.5965, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:10<05:50,  2.83s/it, loss=6.5886, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:12<05:08,  2.50s/it, loss=6.5793, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:15<05:16,  2.60s/it, loss=6.5706, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:17<04:54,  2.43s/it, loss=6.5625, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:19<04:45,  2.38s/it, loss=6.5547, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:22<04:55,  2.48s/it, loss=6.5490, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:23<04:33,  2.32s/it, loss=6.5407, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:26<04:52,  2.50s/it, loss=6.5331, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:32<06:48,  3.52s/it, loss=6.5247, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:36<07:05,  3.70s/it, loss=6.5167, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:39<06:17,  3.31s/it, loss=6.5098, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:41<05:30,  2.92s/it, loss=6.5015, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:43<05:11,  2.79s/it, loss=6.4937, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:45<04:43,  2.56s/it, loss=6.4868, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:47<04:26,  2.42s/it, loss=6.4807, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:51<04:58,  2.74s/it, loss=6.4731, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:53<04:29,  2.49s/it, loss=6.4668, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [07:56<04:51,  2.73s/it, loss=6.4595, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [07:59<04:45,  2.70s/it, loss=6.4513, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:05<06:35,  3.77s/it, loss=6.4457, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:09<06:46,  3.91s/it, loss=6.4396, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:11<05:46,  3.37s/it, loss=6.4320, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:14<05:15,  3.09s/it, loss=6.4272, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:18<05:41,  3.38s/it, loss=6.4211, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:23<06:40,  4.00s/it, loss=6.4150, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:26<06:05,  3.70s/it, loss=6.4101, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:31<06:37,  4.05s/it, loss=6.4040, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:33<05:41,  3.52s/it, loss=6.3979, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:38<06:21,  3.97s/it, loss=6.3913, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:40<05:18,  3.35s/it, loss=6.3852, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:44<05:19,  3.40s/it, loss=6.3803, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:48<05:27,  3.52s/it, loss=6.3752, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:52<05:33,  3.62s/it, loss=6.3722, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:56<05:45,  3.80s/it, loss=6.3663, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:59<05:29,  3.66s/it, loss=6.3609, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [09:05<06:21,  4.28s/it, loss=6.3554, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [09:08<05:47,  3.95s/it, loss=6.3510, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [09:10<04:48,  3.31s/it, loss=6.3447, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [09:12<04:16,  2.98s/it, loss=6.3389, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:17<04:51,  3.43s/it, loss=6.3338, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:21<05:23,  3.85s/it, loss=6.3293, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:26<05:42,  4.12s/it, loss=6.3228, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:31<05:55,  4.34s/it, loss=6.3184, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:33<04:47,  3.55s/it, loss=6.3124, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:37<04:56,  3.71s/it, loss=6.3073, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:44<06:22,  4.84s/it, loss=6.3005, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [09:46<05:08,  3.96s/it, loss=6.2953, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:49<04:39,  3.63s/it, loss=6.2920, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:51<04:01,  3.17s/it, loss=6.2871, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:53<03:29,  2.79s/it, loss=6.2824, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:56<03:26,  2.79s/it, loss=6.2772, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:58<03:14,  2.67s/it, loss=6.2731, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [10:00<02:59,  2.50s/it, loss=6.2679, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [10:03<02:57,  2.49s/it, loss=6.2638, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [10:06<03:11,  2.73s/it, loss=6.2593, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [10:08<02:52,  2.50s/it, loss=6.2536, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [10:15<04:17,  3.79s/it, loss=6.2480, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [10:17<03:44,  3.35s/it, loss=6.2432, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [10:19<03:17,  3.00s/it, loss=6.2383, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:21<02:55,  2.69s/it, loss=6.2342, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:24<02:54,  2.73s/it, loss=6.2291, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:26<02:31,  2.41s/it, loss=6.2246, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:28<02:28,  2.40s/it, loss=6.2192, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:31<02:32,  2.49s/it, loss=6.2143, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:37<03:43,  3.72s/it, loss=6.2101, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [10:40<03:24,  3.46s/it, loss=6.2065, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [10:44<03:18,  3.42s/it, loss=6.2013, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [10:46<02:59,  3.14s/it, loss=6.1976, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:49<02:53,  3.09s/it, loss=6.1931, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:51<02:37,  2.86s/it, loss=6.1889, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:54<02:27,  2.74s/it, loss=6.1841, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:56<02:18,  2.61s/it, loss=6.1792, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [11:00<02:32,  2.94s/it, loss=6.1737, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [11:03<02:33,  3.01s/it, loss=6.1696, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [11:07<02:46,  3.33s/it, loss=6.1652, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [11:13<03:16,  4.01s/it, loss=6.1619, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:19<03:41,  4.61s/it, loss=6.1588, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:21<03:10,  4.05s/it, loss=6.1545, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:25<02:56,  3.84s/it, loss=6.1507, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:29<02:52,  3.84s/it, loss=6.1479, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:32<02:43,  3.71s/it, loss=6.1445, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [11:35<02:33,  3.57s/it, loss=6.1408, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [11:38<02:14,  3.19s/it, loss=6.1375, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [11:40<02:01,  2.95s/it, loss=6.1335, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [11:43<01:58,  2.96s/it, loss=6.1295, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:46<01:59,  3.06s/it, loss=6.1245, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:50<01:59,  3.13s/it, loss=6.1202, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:52<01:43,  2.81s/it, loss=6.1165, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:54<01:36,  2.69s/it, loss=6.1131, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:57<01:33,  2.67s/it, loss=6.1084, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:59<01:31,  2.68s/it, loss=6.1052, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [12:04<01:51,  3.37s/it, loss=6.1009, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [12:07<01:43,  3.23s/it, loss=6.0972, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [12:10<01:33,  3.01s/it, loss=6.0933, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [12:12<01:26,  2.87s/it, loss=6.0906, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [12:15<01:23,  2.90s/it, loss=6.0863, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [12:18<01:17,  2.77s/it, loss=6.0819, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [12:20<01:14,  2.77s/it, loss=6.0783, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:24<01:18,  3.03s/it, loss=6.0756, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:27<01:15,  3.03s/it, loss=6.0716, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [12:29<01:07,  2.80s/it, loss=6.0662, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [12:31<00:56,  2.45s/it, loss=6.0618, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [12:33<00:49,  2.27s/it, loss=6.0573, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [12:37<00:58,  2.80s/it, loss=6.0541, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [12:39<00:49,  2.49s/it, loss=6.0503, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [12:43<00:57,  3.03s/it, loss=6.0463, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [12:49<01:08,  3.83s/it, loss=6.0423, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [12:51<00:55,  3.26s/it, loss=6.0383, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [12:57<01:08,  4.27s/it, loss=6.0348, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [13:00<00:55,  3.70s/it, loss=6.0321, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [13:06<01:04,  4.59s/it, loss=6.0296, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [13:10<00:54,  4.22s/it, loss=6.0259, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [13:13<00:46,  3.87s/it, loss=6.0226, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [13:17<00:45,  4.13s/it, loss=6.0192, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [13:20<00:35,  3.59s/it, loss=6.0164, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [13:22<00:28,  3.15s/it, loss=6.0132, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [13:25<00:24,  3.04s/it, loss=6.0104, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:27<00:19,  2.82s/it, loss=6.0066, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [13:30<00:16,  2.76s/it, loss=6.0039, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [13:32<00:12,  2.56s/it, loss=6.0001, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [13:35<00:10,  2.71s/it, loss=5.9964, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [13:38<00:08,  2.99s/it, loss=5.9947, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [13:41<00:05,  2.81s/it, loss=5.9923, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [13:43<00:02,  2.64s/it, loss=5.9903, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [14:01<00:00,  3.37s/it, loss=5.9879, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "/opt/anaconda3/envs/de-en-translator/lib/python3.9/site-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:42,  7.19s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:51,  3.70s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:09<01:16,  2.65s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:10<00:54,  1.95s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:11<00:44,  1.65s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:12<00:38,  1.46s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:13<00:32,  1.28s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:14<00:28,  1.17s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:16<00:27,  1.19s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:16<00:23,  1.06s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:17<00:22,  1.07s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:18<00:18,  1.06it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:19<00:18,  1.00it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:20<00:16,  1.09it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:21<00:15,  1.09it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:22<00:17,  1.11s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:23<00:16,  1.11s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:24<00:14,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:25<00:13,  1.02s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:26<00:11,  1.03it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:27<00:09,  1.13it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:28<00:08,  1.12it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:29<00:08,  1.02it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:30<00:08,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:31<00:07,  1.00s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:32<00:06,  1.11s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:33<00:05,  1.04s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:47<00:00,  1.47s/it]\n",
      "Removed old checkpoint: model_epoch_1.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_1.pt\n",
      "Saved training results to training_results_20250511_162111.csv\n",
      "Train Loss: 5.9879\n",
      "Val Loss: 5.3034\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: I would like to the European Parliament to the European Parliament, the European Parliament, the European Parliament.\n",
      "\n",
      "Epoch 2/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "Tokenizer vocab size:   [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:10<43:27, 10.47s/it, loss=4.9719, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:14<28:14,  6.83s/it, loss=5.1907, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:16<19:09,  4.66s/it, loss=5.2119, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:22<20:54,  5.10s/it, loss=5.1574, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<16:05,  3.94s/it, loss=5.1484, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:27<15:17,  3.76s/it, loss=5.1340, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:29<12:57,  3.20s/it, loss=5.1260, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:32<11:50,  2.94s/it, loss=5.1295, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:37<14:12,  3.54s/it, loss=5.1275, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:39<12:08,  3.04s/it, loss=5.1249, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:43<13:30,  3.39s/it, loss=5.1230, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:46<13:33,  3.42s/it, loss=5.1322, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:49<12:30,  3.17s/it, loss=5.1288, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:51<10:49,  2.75s/it, loss=5.1274, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:52<09:27,  2.41s/it, loss=5.1170, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:54<08:40,  2.23s/it, loss=5.1046, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [00:56<08:52,  2.28s/it, loss=5.1103, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [00:58<08:25,  2.18s/it, loss=5.1012, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:02<09:48,  2.55s/it, loss=5.0986, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:08<13:54,  3.63s/it, loss=5.0941, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:10<12:11,  3.20s/it, loss=5.0872, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:14<13:16,  3.49s/it, loss=5.0994, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:16<11:40,  3.09s/it, loss=5.0909, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:19<11:25,  3.03s/it, loss=5.0944, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:24<13:32,  3.61s/it, loss=5.0974, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:31<17:06,  4.58s/it, loss=5.0972, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:34<14:40,  3.95s/it, loss=5.0966, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:39<16:31,  4.47s/it, loss=5.1024, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:42<14:36,  3.96s/it, loss=5.1026, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:44<12:34,  3.43s/it, loss=5.0991, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:47<11:46,  3.23s/it, loss=5.1015, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [01:54<15:25,  4.24s/it, loss=5.1022, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [01:57<14:52,  4.11s/it, loss=5.1046, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:00<13:03,  3.63s/it, loss=5.1072, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:03<12:32,  3.50s/it, loss=5.1117, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:06<11:28,  3.22s/it, loss=5.1106, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:09<11:15,  3.17s/it, loss=5.1101, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:11<10:07,  2.87s/it, loss=5.1061, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:14<10:35,  3.01s/it, loss=5.1036, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:17<10:09,  2.90s/it, loss=5.1017, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:20<09:45,  2.80s/it, loss=5.1000, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:22<09:41,  2.79s/it, loss=5.0984, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:26<10:08,  2.94s/it, loss=5.1007, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:29<10:19,  3.01s/it, loss=5.0978, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:32<10:19,  3.02s/it, loss=5.0976, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:34<09:32,  2.81s/it, loss=5.0999, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:37<09:27,  2.80s/it, loss=5.0992, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [02:42<12:04,  3.59s/it, loss=5.0962, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [02:45<11:07,  3.32s/it, loss=5.0968, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [02:48<10:33,  3.17s/it, loss=5.1006, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [02:53<12:32,  3.78s/it, loss=5.1033, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [02:55<11:07,  3.37s/it, loss=5.1025, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [02:58<10:26,  3.18s/it, loss=5.0998, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:03<12:24,  3.80s/it, loss=5.1022, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:06<11:23,  3.51s/it, loss=5.1045, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:11<12:21,  3.82s/it, loss=5.1015, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:13<10:40,  3.32s/it, loss=5.0986, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:18<11:58,  3.74s/it, loss=5.0981, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:20<10:19,  3.24s/it, loss=5.0975, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:23<10:11,  3.22s/it, loss=5.0952, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:26<09:53,  3.14s/it, loss=5.0925, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:28<09:15,  2.96s/it, loss=5.0896, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:31<08:31,  2.74s/it, loss=5.0855, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:35<10:05,  3.26s/it, loss=5.0842, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:38<09:28,  3.07s/it, loss=5.0802, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [03:41<09:27,  3.08s/it, loss=5.0786, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [03:43<08:28,  2.78s/it, loss=5.0753, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [03:46<08:49,  2.91s/it, loss=5.0740, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [03:49<08:33,  2.84s/it, loss=5.0729, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [03:51<07:58,  2.66s/it, loss=5.0730, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [03:54<08:26,  2.83s/it, loss=5.0714, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [03:56<07:51,  2.65s/it, loss=5.0704, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [03:58<07:09,  2.43s/it, loss=5.0678, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:00<06:37,  2.26s/it, loss=5.0681, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:02<06:18,  2.16s/it, loss=5.0660, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:05<06:50,  2.36s/it, loss=5.0651, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:08<07:03,  2.45s/it, loss=5.0655, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:11<08:09,  2.84s/it, loss=5.0655, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:14<07:46,  2.73s/it, loss=5.0656, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:17<08:19,  2.94s/it, loss=5.0660, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:19<07:35,  2.70s/it, loss=5.0653, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:23<08:11,  2.93s/it, loss=5.0639, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:27<08:55,  3.20s/it, loss=5.0626, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:30<08:58,  3.25s/it, loss=5.0606, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [04:33<08:28,  3.08s/it, loss=5.0590, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [04:35<07:52,  2.88s/it, loss=5.0547, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [04:37<07:02,  2.59s/it, loss=5.0535, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [04:40<06:48,  2.52s/it, loss=5.0521, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [04:46<09:46,  3.64s/it, loss=5.0534, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [04:48<08:18,  3.11s/it, loss=5.0524, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [04:51<08:40,  3.27s/it, loss=5.0510, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [04:54<08:13,  3.13s/it, loss=5.0499, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [04:57<08:11,  3.13s/it, loss=5.0510, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [04:59<06:56,  2.67s/it, loss=5.0475, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:01<06:42,  2.60s/it, loss=5.0494, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:03<06:08,  2.40s/it, loss=5.0459, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:05<05:55,  2.32s/it, loss=5.0446, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:10<07:29,  2.96s/it, loss=5.0469, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:12<07:01,  2.79s/it, loss=5.0420, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:19<09:56,  3.98s/it, loss=5.0436, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:21<08:19,  3.35s/it, loss=5.0424, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:23<07:26,  3.02s/it, loss=5.0400, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:27<07:57,  3.25s/it, loss=5.0377, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:30<07:30,  3.09s/it, loss=5.0384, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [05:32<06:47,  2.81s/it, loss=5.0378, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [05:35<07:10,  2.99s/it, loss=5.0364, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [05:38<07:00,  2.94s/it, loss=5.0358, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [05:40<06:37,  2.80s/it, loss=5.0350, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [05:43<06:13,  2.65s/it, loss=5.0355, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [05:47<07:18,  3.13s/it, loss=5.0337, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [05:50<06:55,  2.99s/it, loss=5.0341, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [05:52<06:31,  2.84s/it, loss=5.0319, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:00<09:38,  4.23s/it, loss=5.0315, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:02<08:14,  3.63s/it, loss=5.0305, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:04<07:05,  3.15s/it, loss=5.0291, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:09<08:03,  3.61s/it, loss=5.0280, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:15<09:47,  4.42s/it, loss=5.0281, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:21<10:59,  5.00s/it, loss=5.0280, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:24<09:36,  4.40s/it, loss=5.0283, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:26<07:46,  3.59s/it, loss=5.0273, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:28<06:54,  3.21s/it, loss=5.0272, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [06:31<06:27,  3.03s/it, loss=5.0283, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [06:34<06:12,  2.94s/it, loss=5.0272, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [06:35<05:27,  2.60s/it, loss=5.0245, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [06:39<06:02,  2.90s/it, loss=5.0230, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [06:42<06:03,  2.93s/it, loss=5.0223, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [06:44<05:17,  2.58s/it, loss=5.0202, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [06:47<05:28,  2.69s/it, loss=5.0188, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [06:49<05:00,  2.48s/it, loss=5.0176, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [06:51<04:52,  2.44s/it, loss=5.0169, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [06:54<05:01,  2.53s/it, loss=5.0183, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [06:56<04:35,  2.34s/it, loss=5.0165, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [06:57<04:12,  2.16s/it, loss=5.0155, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:03<06:04,  3.14s/it, loss=5.0138, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:06<06:05,  3.18s/it, loss=5.0127, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:09<05:38,  2.97s/it, loss=5.0119, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:11<05:32,  2.94s/it, loss=5.0101, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:14<05:19,  2.85s/it, loss=5.0089, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:16<04:48,  2.60s/it, loss=5.0082, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:18<04:30,  2.46s/it, loss=5.0085, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:22<05:06,  2.81s/it, loss=5.0064, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:24<04:34,  2.54s/it, loss=5.0063, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [07:27<05:09,  2.89s/it, loss=5.0055, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [07:30<04:52,  2.76s/it, loss=5.0031, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [07:35<05:53,  3.37s/it, loss=5.0032, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [07:37<05:18,  3.06s/it, loss=5.0024, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [07:39<04:37,  2.70s/it, loss=5.0003, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [07:41<04:16,  2.52s/it, loss=5.0012, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [07:46<05:21,  3.19s/it, loss=5.0006, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [07:51<06:12,  3.73s/it, loss=4.9999, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [07:54<05:42,  3.46s/it, loss=5.0004, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [07:58<05:58,  3.66s/it, loss=4.9998, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:00<05:12,  3.22s/it, loss=4.9992, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:04<05:23,  3.37s/it, loss=4.9979, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:05<04:37,  2.92s/it, loss=4.9969, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:08<04:26,  2.83s/it, loss=4.9971, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:11<04:21,  2.82s/it, loss=4.9966, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:14<04:16,  2.79s/it, loss=4.9984, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:16<04:13,  2.78s/it, loss=4.9978, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:19<04:07,  2.75s/it, loss=4.9975, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [08:26<05:48,  3.91s/it, loss=4.9970, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [08:29<05:15,  3.59s/it, loss=4.9973, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [08:30<04:26,  3.06s/it, loss=4.9955, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [08:33<04:01,  2.80s/it, loss=4.9942, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [08:36<04:05,  2.89s/it, loss=4.9936, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [08:39<04:05,  2.93s/it, loss=4.9932, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [08:41<03:52,  2.80s/it, loss=4.9915, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [08:44<03:54,  2.86s/it, loss=4.9919, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [08:46<03:23,  2.51s/it, loss=4.9905, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [08:49<03:25,  2.57s/it, loss=4.9897, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [08:56<05:19,  4.05s/it, loss=4.9871, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [08:58<04:23,  3.38s/it, loss=4.9865, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:01<04:10,  3.26s/it, loss=4.9873, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:03<03:39,  2.89s/it, loss=4.9868, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:05<03:12,  2.57s/it, loss=4.9866, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:08<03:21,  2.73s/it, loss=4.9859, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:10<03:08,  2.58s/it, loss=4.9860, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [09:12<02:53,  2.40s/it, loss=4.9850, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [09:14<02:51,  2.41s/it, loss=4.9849, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [09:17<02:48,  2.41s/it, loss=4.9845, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [09:19<02:35,  2.26s/it, loss=4.9828, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [09:25<03:55,  3.46s/it, loss=4.9808, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [09:27<03:28,  3.11s/it, loss=4.9800, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [09:30<03:24,  3.09s/it, loss=4.9787, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [09:32<03:01,  2.79s/it, loss=4.9784, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [09:36<03:15,  3.06s/it, loss=4.9772, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [09:38<02:46,  2.64s/it, loss=4.9761, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [09:40<02:44,  2.65s/it, loss=4.9746, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [09:43<02:43,  2.68s/it, loss=4.9734, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [09:50<03:49,  3.83s/it, loss=4.9725, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [09:53<03:29,  3.54s/it, loss=4.9725, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [09:55<03:06,  3.22s/it, loss=4.9709, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [09:58<02:50,  2.98s/it, loss=4.9708, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:02<03:07,  3.35s/it, loss=4.9701, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:04<02:45,  3.01s/it, loss=4.9694, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:07<02:36,  2.91s/it, loss=4.9682, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:09<02:25,  2.75s/it, loss=4.9667, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [10:13<02:36,  3.02s/it, loss=4.9647, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [10:16<02:36,  3.06s/it, loss=4.9640, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [10:20<02:47,  3.36s/it, loss=4.9629, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [10:25<03:17,  4.03s/it, loss=4.9630, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [10:32<03:42,  4.63s/it, loss=4.9635, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [10:34<03:10,  4.06s/it, loss=4.9625, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [10:38<02:56,  3.85s/it, loss=4.9621, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [10:42<03:00,  4.02s/it, loss=4.9626, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [10:46<02:58,  4.07s/it, loss=4.9626, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [10:50<02:45,  3.85s/it, loss=4.9623, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [10:52<02:22,  3.40s/it, loss=4.9622, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [10:54<02:07,  3.11s/it, loss=4.9613, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [10:57<02:04,  3.12s/it, loss=4.9603, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:01<02:03,  3.17s/it, loss=4.9584, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:04<01:58,  3.12s/it, loss=4.9572, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:06<01:44,  2.82s/it, loss=4.9565, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:08<01:37,  2.70s/it, loss=4.9561, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:10<01:24,  2.41s/it, loss=4.9542, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:13<01:29,  2.62s/it, loss=4.9539, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [11:18<01:46,  3.24s/it, loss=4.9527, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [11:21<01:40,  3.14s/it, loss=4.9517, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [11:23<01:31,  2.96s/it, loss=4.9507, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [11:26<01:25,  2.84s/it, loss=4.9510, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [11:29<01:23,  2.89s/it, loss=4.9499, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [11:31<01:17,  2.76s/it, loss=4.9483, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [11:34<01:14,  2.75s/it, loss=4.9474, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [11:37<01:14,  2.88s/it, loss=4.9477, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [11:39<01:06,  2.67s/it, loss=4.9465, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [11:42<01:06,  2.76s/it, loss=4.9437, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [11:44<00:55,  2.42s/it, loss=4.9419, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [11:46<00:49,  2.24s/it, loss=4.9403, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [11:49<00:51,  2.47s/it, loss=4.9396, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [11:51<00:45,  2.26s/it, loss=4.9385, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [11:55<00:55,  2.90s/it, loss=4.9372, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [12:00<01:05,  3.63s/it, loss=4.9360, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [12:02<00:52,  3.11s/it, loss=4.9346, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [12:08<01:01,  3.86s/it, loss=4.9339, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [12:10<00:51,  3.43s/it, loss=4.9339, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [12:16<00:59,  4.23s/it, loss=4.9341, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [12:20<00:51,  3.98s/it, loss=4.9330, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [12:22<00:42,  3.57s/it, loss=4.9322, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [12:26<00:40,  3.66s/it, loss=4.9314, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [12:29<00:32,  3.27s/it, loss=4.9309, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [12:31<00:26,  2.93s/it, loss=4.9304, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [12:34<00:23,  2.92s/it, loss=4.9301, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [12:36<00:19,  2.76s/it, loss=4.9288, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [12:39<00:16,  2.74s/it, loss=4.9283, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [12:41<00:12,  2.54s/it, loss=4.9269, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [12:44<00:10,  2.63s/it, loss=4.9255, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [12:47<00:08,  2.75s/it, loss=4.9263, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [12:49<00:05,  2.64s/it, loss=4.9262, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [12:52<00:02,  2.79s/it, loss=4.9266, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [13:09<00:00,  3.16s/it, loss=4.9267, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:41,  7.13s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:52,  3.73s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:09<01:17,  2.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:10<00:54,  1.96s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:11<00:44,  1.66s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:13<00:38,  1.47s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:13<00:32,  1.29s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:15<00:33,  1.40s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:16<00:32,  1.40s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:17<00:26,  1.20s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:18<00:24,  1.16s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:19<00:20,  1.01s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:20<00:19,  1.05s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:21<00:17,  1.05it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:22<00:15,  1.09it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:22<00:14,  1.14it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:23<00:13,  1.07it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:24<00:12,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:25<00:11,  1.09it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:26<00:10,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:27<00:09,  1.21it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:28<00:08,  1.16it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:29<00:08,  1.07it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:30<00:08,  1.01s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:31<00:06,  1.01it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:32<00:06,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:33<00:05,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:47<00:00,  1.48s/it]\n",
      "Removed old checkpoint: model_epoch_2.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_2.pt\n",
      "Saved training results to training_results_20250511_163510.csv\n",
      "Train Loss: 4.9267\n",
      "Val Loss: 4.9472\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: this is a European Parliament to be a European Parliament.\n",
      "\n",
      "Epoch 3/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:10<42:16, 10.19s/it, loss=4.5400, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:14<28:06,  6.80s/it, loss=4.7626, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:16<19:01,  4.62s/it, loss=4.7800, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:22<20:09,  4.92s/it, loss=4.7198, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:23<15:33,  3.81s/it, loss=4.7067, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:28<16:58,  4.18s/it, loss=4.6932, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:30<14:06,  3.48s/it, loss=4.6840, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:33<12:34,  3.12s/it, loss=4.6816, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:38<14:52,  3.70s/it, loss=4.6760, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:40<12:38,  3.16s/it, loss=4.6713, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:42<12:09,  3.05s/it, loss=4.6687, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:45<12:06,  3.05s/it, loss=4.6806, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:48<11:26,  2.90s/it, loss=4.6791, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:50<10:06,  2.57s/it, loss=4.6805, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:51<08:56,  2.28s/it, loss=4.6729, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:53<08:09,  2.09s/it, loss=4.6621, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [00:56<09:23,  2.42s/it, loss=4.6715, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [00:58<08:48,  2.28s/it, loss=4.6646, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:01<09:46,  2.54s/it, loss=4.6638, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:07<13:45,  3.59s/it, loss=4.6629, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:10<12:12,  3.20s/it, loss=4.6576, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:12<11:42,  3.08s/it, loss=4.6710, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:15<10:37,  2.81s/it, loss=4.6657, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:17<10:19,  2.74s/it, loss=4.6694, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:22<12:15,  3.27s/it, loss=4.6719, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:28<15:57,  4.27s/it, loss=4.6740, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:31<13:52,  3.73s/it, loss=4.6738, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:38<17:37,  4.76s/it, loss=4.6803, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:41<15:30,  4.21s/it, loss=4.6802, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:43<13:13,  3.61s/it, loss=4.6776, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:46<12:19,  3.38s/it, loss=4.6818, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [01:53<16:23,  4.51s/it, loss=4.6822, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [01:56<14:57,  4.14s/it, loss=4.6844, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:00<13:54,  3.86s/it, loss=4.6863, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:03<13:17,  3.71s/it, loss=4.6887, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:05<11:54,  3.34s/it, loss=4.6887, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:09<11:35,  3.27s/it, loss=4.6886, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:11<10:24,  2.95s/it, loss=4.6857, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:14<10:46,  3.06s/it, loss=4.6841, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:17<10:16,  2.94s/it, loss=4.6819, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:19<09:48,  2.82s/it, loss=4.6797, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:22<09:46,  2.82s/it, loss=4.6796, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:26<11:14,  3.26s/it, loss=4.6817, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:29<10:15,  2.99s/it, loss=4.6788, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:32<10:47,  3.16s/it, loss=4.6799, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:35<10:40,  3.14s/it, loss=4.6820, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:38<10:16,  3.04s/it, loss=4.6824, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [02:44<13:07,  3.90s/it, loss=4.6787, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [02:47<11:57,  3.57s/it, loss=4.6790, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [02:50<11:21,  3.41s/it, loss=4.6835, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [02:55<13:14,  3.99s/it, loss=4.6871, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [02:58<11:36,  3.52s/it, loss=4.6867, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:01<11:06,  3.38s/it, loss=4.6839, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:06<13:09,  4.03s/it, loss=4.6866, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:09<11:57,  3.68s/it, loss=4.6876, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:15<13:35,  4.21s/it, loss=4.6843, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:17<11:30,  3.58s/it, loss=4.6816, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:21<12:23,  3.87s/it, loss=4.6813, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:23<10:35,  3.33s/it, loss=4.6810, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:27<11:00,  3.48s/it, loss=4.6789, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:30<10:29,  3.33s/it, loss=4.6760, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:33<09:39,  3.08s/it, loss=4.6723, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:35<08:49,  2.83s/it, loss=4.6683, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:41<11:30,  3.71s/it, loss=4.6678, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:43<10:24,  3.38s/it, loss=4.6629, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [03:47<10:45,  3.51s/it, loss=4.6616, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [03:50<10:11,  3.34s/it, loss=4.6587, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [03:52<09:20,  3.08s/it, loss=4.6572, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [03:55<09:10,  3.04s/it, loss=4.6567, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [03:58<08:25,  2.81s/it, loss=4.6573, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:01<09:14,  3.10s/it, loss=4.6565, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:04<08:22,  2.82s/it, loss=4.6560, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:05<07:29,  2.54s/it, loss=4.6533, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:07<06:56,  2.37s/it, loss=4.6536, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:09<06:31,  2.24s/it, loss=4.6516, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:12<06:58,  2.41s/it, loss=4.6512, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:15<07:02,  2.44s/it, loss=4.6518, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:19<08:16,  2.89s/it, loss=4.6524, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:21<07:54,  2.77s/it, loss=4.6534, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:25<08:28,  2.99s/it, loss=4.6540, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:27<07:46,  2.76s/it, loss=4.6530, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:31<08:49,  3.15s/it, loss=4.6515, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:34<09:00,  3.24s/it, loss=4.6508, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:38<09:17,  3.36s/it, loss=4.6492, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [04:41<08:38,  3.14s/it, loss=4.6480, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [04:43<07:57,  2.91s/it, loss=4.6435, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [04:45<07:01,  2.59s/it, loss=4.6428, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [04:47<06:46,  2.51s/it, loss=4.6413, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [04:54<09:53,  3.69s/it, loss=4.6430, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [04:55<08:21,  3.14s/it, loss=4.6424, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [04:58<08:02,  3.03s/it, loss=4.6416, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:02<08:30,  3.23s/it, loss=4.6400, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:06<08:55,  3.41s/it, loss=4.6409, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:07<07:27,  2.87s/it, loss=4.6372, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:10<07:16,  2.82s/it, loss=4.6393, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:12<06:33,  2.55s/it, loss=4.6356, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:14<06:09,  2.42s/it, loss=4.6349, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:18<07:35,  3.00s/it, loss=4.6376, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:21<07:01,  2.79s/it, loss=4.6332, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:28<10:07,  4.05s/it, loss=4.6354, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:30<08:28,  3.41s/it, loss=4.6342, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:33<08:13,  3.33s/it, loss=4.6319, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:36<08:06,  3.31s/it, loss=4.6298, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:39<07:44,  3.18s/it, loss=4.6309, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [05:41<06:56,  2.87s/it, loss=4.6306, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [05:45<07:46,  3.24s/it, loss=4.6295, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [05:48<07:28,  3.14s/it, loss=4.6291, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [05:51<06:57,  2.94s/it, loss=4.6287, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [05:53<06:26,  2.74s/it, loss=4.6294, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [05:57<07:14,  3.11s/it, loss=4.6276, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:00<06:57,  3.00s/it, loss=4.6285, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:02<06:30,  2.83s/it, loss=4.6268, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:11<10:25,  4.57s/it, loss=4.6271, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:13<08:49,  3.90s/it, loss=4.6259, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:15<07:28,  3.32s/it, loss=4.6245, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:18<07:18,  3.27s/it, loss=4.6234, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:24<09:07,  4.12s/it, loss=4.6235, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:31<10:35,  4.81s/it, loss=4.6238, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:34<09:19,  4.27s/it, loss=4.6246, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:35<07:36,  3.51s/it, loss=4.6240, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:38<06:50,  3.18s/it, loss=4.6246, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [06:41<06:58,  3.27s/it, loss=4.6259, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [06:45<07:14,  3.42s/it, loss=4.6251, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [06:47<06:11,  2.95s/it, loss=4.6226, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [06:50<06:28,  3.11s/it, loss=4.6216, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [06:53<06:15,  3.02s/it, loss=4.6211, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [06:55<05:25,  2.64s/it, loss=4.6191, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [06:58<05:27,  2.68s/it, loss=4.6179, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:00<04:58,  2.47s/it, loss=4.6171, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:02<04:52,  2.43s/it, loss=4.6167, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:05<04:54,  2.47s/it, loss=4.6182, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:07<04:33,  2.32s/it, loss=4.6163, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:08<04:10,  2.14s/it, loss=4.6153, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:15<06:40,  3.45s/it, loss=4.6142, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:18<06:39,  3.48s/it, loss=4.6133, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:21<05:59,  3.15s/it, loss=4.6129, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:23<05:19,  2.83s/it, loss=4.6116, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:25<05:08,  2.75s/it, loss=4.6105, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:27<04:40,  2.52s/it, loss=4.6101, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:30<04:23,  2.40s/it, loss=4.6107, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:32<04:33,  2.51s/it, loss=4.6087, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:34<04:12,  2.34s/it, loss=4.6090, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [07:37<04:26,  2.49s/it, loss=4.6085, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [07:40<04:24,  2.50s/it, loss=4.6065, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [07:46<06:24,  3.66s/it, loss=4.6065, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [07:48<05:38,  3.26s/it, loss=4.6059, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [07:50<04:53,  2.85s/it, loss=4.6039, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [07:52<04:24,  2.59s/it, loss=4.6049, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [07:55<04:25,  2.63s/it, loss=4.6044, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:00<05:26,  3.26s/it, loss=4.6041, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:02<05:06,  3.10s/it, loss=4.6049, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:06<05:29,  3.36s/it, loss=4.6047, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:08<04:51,  3.00s/it, loss=4.6043, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:15<06:15,  3.91s/it, loss=4.6034, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:16<05:13,  3.30s/it, loss=4.6025, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:19<04:58,  3.18s/it, loss=4.6030, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:22<04:42,  3.04s/it, loss=4.6025, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:25<04:32,  2.97s/it, loss=4.6046, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:28<04:25,  2.92s/it, loss=4.6045, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:30<04:17,  2.86s/it, loss=4.6047, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [08:36<05:22,  3.62s/it, loss=4.6046, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [08:39<04:57,  3.38s/it, loss=4.6054, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [08:40<04:11,  2.89s/it, loss=4.6037, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [08:43<04:12,  2.94s/it, loss=4.6026, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [08:48<04:43,  3.33s/it, loss=4.6025, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [08:52<05:04,  3.62s/it, loss=4.6025, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [08:54<04:32,  3.28s/it, loss=4.6015, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [08:58<04:26,  3.25s/it, loss=4.6023, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [08:59<03:46,  2.79s/it, loss=4.6011, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:02<03:42,  2.78s/it, loss=4.6009, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:09<05:12,  3.95s/it, loss=4.5987, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [09:11<04:17,  3.30s/it, loss=4.5984, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:13<03:56,  3.08s/it, loss=4.5994, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:16<03:49,  3.02s/it, loss=4.5992, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:18<03:19,  2.66s/it, loss=4.5995, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:21<03:29,  2.83s/it, loss=4.5993, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:23<03:13,  2.65s/it, loss=4.5997, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [09:25<02:57,  2.46s/it, loss=4.5989, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [09:28<02:58,  2.52s/it, loss=4.5990, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [09:30<02:54,  2.49s/it, loss=4.5987, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [09:32<02:39,  2.32s/it, loss=4.5974, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [09:39<04:04,  3.60s/it, loss=4.5957, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [09:41<03:35,  3.21s/it, loss=4.5953, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [09:44<03:31,  3.20s/it, loss=4.5944, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [09:47<03:11,  2.95s/it, loss=4.5946, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [09:51<03:34,  3.35s/it, loss=4.5936, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [09:53<02:59,  2.85s/it, loss=4.5927, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [09:55<02:52,  2.79s/it, loss=4.5916, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [09:58<02:49,  2.78s/it, loss=4.5906, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:05<03:59,  4.00s/it, loss=4.5899, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [10:08<03:42,  3.77s/it, loss=4.5901, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [10:11<03:16,  3.39s/it, loss=4.5888, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [10:13<02:57,  3.11s/it, loss=4.5889, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:19<03:33,  3.81s/it, loss=4.5886, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:21<03:03,  3.35s/it, loss=4.5882, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:23<02:48,  3.12s/it, loss=4.5873, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:26<02:32,  2.87s/it, loss=4.5860, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [10:29<02:39,  3.07s/it, loss=4.5843, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [10:33<02:41,  3.16s/it, loss=4.5838, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [10:37<03:03,  3.66s/it, loss=4.5829, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [10:44<03:37,  4.45s/it, loss=4.5834, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [10:50<04:01,  5.02s/it, loss=4.5842, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [10:54<03:37,  4.62s/it, loss=4.5835, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [10:56<03:05,  4.02s/it, loss=4.5832, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:01<03:14,  4.31s/it, loss=4.5840, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:05<03:00,  4.10s/it, loss=4.5842, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [11:09<02:55,  4.07s/it, loss=4.5844, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [11:11<02:27,  3.52s/it, loss=4.5845, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [11:14<02:10,  3.17s/it, loss=4.5840, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [11:18<02:20,  3.50s/it, loss=4.5834, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:22<02:26,  3.77s/it, loss=4.5817, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:27<02:39,  4.20s/it, loss=4.5808, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:30<02:11,  3.56s/it, loss=4.5806, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:33<02:03,  3.44s/it, loss=4.5802, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:34<01:42,  2.93s/it, loss=4.5786, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:37<01:31,  2.70s/it, loss=4.5784, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [11:41<01:44,  3.17s/it, loss=4.5775, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [11:44<01:38,  3.08s/it, loss=4.5766, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [11:46<01:30,  2.91s/it, loss=4.5759, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [11:49<01:24,  2.80s/it, loss=4.5765, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [11:52<01:22,  2.85s/it, loss=4.5757, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [11:55<01:23,  2.98s/it, loss=4.5741, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [11:59<01:30,  3.33s/it, loss=4.5734, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:04<01:37,  3.75s/it, loss=4.5738, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:06<01:21,  3.27s/it, loss=4.5727, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [12:08<01:10,  2.93s/it, loss=4.5702, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [12:10<00:58,  2.54s/it, loss=4.5688, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [12:12<00:50,  2.32s/it, loss=4.5673, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [12:14<00:52,  2.49s/it, loss=4.5669, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [12:16<00:45,  2.26s/it, loss=4.5659, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [12:20<00:49,  2.63s/it, loss=4.5649, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [12:26<01:05,  3.65s/it, loss=4.5640, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [12:29<00:58,  3.44s/it, loss=4.5630, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [12:35<01:09,  4.32s/it, loss=4.5623, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [12:37<00:56,  3.74s/it, loss=4.5626, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [12:44<01:03,  4.51s/it, loss=4.5633, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [12:46<00:50,  3.90s/it, loss=4.5625, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [12:49<00:41,  3.48s/it, loss=4.5621, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [12:53<00:39,  3.61s/it, loss=4.5617, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [12:55<00:32,  3.22s/it, loss=4.5612, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [12:58<00:28,  3.17s/it, loss=4.5609, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [13:02<00:26,  3.37s/it, loss=4.5609, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:05<00:22,  3.22s/it, loss=4.5598, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [13:08<00:18,  3.09s/it, loss=4.5593, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [13:10<00:13,  2.79s/it, loss=4.5584, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [13:13<00:12,  3.02s/it, loss=4.5573, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [13:17<00:09,  3.20s/it, loss=4.5582, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [13:19<00:05,  2.95s/it, loss=4.5584, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [13:21<00:02,  2.75s/it, loss=4.5592, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [13:38<00:00,  3.27s/it, loss=4.5596, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:45,  7.28s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:54,  3.83s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:10<01:18,  2.72s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:10<00:55,  1.99s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:12<00:45,  1.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:13<00:38,  1.49s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:14<00:32,  1.30s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:15<00:28,  1.19s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:16<00:28,  1.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:17<00:23,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:18<00:22,  1.08s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:18<00:19,  1.04it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:20<00:23,  1.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:21<00:19,  1.08s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:22<00:17,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:23<00:15,  1.06it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:24<00:14,  1.02it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:25<00:13,  1.05it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:26<00:12,  1.05it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:26<00:11,  1.08it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:27<00:09,  1.18it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:28<00:08,  1.12it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:29<00:08,  1.03it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:30<00:08,  1.04s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:31<00:07,  1.01s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:33<00:06,  1.10s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:34<00:05,  1.04s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:47<00:00,  1.48s/it]\n",
      "Removed old checkpoint: model_epoch_3.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_3.pt\n",
      "Saved training results to training_results_20250511_164938.csv\n",
      "Train Loss: 4.5596\n",
      "Val Loss: 4.7856\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: this is a European?\n",
      "\n",
      "Epoch 4/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:09<39:53,  9.61s/it, loss=4.2619, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:14<27:22,  6.62s/it, loss=4.4483, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:16<18:36,  4.52s/it, loss=4.4775, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:21<20:04,  4.90s/it, loss=4.4230, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<16:46,  4.11s/it, loss=4.4028, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:27<15:24,  3.79s/it, loss=4.3945, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:29<13:02,  3.22s/it, loss=4.3887, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:31<11:51,  2.94s/it, loss=4.3815, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:36<13:23,  3.33s/it, loss=4.3741, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:37<11:31,  2.88s/it, loss=4.3694, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:40<11:33,  2.90s/it, loss=4.3655, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:43<11:38,  2.93s/it, loss=4.3752, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:46<11:04,  2.80s/it, loss=4.3786, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:48<09:47,  2.49s/it, loss=4.3811, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:50<09:40,  2.47s/it, loss=4.3725, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:52<08:45,  2.24s/it, loss=4.3649, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [00:54<08:53,  2.29s/it, loss=4.3750, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [00:56<08:34,  2.22s/it, loss=4.3708, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:00<09:47,  2.54s/it, loss=4.3698, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:06<13:43,  3.58s/it, loss=4.3698, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:08<12:06,  3.17s/it, loss=4.3649, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:11<11:36,  3.06s/it, loss=4.3785, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:13<10:28,  2.77s/it, loss=4.3731, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:15<10:10,  2.70s/it, loss=4.3752, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:21<13:51,  3.69s/it, loss=4.3781, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:28<17:04,  4.58s/it, loss=4.3820, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:30<14:38,  3.94s/it, loss=4.3816, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:36<16:07,  4.36s/it, loss=4.3905, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:38<14:21,  3.90s/it, loss=4.3904, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:41<12:26,  3.39s/it, loss=4.3893, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:44<11:46,  3.23s/it, loss=4.3947, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [01:50<15:28,  4.26s/it, loss=4.3953, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [01:54<14:50,  4.10s/it, loss=4.3978, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [01:57<13:07,  3.65s/it, loss=4.3998, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:00<12:52,  3.59s/it, loss=4.4007, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:02<11:35,  3.25s/it, loss=4.4014, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:06<11:29,  3.24s/it, loss=4.4017, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:08<10:18,  2.92s/it, loss=4.3989, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:11<10:53,  3.10s/it, loss=4.3970, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:14<10:19,  2.95s/it, loss=4.3937, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:16<09:47,  2.81s/it, loss=4.3915, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:19<09:52,  2.85s/it, loss=4.3924, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:25<12:39,  3.67s/it, loss=4.3944, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:29<12:36,  3.67s/it, loss=4.3914, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:33<13:15,  3.88s/it, loss=4.3931, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:35<11:41,  3.44s/it, loss=4.3955, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:39<11:37,  3.44s/it, loss=4.3957, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [02:45<14:46,  4.39s/it, loss=4.3923, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [02:49<13:47,  4.12s/it, loss=4.3922, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [02:52<13:04,  3.92s/it, loss=4.3966, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [02:59<15:52,  4.79s/it, loss=4.4013, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:02<13:34,  4.11s/it, loss=4.4018, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:05<12:24,  3.78s/it, loss=4.3987, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:11<14:19,  4.39s/it, loss=4.4011, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:14<13:36,  4.19s/it, loss=4.4017, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:19<14:15,  4.41s/it, loss=4.3977, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:22<12:45,  3.97s/it, loss=4.3949, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:27<13:45,  4.30s/it, loss=4.3950, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:30<11:52,  3.73s/it, loss=4.3944, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:37<14:56,  4.72s/it, loss=4.3924, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:42<15:04,  4.78s/it, loss=4.3895, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:44<12:59,  4.14s/it, loss=4.3856, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:47<11:18,  3.63s/it, loss=4.3816, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:53<13:59,  4.51s/it, loss=4.3813, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:56<12:17,  3.99s/it, loss=4.3760, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [04:00<12:23,  4.04s/it, loss=4.3747, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:02<10:38,  3.49s/it, loss=4.3721, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:05<10:04,  3.32s/it, loss=4.3708, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:09<10:33,  3.50s/it, loss=4.3704, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:12<09:30,  3.17s/it, loss=4.3713, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:17<11:00,  3.69s/it, loss=4.3705, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:19<10:10,  3.43s/it, loss=4.3701, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:22<09:14,  3.13s/it, loss=4.3676, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:24<08:13,  2.80s/it, loss=4.3683, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:26<07:32,  2.58s/it, loss=4.3665, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:30<08:47,  3.03s/it, loss=4.3665, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:34<09:31,  3.30s/it, loss=4.3673, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:39<10:57,  3.82s/it, loss=4.3680, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:42<10:00,  3.51s/it, loss=4.3694, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:45<09:25,  3.33s/it, loss=4.3707, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:47<08:27,  3.01s/it, loss=4.3699, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:51<09:10,  3.28s/it, loss=4.3686, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:55<10:12,  3.67s/it, loss=4.3684, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [05:00<10:58,  3.97s/it, loss=4.3665, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [05:03<09:51,  3.59s/it, loss=4.3649, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [05:05<09:00,  3.30s/it, loss=4.3610, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [05:08<08:04,  2.97s/it, loss=4.3604, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [05:11<08:00,  2.97s/it, loss=4.3587, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:18<11:25,  4.26s/it, loss=4.3607, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:20<09:26,  3.54s/it, loss=4.3605, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:24<09:48,  3.70s/it, loss=4.3599, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:27<09:02,  3.43s/it, loss=4.3584, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:30<09:00,  3.44s/it, loss=4.3598, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:32<07:31,  2.90s/it, loss=4.3557, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:34<07:07,  2.76s/it, loss=4.3581, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:36<06:31,  2.54s/it, loss=4.3550, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:38<06:09,  2.42s/it, loss=4.3547, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:42<07:15,  2.86s/it, loss=4.3573, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:45<06:52,  2.73s/it, loss=4.3534, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:53<10:57,  4.38s/it, loss=4.3560, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:55<09:06,  3.67s/it, loss=4.3546, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:57<08:02,  3.26s/it, loss=4.3519, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [06:00<07:42,  3.15s/it, loss=4.3497, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [06:03<07:23,  3.04s/it, loss=4.3509, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [06:05<06:41,  2.77s/it, loss=4.3501, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [06:08<07:05,  2.96s/it, loss=4.3496, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [06:11<07:04,  2.97s/it, loss=4.3491, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:14<06:44,  2.85s/it, loss=4.3488, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:16<06:19,  2.70s/it, loss=4.3491, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:20<07:17,  3.12s/it, loss=4.3469, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:23<07:02,  3.04s/it, loss=4.3479, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:26<06:40,  2.91s/it, loss=4.3468, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:34<10:15,  4.49s/it, loss=4.3472, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:36<08:44,  3.85s/it, loss=4.3462, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:38<07:30,  3.34s/it, loss=4.3454, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:43<07:59,  3.58s/it, loss=4.3444, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:49<09:55,  4.48s/it, loss=4.3447, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:56<11:09,  5.07s/it, loss=4.3452, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:59<09:45,  4.47s/it, loss=4.3465, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [07:00<07:54,  3.65s/it, loss=4.3460, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [07:03<07:03,  3.28s/it, loss=4.3467, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [07:06<06:36,  3.10s/it, loss=4.3484, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [07:08<06:19,  2.99s/it, loss=4.3479, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:10<05:32,  2.64s/it, loss=4.3455, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:13<05:31,  2.65s/it, loss=4.3447, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:15<05:31,  2.68s/it, loss=4.3442, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:17<04:54,  2.39s/it, loss=4.3426, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:21<05:32,  2.73s/it, loss=4.3413, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:23<05:05,  2.53s/it, loss=4.3405, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:25<05:00,  2.50s/it, loss=4.3400, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:28<05:07,  2.59s/it, loss=4.3417, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:30<04:40,  2.38s/it, loss=4.3399, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:32<04:15,  2.18s/it, loss=4.3389, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:37<06:07,  3.17s/it, loss=4.3378, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:41<06:16,  3.27s/it, loss=4.3370, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:43<05:49,  3.06s/it, loss=4.3367, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:45<05:13,  2.77s/it, loss=4.3355, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:48<05:05,  2.73s/it, loss=4.3346, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:50<04:42,  2.54s/it, loss=4.3343, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:52<04:28,  2.44s/it, loss=4.3347, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:56<05:24,  2.98s/it, loss=4.3328, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:58<04:50,  2.69s/it, loss=4.3333, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [08:03<05:44,  3.22s/it, loss=4.3331, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [08:06<05:22,  3.05s/it, loss=4.3314, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:11<06:24,  3.66s/it, loss=4.3312, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:13<05:44,  3.31s/it, loss=4.3307, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:15<05:00,  2.92s/it, loss=4.3288, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:17<04:33,  2.68s/it, loss=4.3297, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:20<04:32,  2.69s/it, loss=4.3295, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:25<05:48,  3.48s/it, loss=4.3293, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:28<05:23,  3.27s/it, loss=4.3301, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:32<05:46,  3.53s/it, loss=4.3301, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:35<05:07,  3.17s/it, loss=4.3296, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:40<06:13,  3.89s/it, loss=4.3289, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:42<05:14,  3.31s/it, loss=4.3282, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:46<05:30,  3.52s/it, loss=4.3291, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:49<05:14,  3.38s/it, loss=4.3285, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:52<04:56,  3.22s/it, loss=4.3305, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:55<04:46,  3.14s/it, loss=4.3308, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:58<04:34,  3.05s/it, loss=4.3312, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [09:03<05:34,  3.75s/it, loss=4.3313, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [09:06<05:10,  3.53s/it, loss=4.3324, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [09:08<04:24,  3.04s/it, loss=4.3307, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [09:10<04:03,  2.83s/it, loss=4.3296, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:14<04:24,  3.11s/it, loss=4.3299, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:19<04:58,  3.55s/it, loss=4.3300, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:22<04:44,  3.43s/it, loss=4.3292, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:26<04:51,  3.56s/it, loss=4.3303, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:28<04:04,  3.02s/it, loss=4.3293, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:31<04:08,  3.11s/it, loss=4.3292, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:38<05:36,  4.26s/it, loss=4.3271, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [09:40<04:36,  3.55s/it, loss=4.3265, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:42<04:12,  3.28s/it, loss=4.3274, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:44<03:42,  2.92s/it, loss=4.3272, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:46<03:14,  2.60s/it, loss=4.3278, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:49<03:17,  2.67s/it, loss=4.3275, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:51<03:07,  2.57s/it, loss=4.3282, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [09:54<02:56,  2.45s/it, loss=4.3274, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [09:57<03:05,  2.61s/it, loss=4.3278, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [09:59<03:01,  2.59s/it, loss=4.3280, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [10:01<02:45,  2.40s/it, loss=4.3268, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [10:08<04:13,  3.73s/it, loss=4.3254, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [10:10<03:42,  3.33s/it, loss=4.3254, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [10:13<03:18,  3.00s/it, loss=4.3246, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:15<02:59,  2.76s/it, loss=4.3248, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:18<03:03,  2.86s/it, loss=4.3241, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:20<02:37,  2.49s/it, loss=4.3233, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:22<02:35,  2.50s/it, loss=4.3224, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:25<02:38,  2.60s/it, loss=4.3217, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:32<03:54,  3.91s/it, loss=4.3212, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [10:35<03:33,  3.61s/it, loss=4.3216, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [10:38<03:15,  3.38s/it, loss=4.3205, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [10:41<03:09,  3.33s/it, loss=4.3209, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:46<03:31,  3.78s/it, loss=4.3207, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:48<03:03,  3.34s/it, loss=4.3204, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:51<02:48,  3.11s/it, loss=4.3198, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:53<02:33,  2.89s/it, loss=4.3188, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [10:56<02:34,  2.96s/it, loss=4.3176, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [10:59<02:32,  2.99s/it, loss=4.3172, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [11:03<02:47,  3.34s/it, loss=4.3163, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [11:09<03:19,  4.06s/it, loss=4.3168, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:15<03:46,  4.72s/it, loss=4.3178, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:18<03:14,  4.15s/it, loss=4.3172, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:21<02:48,  3.67s/it, loss=4.3169, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:24<02:39,  3.54s/it, loss=4.3177, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:28<02:37,  3.57s/it, loss=4.3179, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [11:32<02:48,  3.91s/it, loss=4.3183, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [11:35<02:24,  3.44s/it, loss=4.3185, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [11:37<02:08,  3.13s/it, loss=4.3183, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [11:41<02:19,  3.48s/it, loss=4.3178, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:45<02:15,  3.47s/it, loss=4.3164, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:48<02:06,  3.32s/it, loss=4.3155, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:50<01:50,  2.97s/it, loss=4.3152, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:52<01:41,  2.83s/it, loss=4.3151, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:54<01:27,  2.50s/it, loss=4.3136, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:56<01:21,  2.39s/it, loss=4.3138, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [12:00<01:28,  2.67s/it, loss=4.3131, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [12:04<01:38,  3.08s/it, loss=4.3123, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [12:06<01:32,  2.98s/it, loss=4.3117, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [12:09<01:26,  2.90s/it, loss=4.3124, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [12:13<01:29,  3.09s/it, loss=4.3116, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [12:15<01:20,  2.89s/it, loss=4.3102, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [12:18<01:19,  2.93s/it, loss=4.3096, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:22<01:21,  3.12s/it, loss=4.3102, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:24<01:11,  2.84s/it, loss=4.3091, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [12:26<01:03,  2.63s/it, loss=4.3067, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [12:28<00:53,  2.34s/it, loss=4.3052, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [12:29<00:48,  2.20s/it, loss=4.3041, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [12:32<00:49,  2.38s/it, loss=4.3037, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [12:34<00:45,  2.27s/it, loss=4.3030, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [12:39<00:57,  3.01s/it, loss=4.3023, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [13:13<03:42, 12.35s/it, loss=4.3015, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [13:15<02:36,  9.22s/it, loss=4.3004, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [13:22<02:14,  8.41s/it, loss=4.3001, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [13:24<01:38,  6.59s/it, loss=4.3004, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [13:30<01:28,  6.35s/it, loss=4.3013, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [13:32<01:07,  5.18s/it, loss=4.3005, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [13:35<00:52,  4.34s/it, loss=4.3000, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [13:37<00:42,  3.90s/it, loss=4.2998, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [13:40<00:34,  3.41s/it, loss=4.2995, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [13:42<00:27,  3.02s/it, loss=4.2994, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [13:45<00:23,  2.95s/it, loss=4.2997, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:47<00:19,  2.84s/it, loss=4.2986, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [13:50<00:16,  2.80s/it, loss=4.2984, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [13:52<00:12,  2.57s/it, loss=4.2976, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [13:55<00:10,  2.62s/it, loss=4.2966, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [13:59<00:09,  3.12s/it, loss=4.2978, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [14:01<00:05,  2.90s/it, loss=4.2982, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [14:04<00:02,  2.72s/it, loss=4.2990, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [14:21<00:00,  3.44s/it, loss=4.2997, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:06<03:27,  6.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:45,  3.53s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:09<01:13,  2.52s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:10<00:52,  1.88s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:11<00:44,  1.65s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:12<00:39,  1.52s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:13<00:33,  1.32s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:14<00:28,  1.21s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:15<00:28,  1.25s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:16<00:24,  1.10s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:17<00:22,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:18<00:19,  1.05it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:19<00:19,  1.00s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:20<00:16,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:21<00:15,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:21<00:13,  1.17it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:22<00:13,  1.07it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:23<00:12,  1.13it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:39<01:08,  5.30s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:40<00:47,  3.97s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:40<00:32,  2.97s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:41<00:23,  2.36s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:42<00:17,  2.00s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:44<00:13,  1.74s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:44<00:10,  1.48s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:46<00:08,  1.42s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:47<00:07,  1.43s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [01:01<00:00,  1.92s/it]\n",
      "Removed old checkpoint: model_epoch_4.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_4.pt\n",
      "Saved training results to training_results_20250511_170503.csv\n",
      "Train Loss: 4.2997\n",
      "Val Loss: 4.6950\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: ?\n",
      "\n",
      "Epoch 5/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:09<37:28,  9.03s/it, loss=4.0338, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:13<25:24,  6.15s/it, loss=4.2340, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:15<17:27,  4.24s/it, loss=4.2513, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:19<18:17,  4.46s/it, loss=4.1844, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:21<14:25,  3.53s/it, loss=4.1657, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:38<32:23,  7.97s/it, loss=4.1650, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:40<24:29,  6.05s/it, loss=4.1601, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:43<19:59,  4.95s/it, loss=4.1548, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:49<21:39,  5.39s/it, loss=4.1421, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:51<17:14,  4.31s/it, loss=4.1271, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:54<15:17,  3.84s/it, loss=4.1213, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:57<14:08,  3.57s/it, loss=4.1291, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:59<12:45,  3.23s/it, loss=4.1362, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [01:01<10:57,  2.79s/it, loss=4.1399, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [01:02<09:32,  2.44s/it, loss=4.1296, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [01:04<08:35,  2.20s/it, loss=4.1212, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [01:06<08:43,  2.25s/it, loss=4.1309, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:08<08:27,  2.19s/it, loss=4.1270, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:12<09:27,  2.45s/it, loss=4.1260, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:18<14:08,  3.69s/it, loss=4.1259, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:20<12:19,  3.23s/it, loss=4.1218, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [10:05<10:06:53, 159.71s/it, loss=4.1351, tokeMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [10:07<7:05:24, 112.44s/it, loss=4.1269, tokenMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [10:10<4:59:16, 79.45s/it, loss=4.1284, tokensMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [10:14<3:33:50, 57.02s/it, loss=4.1328, tokensMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [10:21<2:36:51, 42.02s/it, loss=4.1368, tokensMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [10:24<1:51:55, 30.11s/it, loss=4.1370, tokensMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [10:29<1:24:28, 22.83s/it, loss=4.1472, tokensMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [10:32<1:02:00, 16.84s/it, loss=4.1482, tokensMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [10:35<45:42, 12.47s/it, loss=4.1479, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [10:37<34:50,  9.55s/it, loss=4.1536, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [10:45<32:34,  8.97s/it, loss=4.1522, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [10:48<25:51,  7.15s/it, loss=4.1548, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [10:50<20:41,  5.75s/it, loss=4.1578, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [10:54<17:50,  4.98s/it, loss=4.1589, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [10:56<14:56,  4.19s/it, loss=4.1607, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [10:59<13:27,  3.79s/it, loss=4.1620, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [11:01<11:35,  3.28s/it, loss=4.1594, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [11:04<11:42,  3.33s/it, loss=4.1572, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [11:07<10:51,  3.10s/it, loss=4.1535, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [11:27<28:38,  8.22s/it, loss=4.1517, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [11:30<22:49,  6.58s/it, loss=4.1529, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [11:34<20:20,  5.89s/it, loss=4.1534, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [11:36<16:33,  4.82s/it, loss=4.1502, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [11:40<15:10,  4.44s/it, loss=4.1526, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [11:42<12:47,  3.76s/it, loss=4.1543, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [11:45<11:39,  3.45s/it, loss=4.1557, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [11:50<13:34,  4.03s/it, loss=4.1526, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [11:53<12:06,  3.62s/it, loss=4.1516, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [11:56<11:09,  3.35s/it, loss=4.1565, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [12:01<13:05,  3.95s/it, loss=4.1614, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [12:03<11:30,  3.49s/it, loss=4.1624, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [12:06<10:43,  3.27s/it, loss=4.1598, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [12:12<13:29,  4.13s/it, loss=4.1637, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [12:29<26:14,  8.07s/it, loss=4.1644, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [12:33<21:44,  6.73s/it, loss=4.1603, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [12:35<17:09,  5.33s/it, loss=4.1577, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [12:39<15:57,  4.99s/it, loss=4.1585, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [12:41<13:01,  4.09s/it, loss=4.1582, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [12:44<11:56,  3.77s/it, loss=4.1565, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [12:47<11:06,  3.52s/it, loss=4.1538, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [12:50<10:06,  3.22s/it, loss=4.1494, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [12:52<09:08,  2.94s/it, loss=4.1455, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [12:57<11:11,  3.61s/it, loss=4.1460, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [13:00<10:11,  3.30s/it, loss=4.1407, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [13:04<11:15,  3.67s/it, loss=4.1393, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [13:06<09:45,  3.20s/it, loss=4.1379, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [13:09<09:10,  3.02s/it, loss=4.1361, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [13:12<08:49,  2.93s/it, loss=4.1356, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [13:28<20:38,  6.88s/it, loss=4.1370, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [13:31<17:08,  5.74s/it, loss=4.1366, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [13:33<13:52,  4.67s/it, loss=4.1357, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [13:35<11:19,  3.84s/it, loss=4.1334, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [13:37<09:33,  3.26s/it, loss=4.1344, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [13:39<08:21,  2.86s/it, loss=4.1321, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [13:42<08:17,  2.86s/it, loss=4.1320, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [13:44<07:59,  2.77s/it, loss=4.1330, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [13:50<10:13,  3.56s/it, loss=4.1342, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [13:52<09:22,  3.29s/it, loss=4.1351, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [13:55<08:55,  3.15s/it, loss=4.1361, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [13:58<08:08,  2.89s/it, loss=4.1347, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [14:01<08:16,  2.96s/it, loss=4.1328, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [14:04<08:25,  3.03s/it, loss=4.1326, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [14:08<09:03,  3.27s/it, loss=4.1306, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [14:10<08:33,  3.11s/it, loss=4.1296, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [14:13<08:05,  2.96s/it, loss=4.1257, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [14:29<18:27,  6.79s/it, loss=4.1249, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [14:31<14:47,  5.48s/it, loss=4.1235, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [14:38<15:28,  5.76s/it, loss=4.1263, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [14:39<12:11,  4.57s/it, loss=4.1264, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [14:43<11:03,  4.17s/it, loss=4.1260, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [14:45<09:56,  3.77s/it, loss=4.1244, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [14:49<09:45,  3.73s/it, loss=4.1261, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [14:51<08:07,  3.13s/it, loss=4.1221, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [14:53<07:39,  2.97s/it, loss=4.1240, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [14:55<06:55,  2.70s/it, loss=4.1209, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [14:58<06:33,  2.57s/it, loss=4.1208, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [15:02<08:01,  3.17s/it, loss=4.1238, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [15:05<07:26,  2.96s/it, loss=4.1200, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [15:12<10:20,  4.14s/it, loss=4.1226, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [15:14<08:45,  3.53s/it, loss=4.1210, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [15:31<18:36,  7.55s/it, loss=4.1191, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [15:34<15:33,  6.35s/it, loss=4.1174, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [15:37<13:04,  5.37s/it, loss=4.1186, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [15:40<10:38,  4.40s/it, loss=4.1181, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [15:43<10:05,  4.20s/it, loss=4.1172, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [15:46<08:59,  3.77s/it, loss=4.1171, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [15:48<07:57,  3.36s/it, loss=4.1174, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [15:51<07:08,  3.04s/it, loss=4.1180, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [15:55<07:50,  3.36s/it, loss=4.1155, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [15:57<07:18,  3.15s/it, loss=4.1167, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [16:00<06:48,  2.96s/it, loss=4.1157, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [16:08<10:12,  4.47s/it, loss=4.1163, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [16:10<08:39,  3.82s/it, loss=4.1156, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [16:12<07:23,  3.29s/it, loss=4.1155, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [16:32<18:05,  8.10s/it, loss=4.1147, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [16:38<16:53,  7.62s/it, loss=4.1149, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [16:45<15:58,  7.26s/it, loss=4.1156, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [16:48<13:12,  6.05s/it, loss=4.1171, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [16:50<10:22,  4.79s/it, loss=4.1165, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [16:52<08:52,  4.12s/it, loss=4.1178, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [16:55<07:54,  3.71s/it, loss=4.1196, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [16:58<07:16,  3.44s/it, loss=4.1191, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [17:00<06:14,  2.97s/it, loss=4.1168, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [17:03<06:28,  3.11s/it, loss=4.1162, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [17:06<06:24,  3.10s/it, loss=4.1156, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [17:08<05:33,  2.71s/it, loss=4.1142, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [17:11<05:46,  2.84s/it, loss=4.1132, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [17:13<05:19,  2.64s/it, loss=4.1129, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [17:16<05:14,  2.62s/it, loss=4.1128, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [17:32<13:14,  6.68s/it, loss=4.1143, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [17:34<10:20,  5.26s/it, loss=4.1125, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [17:36<08:09,  4.18s/it, loss=4.1120, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [17:41<08:40,  4.48s/it, loss=4.1113, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [17:44<07:56,  4.15s/it, loss=4.1106, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [17:47<07:03,  3.72s/it, loss=4.1107, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [17:49<06:09,  3.27s/it, loss=4.1100, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [17:52<06:02,  3.24s/it, loss=4.1090, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [17:54<05:21,  2.90s/it, loss=4.1090, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [17:57<05:01,  2.74s/it, loss=4.1095, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [18:00<05:25,  2.98s/it, loss=4.1077, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [18:02<04:53,  2.71s/it, loss=4.1083, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [18:06<05:18,  2.98s/it, loss=4.1082, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [18:09<05:08,  2.91s/it, loss=4.1068, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [18:14<06:12,  3.55s/it, loss=4.1069, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [18:16<05:39,  3.26s/it, loss=4.1067, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [18:33<12:16,  7.15s/it, loss=4.1054, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [18:35<09:37,  5.66s/it, loss=4.1066, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [18:37<08:02,  4.78s/it, loss=4.1062, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [18:42<07:55,  4.76s/it, loss=4.1063, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [18:45<06:59,  4.23s/it, loss=4.1070, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [18:50<07:20,  4.50s/it, loss=4.1072, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [18:53<06:13,  3.85s/it, loss=4.1069, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [18:57<06:33,  4.10s/it, loss=4.1067, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [18:59<05:31,  3.49s/it, loss=4.1059, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [19:03<05:21,  3.42s/it, loss=4.1069, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [19:06<05:05,  3.28s/it, loss=4.1066, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [19:09<04:55,  3.21s/it, loss=4.1089, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [19:12<04:51,  3.20s/it, loss=4.1093, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [19:15<04:46,  3.18s/it, loss=4.1097, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [19:21<06:10,  4.17s/it, loss=4.1100, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [19:26<06:16,  4.27s/it, loss=4.1114, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [19:28<05:15,  3.62s/it, loss=4.1100, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [19:31<04:48,  3.35s/it, loss=4.1091, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [19:36<05:23,  3.80s/it, loss=4.1095, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [19:39<05:13,  3.73s/it, loss=4.1097, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [19:42<04:52,  3.52s/it, loss=4.1090, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [19:46<04:54,  3.59s/it, loss=4.1102, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [19:48<04:15,  3.15s/it, loss=4.1091, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [19:51<04:14,  3.18s/it, loss=4.1092, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [19:59<06:09,  4.67s/it, loss=4.1073, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [20:02<05:07,  3.94s/it, loss=4.1067, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [20:05<04:47,  3.74s/it, loss=4.1075, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [20:08<04:16,  3.38s/it, loss=4.1077, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [20:10<03:48,  3.05s/it, loss=4.1087, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [20:13<03:50,  3.12s/it, loss=4.1084, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [20:16<03:39,  3.01s/it, loss=4.1091, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [20:18<03:27,  2.88s/it, loss=4.1085, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [20:21<03:25,  2.90s/it, loss=4.1092, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [20:24<03:21,  2.88s/it, loss=4.1092, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [20:27<03:08,  2.72s/it, loss=4.1083, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [20:34<04:41,  4.14s/it, loss=4.1070, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [20:37<04:20,  3.89s/it, loss=4.1069, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [20:40<04:00,  3.65s/it, loss=4.1062, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [20:43<03:37,  3.34s/it, loss=4.1067, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [20:48<03:56,  3.70s/it, loss=4.1062, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [20:50<03:21,  3.20s/it, loss=4.1052, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [20:53<03:24,  3.30s/it, loss=4.1044, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [20:58<03:55,  3.86s/it, loss=4.1038, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [21:06<04:58,  4.97s/it, loss=4.1033, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [21:11<04:50,  4.93s/it, loss=4.1036, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [21:15<04:32,  4.70s/it, loss=4.1025, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [21:19<04:17,  4.51s/it, loss=4.1027, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [21:24<04:17,  4.60s/it, loss=4.1026, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [21:28<04:04,  4.45s/it, loss=4.1024, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [21:33<04:06,  4.56s/it, loss=4.1016, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [21:37<03:57,  4.47s/it, loss=4.1004, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [21:43<04:15,  4.92s/it, loss=4.0990, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [21:49<04:22,  5.15s/it, loss=4.0984, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [21:56<04:58,  5.97s/it, loss=4.0973, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [22:06<05:42,  7.00s/it, loss=4.0978, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [22:15<06:11,  7.74s/it, loss=4.0985, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [22:20<05:20,  6.82s/it, loss=4.0977, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [22:24<04:36,  6.01s/it, loss=4.0974, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [22:32<04:50,  6.46s/it, loss=4.0979, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [22:55<08:33, 11.68s/it, loss=4.0981, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [23:00<06:53,  9.61s/it, loss=4.0987, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [23:03<05:12,  7.44s/it, loss=4.0988, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [23:05<04:07,  6.03s/it, loss=4.0986, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [23:10<03:44,  5.60s/it, loss=4.0980, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [23:14<03:18,  5.09s/it, loss=4.0967, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [23:17<02:52,  4.53s/it, loss=4.0955, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [23:19<02:21,  3.84s/it, loss=4.0952, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [23:22<02:04,  3.46s/it, loss=4.0952, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [23:24<01:46,  3.03s/it, loss=4.0938, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [23:27<01:41,  2.98s/it, loss=4.0938, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [23:32<02:02,  3.72s/it, loss=4.0934, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [23:36<01:58,  3.71s/it, loss=4.0928, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [23:39<01:47,  3.46s/it, loss=4.0922, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [23:42<01:38,  3.30s/it, loss=4.0929, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [23:45<01:36,  3.33s/it, loss=4.0922, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [23:48<01:29,  3.19s/it, loss=4.0908, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [23:51<01:26,  3.20s/it, loss=4.0903, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [23:56<01:33,  3.59s/it, loss=4.0908, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [23:58<01:21,  3.25s/it, loss=4.0899, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [24:01<01:11,  3.00s/it, loss=4.0875, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [24:03<01:02,  2.71s/it, loss=4.0862, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [24:05<01:00,  2.74s/it, loss=4.0849, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [24:11<01:13,  3.51s/it, loss=4.0846, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [24:13<01:02,  3.11s/it, loss=4.0839, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [24:18<01:08,  3.63s/it, loss=4.0835, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [24:24<01:18,  4.39s/it, loss=4.0828, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [24:26<01:04,  3.81s/it, loss=4.0818, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [24:33<01:14,  4.65s/it, loss=4.0813, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [24:36<01:02,  4.15s/it, loss=4.0817, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [24:43<01:09,  4.95s/it, loss=4.0829, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [24:46<00:56,  4.38s/it, loss=4.0824, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [24:49<00:48,  4.03s/it, loss=4.0820, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [24:55<00:52,  4.75s/it, loss=4.0821, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [25:00<00:47,  4.79s/it, loss=4.0817, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [25:04<00:40,  4.53s/it, loss=4.0816, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [25:11<00:40,  5.07s/it, loss=4.0821, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [25:15<00:35,  5.01s/it, loss=4.0812, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [25:20<00:29,  4.96s/it, loss=4.0809, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [25:24<00:23,  4.61s/it, loss=4.0802, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [25:30<00:19,  4.91s/it, loss=4.0792, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [25:35<00:15,  5.15s/it, loss=4.0804, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [25:40<00:09,  4.90s/it, loss=4.0805, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [25:44<00:04,  4.85s/it, loss=4.0812, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [26:06<00:00,  6.27s/it, loss=4.0819, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size:Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      " 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:12<06:25, 12.43s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:14<03:14,  6.49s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:17<02:13,  4.60s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:18<01:35,  3.40s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:20<01:17,  2.89s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:22<01:06,  2.56s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:24<00:55,  2.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:25<00:49,  2.05s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:28<00:49,  2.17s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:29<00:42,  1.94s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:31<00:41,  1.99s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:33<00:35,  1.77s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:35<00:36,  1.90s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:36<00:32,  1.78s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:38<00:30,  1.81s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:40<00:27,  1.71s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:42<00:27,  1.86s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:43<00:25,  1.79s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:45<00:23,  1.81s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:47<00:21,  1.80s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:48<00:18,  1.69s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:51<00:18,  1.81s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:54<00:19,  2.21s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:56<00:18,  2.26s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:58<00:15,  2.15s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [01:01<00:13,  2.27s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [01:02<00:10,  2.12s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [01:18<00:00,  2.47s/it]\n",
      "Removed old checkpoint: model_epoch_5.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_5.pt\n",
      "Saved training results to training_results_20250511_173231.csv\n",
      "Train Loss: 4.0819\n",
      "Val Loss: 4.6384\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: this is the European Parliament?\n",
      "\n",
      "Epoch 6/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size:Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      " 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:17<1:11:31, 17.24s/it, loss=3.8244, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:24<46:07, 11.16s/it, loss=4.0225, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:27<31:32,  7.66s/it, loss=4.0171, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:35<32:02,  7.81s/it, loss=3.9636, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:38<25:06,  6.15s/it, loss=3.9465, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:45<26:06,  6.42s/it, loss=3.9520, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:49<22:08,  5.47s/it, loss=3.9372, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:53<20:24,  5.06s/it, loss=3.9302, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [01:02<25:02,  6.24s/it, loss=3.9190, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [01:06<22:47,  5.70s/it, loss=3.9031, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [01:14<25:04,  6.30s/it, loss=3.8980, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [01:22<26:42,  6.74s/it, loss=3.9085, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [01:27<24:29,  6.20s/it, loss=3.9164, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [01:30<21:19,  5.42s/it, loss=3.9190, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [01:34<18:44,  4.78s/it, loss=3.9045, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [01:37<17:16,  4.43s/it, loss=3.8981, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [01:42<17:35,  4.53s/it, loss=3.9067, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:46<17:09,  4.44s/it, loss=3.9044, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:53<20:16,  5.26s/it, loss=3.9055, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [02:07<29:32,  7.70s/it, loss=3.9045, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [02:11<24:53,  6.52s/it, loss=3.8992, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [02:16<24:00,  6.32s/it, loss=3.9126, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [02:20<20:28,  5.41s/it, loss=3.9042, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [02:24<18:59,  5.04s/it, loss=3.9051, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [02:30<19:47,  5.28s/it, loss=3.9103, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [02:40<25:07,  6.73s/it, loss=3.9144, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [02:43<21:29,  5.78s/it, loss=3.9155, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [02:52<24:02,  6.50s/it, loss=3.9266, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [02:57<22:53,  6.21s/it, loss=3.9273, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [03:01<19:52,  5.42s/it, loss=3.9276, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [03:06<19:10,  5.25s/it, loss=3.9338, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [03:15<23:27,  6.45s/it, loss=3.9336, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [03:19<20:43,  5.73s/it, loss=3.9370, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [03:22<18:20,  5.09s/it, loss=3.9405, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [03:27<17:31,  4.89s/it, loss=3.9412, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [03:30<16:01,  4.49s/it, loss=3.9428, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [03:34<15:27,  4.35s/it, loss=3.9444, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [03:38<14:12,  4.02s/it, loss=3.9411, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [03:44<16:04,  4.57s/it, loss=3.9389, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [03:48<16:08,  4.61s/it, loss=3.9357, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [03:52<15:06,  4.34s/it, loss=3.9338, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [03:56<14:50,  4.28s/it, loss=3.9359, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [04:02<16:21,  4.74s/it, loss=3.9365, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [04:05<14:56,  4.35s/it, loss=3.9328, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [04:10<14:42,  4.30s/it, loss=3.9364, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [04:13<13:40,  4.02s/it, loss=3.9383, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [04:17<13:20,  3.94s/it, loss=3.9399, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [04:24<16:54,  5.02s/it, loss=3.9383, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [04:30<17:32,  5.24s/it, loss=3.9392, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [04:36<18:03,  5.42s/it, loss=3.9432, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [04:44<20:25,  6.16s/it, loss=3.9493, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [04:47<17:39,  5.35s/it, loss=3.9512, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [04:51<16:20,  4.98s/it, loss=3.9487, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [04:59<18:28,  5.65s/it, loss=3.9528, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [05:03<16:49,  5.17s/it, loss=3.9531, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [05:07<15:54,  4.92s/it, loss=3.9487, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [05:10<14:11,  4.41s/it, loss=3.9452, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [05:16<15:46,  4.93s/it, loss=3.9467, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [05:20<14:29,  4.55s/it, loss=3.9466, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [05:27<16:20,  5.16s/it, loss=3.9444, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [05:32<16:10,  5.14s/it, loss=3.9422, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [05:35<14:51,  4.74s/it, loss=3.9384, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [05:39<13:34,  4.36s/it, loss=3.9349, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [05:46<16:19,  5.27s/it, loss=3.9362, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [05:50<15:12,  4.93s/it, loss=3.9311, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [05:57<16:34,  5.41s/it, loss=3.9303, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [06:00<14:27,  4.74s/it, loss=3.9291, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [06:04<13:22,  4.41s/it, loss=3.9273, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [06:08<13:15,  4.39s/it, loss=3.9268, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [06:12<13:06,  4.37s/it, loss=3.9287, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [06:18<14:20,  4.81s/it, loss=3.9279, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [06:22<13:04,  4.41s/it, loss=3.9266, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [06:25<11:47,  4.00s/it, loss=3.9245, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [06:28<10:49,  3.69s/it, loss=3.9261, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [06:31<10:08,  3.48s/it, loss=3.9235, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [06:37<12:16,  4.23s/it, loss=3.9235, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [06:41<12:09,  4.22s/it, loss=3.9241, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [06:47<14:00,  4.89s/it, loss=3.9252, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [06:52<13:32,  4.75s/it, loss=3.9266, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [06:58<14:30,  5.12s/it, loss=3.9277, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [07:01<12:52,  4.57s/it, loss=3.9258, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [07:06<13:29,  4.82s/it, loss=3.9241, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [07:11<13:16,  4.77s/it, loss=3.9237, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [07:16<13:24,  4.85s/it, loss=3.9212, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [07:20<12:31,  4.55s/it, loss=3.9206, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [07:23<11:32,  4.22s/it, loss=3.9168, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [07:26<10:18,  3.79s/it, loss=3.9158, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [07:30<09:51,  3.65s/it, loss=3.9146, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [07:38<13:50,  5.16s/it, loss=3.9174, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [07:41<12:13,  4.58s/it, loss=3.9170, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [07:47<13:01,  4.92s/it, loss=3.9171, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [07:52<12:48,  4.87s/it, loss=3.9157, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [07:57<12:43,  4.86s/it, loss=3.9168, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [07:59<10:51,  4.17s/it, loss=3.9128, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [08:03<10:24,  4.03s/it, loss=3.9151, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [08:06<09:35,  3.74s/it, loss=3.9117, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [08:09<09:14,  3.63s/it, loss=3.9116, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [08:15<11:00,  4.34s/it, loss=3.9147, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [08:19<10:25,  4.15s/it, loss=3.9116, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [08:29<14:20,  5.74s/it, loss=3.9141, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [08:32<12:34,  5.06s/it, loss=3.9122, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [08:37<12:12,  4.95s/it, loss=3.9096, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [08:42<12:06,  4.94s/it, loss=3.9074, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [08:46<11:33,  4.75s/it, loss=3.9087, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [08:49<10:31,  4.35s/it, loss=3.9087, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [08:55<11:37,  4.84s/it, loss=3.9076, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [09:00<11:12,  4.70s/it, loss=3.9070, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [09:04<10:30,  4.44s/it, loss=3.9071, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [09:07<09:49,  4.18s/it, loss=3.9075, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [09:14<11:22,  4.87s/it, loss=3.9047, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [09:18<11:11,  4.83s/it, loss=3.9056, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [09:23<11:01,  4.79s/it, loss=3.9045, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [09:32<14:01,  6.14s/it, loss=3.9050, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [09:36<12:24,  5.47s/it, loss=3.9042, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [09:39<10:45,  4.78s/it, loss=3.9041, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [09:45<11:16,  5.05s/it, loss=3.9034, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [09:54<13:27,  6.07s/it, loss=3.9037, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [10:03<15:30,  7.05s/it, loss=3.9043, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [10:08<13:50,  6.34s/it, loss=3.9059, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [10:11<11:33,  5.33s/it, loss=3.9055, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [10:15<10:50,  5.04s/it, loss=3.9065, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [10:19<10:17,  4.82s/it, loss=3.9083, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [10:24<09:58,  4.72s/it, loss=3.9078, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [10:27<08:39,  4.12s/it, loss=3.9056, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [10:31<08:48,  4.23s/it, loss=3.9052, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [10:35<08:37,  4.18s/it, loss=3.9050, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [10:38<07:48,  3.81s/it, loss=3.9036, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [10:42<08:06,  3.99s/it, loss=3.9025, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [10:45<07:25,  3.68s/it, loss=3.9021, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [10:49<07:08,  3.57s/it, loss=3.9021, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [10:53<07:27,  3.76s/it, loss=3.9036, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [10:56<07:12,  3.67s/it, loss=3.9016, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [10:59<06:33,  3.36s/it, loss=3.9009, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [11:06<08:47,  4.55s/it, loss=3.9001, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [11:11<08:57,  4.68s/it, loss=3.8994, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [11:15<08:21,  4.40s/it, loss=3.8991, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [11:18<07:29,  3.97s/it, loss=3.8986, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [11:22<07:12,  3.86s/it, loss=3.8974, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [11:25<06:40,  3.61s/it, loss=3.8970, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [11:28<06:24,  3.50s/it, loss=3.8977, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [11:33<07:25,  4.08s/it, loss=3.8961, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [11:37<06:57,  3.87s/it, loss=3.8962, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [11:43<08:02,  4.51s/it, loss=3.8961, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [11:46<07:35,  4.30s/it, loss=3.8948, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [11:53<08:37,  4.93s/it, loss=3.8951, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [11:56<07:48,  4.51s/it, loss=3.8951, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [11:59<06:57,  4.05s/it, loss=3.8936, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [12:02<06:21,  3.74s/it, loss=3.8946, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [12:06<06:19,  3.76s/it, loss=3.8943, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [12:14<08:17,  4.98s/it, loss=3.8946, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [12:18<07:48,  4.74s/it, loss=3.8953, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [12:24<08:23,  5.14s/it, loss=3.8955, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [12:28<07:23,  4.57s/it, loss=3.8950, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [12:33<07:54,  4.94s/it, loss=3.8949, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [12:37<06:59,  4.41s/it, loss=3.8940, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [12:40<06:38,  4.24s/it, loss=3.8948, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [12:44<06:29,  4.19s/it, loss=3.8942, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [12:48<06:17,  4.10s/it, loss=3.8963, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [12:53<06:31,  4.30s/it, loss=3.8966, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [12:58<06:44,  4.49s/it, loss=3.8976, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [13:05<07:49,  5.28s/it, loss=3.8979, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [13:09<07:17,  4.97s/it, loss=3.8993, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [13:12<06:17,  4.34s/it, loss=3.8979, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [13:16<05:47,  4.04s/it, loss=3.8972, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [13:20<06:01,  4.25s/it, loss=3.8977, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [13:25<05:59,  4.28s/it, loss=3.8978, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [13:28<05:41,  4.11s/it, loss=3.8972, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [13:33<05:41,  4.16s/it, loss=3.8983, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [13:35<05:02,  3.74s/it, loss=3.8972, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [13:41<05:33,  4.17s/it, loss=3.8972, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [13:49<07:12,  5.47s/it, loss=3.8955, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [13:52<06:02,  4.65s/it, loss=3.8949, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [13:55<05:33,  4.34s/it, loss=3.8958, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [13:58<04:58,  3.93s/it, loss=3.8959, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [14:01<04:25,  3.54s/it, loss=3.8971, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [14:05<04:25,  3.59s/it, loss=3.8970, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [14:08<04:16,  3.51s/it, loss=3.8977, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [14:11<03:58,  3.31s/it, loss=3.8975, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [14:15<04:03,  3.43s/it, loss=3.8984, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [14:19<04:14,  3.63s/it, loss=3.8985, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [14:22<03:52,  3.37s/it, loss=3.8973, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [14:30<05:36,  4.95s/it, loss=3.8965, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [14:33<04:56,  4.42s/it, loss=3.8966, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [14:37<04:33,  4.14s/it, loss=3.8959, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [14:40<04:05,  3.78s/it, loss=3.8966, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [14:44<04:08,  3.88s/it, loss=3.8965, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [14:46<03:36,  3.44s/it, loss=3.8957, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [14:50<03:36,  3.49s/it, loss=3.8949, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [14:54<03:37,  3.56s/it, loss=3.8942, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [15:04<05:38,  5.63s/it, loss=3.8938, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [15:09<05:11,  5.27s/it, loss=3.8942, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [15:12<04:38,  4.79s/it, loss=3.8930, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [15:16<04:11,  4.42s/it, loss=3.8931, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [15:20<04:03,  4.35s/it, loss=3.8930, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [15:23<03:39,  4.00s/it, loss=3.8927, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [15:27<03:31,  3.92s/it, loss=3.8918, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [15:30<03:19,  3.77s/it, loss=3.8908, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [15:34<03:22,  3.89s/it, loss=3.8895, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [15:40<03:50,  4.52s/it, loss=3.8892, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [15:47<04:22,  5.24s/it, loss=3.8882, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [15:55<04:48,  5.88s/it, loss=3.8887, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [16:02<05:04,  6.34s/it, loss=3.8895, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [16:06<04:25,  5.64s/it, loss=3.8885, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [16:10<03:52,  5.05s/it, loss=3.8878, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [16:14<03:36,  4.81s/it, loss=3.8884, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [16:18<03:22,  4.61s/it, loss=3.8886, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [16:22<03:14,  4.52s/it, loss=3.8894, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [16:26<02:54,  4.16s/it, loss=3.8896, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [16:30<02:49,  4.14s/it, loss=3.8896, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [16:36<03:05,  4.63s/it, loss=3.8889, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [16:42<03:16,  5.03s/it, loss=3.8874, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [16:46<03:08,  4.96s/it, loss=3.8862, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [16:49<02:41,  4.36s/it, loss=3.8857, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [16:53<02:28,  4.14s/it, loss=3.8857, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [16:56<02:09,  3.70s/it, loss=3.8845, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [16:59<02:00,  3.55s/it, loss=3.8842, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [17:04<02:08,  3.88s/it, loss=3.8838, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [17:08<02:11,  4.10s/it, loss=3.8833, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [17:13<02:09,  4.19s/it, loss=3.8828, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [17:16<02:00,  4.01s/it, loss=3.8834, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [17:21<02:04,  4.28s/it, loss=3.8828, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [17:25<01:53,  4.06s/it, loss=3.8813, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [17:29<01:48,  4.03s/it, loss=3.8807, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [17:33<01:51,  4.28s/it, loss=3.8813, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [17:37<01:41,  4.05s/it, loss=3.8802, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [17:40<01:30,  3.78s/it, loss=3.8779, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [17:43<01:17,  3.38s/it, loss=3.8763, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [17:46<01:13,  3.34s/it, loss=3.8751, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [17:51<01:20,  3.82s/it, loss=3.8752, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [17:53<01:09,  3.47s/it, loss=3.8743, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [17:59<01:17,  4.09s/it, loss=3.8739, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [18:06<01:31,  5.06s/it, loss=3.8735, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [18:09<01:14,  4.38s/it, loss=3.8728, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [18:16<01:22,  5.13s/it, loss=3.8724, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [18:19<01:09,  4.65s/it, loss=3.8729, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [18:27<01:16,  5.47s/it, loss=3.8742, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [18:31<01:05,  5.05s/it, loss=3.8738, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [18:35<00:58,  4.84s/it, loss=3.8737, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [18:42<00:58,  5.31s/it, loss=3.8741, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [18:45<00:47,  4.75s/it, loss=3.8739, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [18:48<00:38,  4.27s/it, loss=3.8738, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [18:53<00:34,  4.31s/it, loss=3.8745, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [18:56<00:28,  4.03s/it, loss=3.8737, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [19:00<00:23,  3.95s/it, loss=3.8735, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [19:03<00:18,  3.70s/it, loss=3.8729, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [19:07<00:15,  3.91s/it, loss=3.8721, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [19:12<00:12,  4.01s/it, loss=3.8732, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [19:15<00:07,  3.98s/it, loss=3.8733, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [19:19<00:03,  3.97s/it, loss=3.8741, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [19:38<00:00,  4.71s/it, loss=3.8747, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:06<03:19,  6.45s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:07<01:43,  3.44s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:09<01:11,  2.47s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:09<00:50,  1.81s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:11<00:42,  1.57s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:12<00:36,  1.42s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:13<00:31,  1.26s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:14<00:29,  1.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:16<00:32,  1.40s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:17<00:30,  1.38s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:19<00:31,  1.51s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:20<00:30,  1.53s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:22<00:29,  1.54s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:23<00:24,  1.38s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:24<00:22,  1.34s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:25<00:19,  1.25s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:27<00:20,  1.34s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:28<00:18,  1.30s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:29<00:16,  1.29s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:30<00:15,  1.26s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:31<00:12,  1.17s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:33<00:11,  1.20s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:34<00:11,  1.33s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:36<00:11,  1.44s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:37<00:09,  1.38s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:39<00:09,  1.53s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:40<00:07,  1.51s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:55<00:00,  1.74s/it]\n",
      "Removed old checkpoint: model_epoch_6.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_6.pt\n",
      "Saved training results to training_results_20250511_175307.csv\n",
      "Train Loss: 3.8747\n",
      "Val Loss: 4.6204\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: this is the European Parliament?\n",
      "\n",
      "Epoch 7/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:08<37:11,  8.96s/it, loss=3.6673, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:13<25:45,  6.23s/it, loss=3.8557, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:15<17:35,  4.27s/it, loss=3.8359, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:22<21:49,  5.32s/it, loss=3.7944, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<17:51,  4.37s/it, loss=3.7701, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:30<19:08,  4.71s/it, loss=3.7726, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:33<17:01,  4.20s/it, loss=3.7537, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:36<16:06,  3.99s/it, loss=3.7477, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:43<18:42,  4.66s/it, loss=3.7421, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:45<16:13,  4.06s/it, loss=3.7232, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:49<15:48,  3.97s/it, loss=3.7202, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:53<16:19,  4.11s/it, loss=3.7297, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:57<16:03,  4.07s/it, loss=3.7371, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [01:01<15:03,  3.83s/it, loss=3.7412, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [01:03<13:18,  3.40s/it, loss=3.7256, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [01:05<12:04,  3.10s/it, loss=3.7164, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [01:09<12:22,  3.19s/it, loss=3.7226, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:12<11:54,  3.08s/it, loss=3.7197, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:17<14:09,  3.68s/it, loss=3.7161, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:24<18:42,  4.88s/it, loss=3.7157, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:27<16:30,  4.32s/it, loss=3.7104, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:32<16:04,  4.23s/it, loss=3.7248, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:35<15:23,  4.07s/it, loss=3.7160, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:40<15:49,  4.20s/it, loss=3.7156, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:48<20:26,  5.45s/it, loss=3.7192, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:56<23:31,  6.30s/it, loss=3.7246, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:59<19:19,  5.20s/it, loss=3.7261, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [02:06<20:56,  5.66s/it, loss=3.7376, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [02:10<18:52,  5.13s/it, loss=3.7393, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [02:12<15:35,  4.25s/it, loss=3.7407, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [02:15<14:02,  3.85s/it, loss=3.7472, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [02:21<17:06,  4.71s/it, loss=3.7470, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [02:24<15:08,  4.19s/it, loss=3.7494, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:27<13:14,  3.68s/it, loss=3.7518, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:30<12:29,  3.49s/it, loss=3.7510, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:33<12:10,  3.42s/it, loss=3.7529, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:36<11:30,  3.24s/it, loss=3.7542, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:38<10:18,  2.92s/it, loss=3.7512, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:42<10:42,  3.04s/it, loss=3.7489, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:44<10:18,  2.94s/it, loss=3.7464, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:47<09:52,  2.83s/it, loss=3.7436, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:50<09:52,  2.85s/it, loss=3.7438, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:53<10:36,  3.07s/it, loss=3.7454, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:56<10:30,  3.06s/it, loss=3.7423, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:59<10:21,  3.03s/it, loss=3.7446, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [03:02<09:35,  2.82s/it, loss=3.7473, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [03:04<09:28,  2.80s/it, loss=3.7485, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [03:10<11:49,  3.51s/it, loss=3.7452, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [03:12<10:59,  3.28s/it, loss=3.7450, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [03:15<10:30,  3.15s/it, loss=3.7500, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [03:22<14:06,  4.25s/it, loss=3.7560, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:24<12:15,  3.72s/it, loss=3.7567, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:27<11:15,  3.43s/it, loss=3.7545, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:32<12:33,  3.84s/it, loss=3.7579, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:35<11:28,  3.53s/it, loss=3.7572, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:38<10:53,  3.37s/it, loss=3.7522, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:40<09:39,  3.00s/it, loss=3.7474, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:45<11:36,  3.63s/it, loss=3.7493, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:47<10:01,  3.15s/it, loss=3.7489, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:51<10:52,  3.43s/it, loss=3.7467, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:54<10:23,  3.30s/it, loss=3.7453, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:57<09:36,  3.07s/it, loss=3.7410, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:59<08:47,  2.82s/it, loss=3.7365, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [04:04<11:07,  3.59s/it, loss=3.7379, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [04:07<10:09,  3.29s/it, loss=3.7326, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [04:12<11:55,  3.89s/it, loss=3.7309, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:14<10:10,  3.34s/it, loss=3.7301, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:17<09:13,  3.04s/it, loss=3.7276, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:19<08:44,  2.90s/it, loss=3.7278, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:21<08:08,  2.72s/it, loss=3.7298, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:25<08:25,  2.83s/it, loss=3.7288, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:27<07:52,  2.66s/it, loss=3.7276, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:29<07:11,  2.44s/it, loss=3.7256, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:31<07:16,  2.48s/it, loss=3.7273, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:33<06:47,  2.33s/it, loss=3.7244, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:36<07:06,  2.45s/it, loss=3.7243, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:39<07:08,  2.48s/it, loss=3.7248, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:42<07:53,  2.76s/it, loss=3.7262, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:44<07:38,  2.68s/it, loss=3.7273, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:47<07:33,  2.67s/it, loss=3.7289, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:50<07:40,  2.73s/it, loss=3.7269, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:53<07:49,  2.80s/it, loss=3.7250, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:56<07:59,  2.87s/it, loss=3.7253, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:59<08:28,  3.06s/it, loss=3.7234, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [05:02<08:06,  2.95s/it, loss=3.7224, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [05:05<07:37,  2.79s/it, loss=3.7181, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [05:06<06:45,  2.49s/it, loss=3.7164, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [05:09<06:36,  2.45s/it, loss=3.7146, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:15<10:02,  3.74s/it, loss=3.7176, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:17<08:27,  3.17s/it, loss=3.7170, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:20<08:02,  3.04s/it, loss=3.7168, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:23<07:38,  2.90s/it, loss=3.7154, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:25<07:34,  2.90s/it, loss=3.7164, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:27<06:28,  2.49s/it, loss=3.7119, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:29<06:19,  2.45s/it, loss=3.7145, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:32<06:34,  2.56s/it, loss=3.7112, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:34<06:08,  2.41s/it, loss=3.7105, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:39<07:56,  3.14s/it, loss=3.7136, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:41<07:17,  2.90s/it, loss=3.7112, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:48<10:00,  4.00s/it, loss=3.7138, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:50<08:24,  3.39s/it, loss=3.7114, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:52<07:30,  3.05s/it, loss=3.7096, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:55<07:04,  2.89s/it, loss=3.7078, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:58<07:29,  3.08s/it, loss=3.7091, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [06:00<06:46,  2.81s/it, loss=3.7085, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [06:05<07:44,  3.22s/it, loss=3.7077, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [06:07<07:24,  3.11s/it, loss=3.7075, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:10<06:52,  2.91s/it, loss=3.7079, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:12<06:23,  2.72s/it, loss=3.7087, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:17<07:33,  3.24s/it, loss=3.7054, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:19<07:08,  3.08s/it, loss=3.7067, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:23<07:10,  3.12s/it, loss=3.7057, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:30<09:48,  4.29s/it, loss=3.7066, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:32<08:21,  3.69s/it, loss=3.7060, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:34<07:12,  3.21s/it, loss=3.7058, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:37<07:05,  3.18s/it, loss=3.7052, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:43<08:40,  3.91s/it, loss=3.7057, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:49<10:16,  4.67s/it, loss=3.7061, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:53<09:47,  4.49s/it, loss=3.7078, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:55<07:56,  3.66s/it, loss=3.7073, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:57<07:04,  3.29s/it, loss=3.7085, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [07:00<06:41,  3.13s/it, loss=3.7104, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [07:03<06:38,  3.14s/it, loss=3.7100, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:05<05:46,  2.75s/it, loss=3.7077, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:08<05:49,  2.79s/it, loss=3.7070, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:11<06:12,  3.00s/it, loss=3.7070, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:13<05:22,  2.62s/it, loss=3.7051, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:16<05:27,  2.68s/it, loss=3.7039, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:18<05:00,  2.48s/it, loss=3.7036, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:20<04:51,  2.43s/it, loss=3.7033, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:23<04:53,  2.47s/it, loss=3.7048, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:25<04:30,  2.29s/it, loss=3.7025, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:27<04:08,  2.13s/it, loss=3.7012, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:33<06:35,  3.41s/it, loss=3.7007, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:36<06:22,  3.32s/it, loss=3.6998, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:38<05:47,  3.05s/it, loss=3.6993, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:40<05:09,  2.73s/it, loss=3.6985, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:43<04:57,  2.66s/it, loss=3.6967, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:45<04:36,  2.49s/it, loss=3.6961, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:47<04:20,  2.36s/it, loss=3.6964, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:50<04:31,  2.49s/it, loss=3.6947, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:53<04:37,  2.57s/it, loss=3.6947, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [07:56<04:44,  2.66s/it, loss=3.6948, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [07:58<04:36,  2.61s/it, loss=3.6936, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:04<06:10,  3.53s/it, loss=3.6939, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:06<05:31,  3.19s/it, loss=3.6936, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:08<04:49,  2.81s/it, loss=3.6917, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:10<04:23,  2.59s/it, loss=3.6924, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:14<04:50,  2.88s/it, loss=3.6922, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:18<05:44,  3.45s/it, loss=3.6922, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:21<05:17,  3.21s/it, loss=3.6927, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:25<05:46,  3.54s/it, loss=3.6926, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:28<05:03,  3.13s/it, loss=3.6918, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:31<05:08,  3.21s/it, loss=3.6918, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:33<04:26,  2.81s/it, loss=3.6908, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:36<04:41,  3.00s/it, loss=3.6916, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:39<04:32,  2.93s/it, loss=3.6909, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:42<04:25,  2.88s/it, loss=3.6931, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:45<04:20,  2.86s/it, loss=3.6936, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:47<04:13,  2.82s/it, loss=3.6949, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [08:53<05:15,  3.55s/it, loss=3.6953, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [08:56<05:00,  3.42s/it, loss=3.6965, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [08:58<04:16,  2.95s/it, loss=3.6952, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [09:01<04:19,  3.01s/it, loss=3.6948, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:04<04:28,  3.15s/it, loss=3.6954, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:07<04:25,  3.16s/it, loss=3.6954, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:10<04:07,  2.98s/it, loss=3.6951, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:13<04:06,  3.00s/it, loss=3.6959, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:15<03:31,  2.62s/it, loss=3.6948, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:17<03:32,  2.66s/it, loss=3.6950, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:24<05:01,  3.81s/it, loss=3.6935, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [09:27<04:29,  3.46s/it, loss=3.6930, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:29<04:09,  3.24s/it, loss=3.6939, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:31<03:38,  2.88s/it, loss=3.6943, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:33<03:12,  2.57s/it, loss=3.6954, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:36<03:14,  2.63s/it, loss=3.6954, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:38<03:05,  2.54s/it, loss=3.6965, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [09:40<02:52,  2.39s/it, loss=3.6959, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [09:43<02:50,  2.41s/it, loss=3.6969, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [09:46<03:02,  2.60s/it, loss=3.6974, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [09:48<02:46,  2.41s/it, loss=3.6962, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [09:54<04:04,  3.59s/it, loss=3.6951, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [09:57<03:36,  3.23s/it, loss=3.6952, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [09:59<03:13,  2.93s/it, loss=3.6946, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:01<02:54,  2.68s/it, loss=3.6954, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:04<02:53,  2.71s/it, loss=3.6954, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:06<02:48,  2.67s/it, loss=3.6941, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:09<02:46,  2.68s/it, loss=3.6935, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:12<02:44,  2.69s/it, loss=3.6930, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:18<03:50,  3.84s/it, loss=3.6923, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [10:21<03:28,  3.54s/it, loss=3.6924, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [10:24<03:07,  3.23s/it, loss=3.6914, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [10:26<02:51,  3.00s/it, loss=3.6912, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:30<03:02,  3.25s/it, loss=3.6910, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:32<02:42,  2.95s/it, loss=3.6905, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:35<02:33,  2.84s/it, loss=3.6899, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:37<02:23,  2.70s/it, loss=3.6888, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [10:40<02:27,  2.84s/it, loss=3.6874, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [10:43<02:29,  2.93s/it, loss=3.6871, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [10:47<02:43,  3.27s/it, loss=3.6860, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [10:53<03:07,  3.83s/it, loss=3.6865, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:00<03:53,  4.87s/it, loss=3.6873, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:03<03:17,  4.21s/it, loss=3.6863, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:05<02:49,  3.68s/it, loss=3.6857, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:08<02:36,  3.47s/it, loss=3.6865, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:11<02:25,  3.32s/it, loss=3.6868, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [11:14<02:18,  3.22s/it, loss=3.6876, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [11:16<02:03,  2.95s/it, loss=3.6876, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [11:19<02:04,  3.03s/it, loss=3.6877, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [11:23<02:03,  3.09s/it, loss=3.6871, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:26<02:08,  3.29s/it, loss=3.6857, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:30<02:03,  3.24s/it, loss=3.6846, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:32<01:46,  2.89s/it, loss=3.6843, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:34<01:39,  2.75s/it, loss=3.6841, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:36<01:25,  2.45s/it, loss=3.6832, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:39<01:27,  2.59s/it, loss=3.6833, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [11:42<01:30,  2.76s/it, loss=3.6828, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [11:45<01:28,  2.75s/it, loss=3.6823, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [11:47<01:22,  2.67s/it, loss=3.6820, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [11:50<01:18,  2.61s/it, loss=3.6831, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [11:53<01:18,  2.72s/it, loss=3.6826, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [11:55<01:13,  2.63s/it, loss=3.6812, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [11:58<01:11,  2.65s/it, loss=3.6805, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:03<01:31,  3.50s/it, loss=3.6814, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:05<01:17,  3.10s/it, loss=3.6806, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [12:07<01:06,  2.79s/it, loss=3.6782, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [12:09<00:55,  2.43s/it, loss=3.6768, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [12:11<00:49,  2.25s/it, loss=3.6758, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [12:14<00:54,  2.59s/it, loss=3.6758, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [12:16<00:47,  2.35s/it, loss=3.6745, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [12:19<00:49,  2.58s/it, loss=3.6743, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [12:26<01:07,  3.77s/it, loss=3.6743, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [12:28<00:54,  3.21s/it, loss=3.6735, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [12:33<01:02,  3.88s/it, loss=3.6729, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [12:35<00:51,  3.45s/it, loss=3.6733, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [12:42<00:59,  4.27s/it, loss=3.6746, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [12:44<00:48,  3.73s/it, loss=3.6743, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [12:47<00:40,  3.33s/it, loss=3.6740, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [12:52<00:42,  3.89s/it, loss=3.6744, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [12:54<00:34,  3.42s/it, loss=3.6741, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [12:56<00:27,  3.05s/it, loss=3.6740, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [12:59<00:24,  3.05s/it, loss=3.6748, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:02<00:19,  2.84s/it, loss=3.6742, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [13:04<00:16,  2.80s/it, loss=3.6741, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [13:06<00:12,  2.58s/it, loss=3.6736, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [13:10<00:11,  2.93s/it, loss=3.6727, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [13:14<00:09,  3.26s/it, loss=3.6741, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [13:17<00:05,  3.00s/it, loss=3.6743, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [13:19<00:02,  2.78s/it, loss=3.6750, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [13:35<00:00,  3.26s/it, loss=3.6760, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<04:05,  7.92s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:09<02:02,  4.08s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:10<01:21,  2.81s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:11<00:57,  2.04s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:12<00:46,  1.71s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:13<00:38,  1.50s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:14<00:32,  1.30s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:15<00:28,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:16<00:28,  1.22s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:17<00:23,  1.08s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:18<00:22,  1.07s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:20<00:23,  1.17s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:21<00:22,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:22<00:19,  1.07s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:22<00:17,  1.02s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:23<00:15,  1.02it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:24<00:15,  1.02s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:25<00:13,  1.03it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:26<00:12,  1.03it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:27<00:11,  1.07it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:28<00:09,  1.17it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:29<00:09,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:30<00:08,  1.00it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:32<00:10,  1.30s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:33<00:08,  1.20s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:34<00:07,  1.24s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:35<00:05,  1.14s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:48<00:00,  1.53s/it]\n",
      "Removed old checkpoint: model_epoch_7.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_7.pt\n",
      "Saved training results to training_results_20250511_180734.csv\n",
      "Train Loss: 3.6760\n",
      "Val Loss: 4.6163\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: Parliament?\n",
      "\n",
      "Epoch 8/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:10<43:25, 10.46s/it, loss=3.5088, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:15<29:29,  7.13s/it, loss=3.6349, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:17<19:56,  4.84s/it, loss=3.6300, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:23<21:12,  5.17s/it, loss=3.5831, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:25<17:41,  4.33s/it, loss=3.5687, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:30<17:26,  4.29s/it, loss=3.5786, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:32<14:53,  3.68s/it, loss=3.5626, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:35<14:11,  3.52s/it, loss=3.5553, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:42<17:44,  4.42s/it, loss=3.5458, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:44<14:38,  3.66s/it, loss=3.5258, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:47<14:16,  3.58s/it, loss=3.5204, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:50<13:54,  3.51s/it, loss=3.5323, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:53<12:39,  3.20s/it, loss=3.5389, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:55<11:55,  3.03s/it, loss=3.5478, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:57<10:19,  2.64s/it, loss=3.5311, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:59<09:13,  2.36s/it, loss=3.5222, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [01:01<09:18,  2.40s/it, loss=3.5290, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:03<08:47,  2.28s/it, loss=3.5247, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:08<11:30,  2.99s/it, loss=3.5226, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:14<15:25,  4.02s/it, loss=3.5231, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:17<13:16,  3.48s/it, loss=3.5186, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:19<12:22,  3.26s/it, loss=3.5323, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:22<11:19,  3.00s/it, loss=3.5237, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:25<11:53,  3.16s/it, loss=3.5253, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:31<14:25,  3.85s/it, loss=3.5305, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:38<18:20,  4.91s/it, loss=3.5363, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:41<15:47,  4.25s/it, loss=3.5372, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:48<19:02,  5.15s/it, loss=3.5503, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:51<16:40,  4.53s/it, loss=3.5514, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:53<14:05,  3.84s/it, loss=3.5537, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:56<12:51,  3.52s/it, loss=3.5612, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [02:03<16:19,  4.49s/it, loss=3.5599, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [02:07<15:21,  4.25s/it, loss=3.5629, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:09<13:25,  3.73s/it, loss=3.5642, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:13<13:03,  3.64s/it, loss=3.5630, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:15<11:40,  3.28s/it, loss=3.5652, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:18<11:33,  3.25s/it, loss=3.5660, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:21<11:06,  3.14s/it, loss=3.5631, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:25<11:53,  3.38s/it, loss=3.5619, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:28<11:00,  3.15s/it, loss=3.5592, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:30<10:17,  2.95s/it, loss=3.5568, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:34<11:10,  3.22s/it, loss=3.5564, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:39<12:44,  3.69s/it, loss=3.5585, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:41<11:09,  3.25s/it, loss=3.5549, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:44<10:51,  3.18s/it, loss=3.5577, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:46<09:52,  2.90s/it, loss=3.5588, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:50<10:25,  3.08s/it, loss=3.5604, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [02:56<13:26,  3.99s/it, loss=3.5568, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [02:59<12:06,  3.61s/it, loss=3.5574, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [03:01<11:16,  3.38s/it, loss=3.5621, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [03:07<13:10,  3.97s/it, loss=3.5688, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:10<12:29,  3.79s/it, loss=3.5697, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:13<11:55,  3.63s/it, loss=3.5665, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:19<13:59,  4.28s/it, loss=3.5701, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:22<12:27,  3.83s/it, loss=3.5691, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:25<11:26,  3.54s/it, loss=3.5641, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:27<10:01,  3.11s/it, loss=3.5597, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:32<12:13,  3.82s/it, loss=3.5620, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:35<10:32,  3.31s/it, loss=3.5625, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:38<10:49,  3.42s/it, loss=3.5608, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:41<10:22,  3.29s/it, loss=3.5595, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:44<09:41,  3.10s/it, loss=3.5550, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:47<09:42,  3.11s/it, loss=3.5507, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:53<12:26,  4.01s/it, loss=3.5527, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:56<11:07,  3.61s/it, loss=3.5481, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [03:59<10:41,  3.49s/it, loss=3.5459, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:01<09:21,  3.07s/it, loss=3.5448, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:04<09:27,  3.12s/it, loss=3.5425, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:07<09:10,  3.04s/it, loss=3.5428, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:10<08:27,  2.82s/it, loss=3.5442, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:14<09:43,  3.26s/it, loss=3.5445, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:16<08:44,  2.95s/it, loss=3.5446, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:19<08:24,  2.85s/it, loss=3.5424, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:21<07:29,  2.56s/it, loss=3.5453, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:22<06:53,  2.36s/it, loss=3.5426, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:26<08:01,  2.77s/it, loss=3.5434, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:29<07:46,  2.70s/it, loss=3.5440, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:35<10:31,  3.67s/it, loss=3.5455, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:37<09:40,  3.40s/it, loss=3.5474, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:40<09:03,  3.20s/it, loss=3.5487, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:42<08:06,  2.88s/it, loss=3.5463, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:45<08:07,  2.90s/it, loss=3.5442, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:48<08:09,  2.93s/it, loss=3.5447, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:54<10:32,  3.81s/it, loss=3.5430, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [04:57<09:33,  3.47s/it, loss=3.5418, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [04:59<08:40,  3.17s/it, loss=3.5373, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [05:01<07:30,  2.76s/it, loss=3.5357, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [05:03<07:06,  2.63s/it, loss=3.5337, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:11<11:28,  4.27s/it, loss=3.5369, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:13<09:34,  3.59s/it, loss=3.5362, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:16<08:56,  3.38s/it, loss=3.5359, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:19<08:20,  3.17s/it, loss=3.5343, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:22<08:10,  3.12s/it, loss=3.5348, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:25<07:37,  2.93s/it, loss=3.5306, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:27<07:13,  2.79s/it, loss=3.5333, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:29<06:31,  2.54s/it, loss=3.5295, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:31<06:07,  2.41s/it, loss=3.5287, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:36<08:08,  3.21s/it, loss=3.5317, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:39<07:28,  2.97s/it, loss=3.5288, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:46<11:05,  4.44s/it, loss=3.5315, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:48<09:07,  3.67s/it, loss=3.5294, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:51<08:02,  3.26s/it, loss=3.5263, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:53<07:28,  3.05s/it, loss=3.5240, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:56<07:22,  3.03s/it, loss=3.5249, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [05:59<07:00,  2.90s/it, loss=3.5242, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [06:03<07:40,  3.20s/it, loss=3.5229, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [06:05<07:18,  3.07s/it, loss=3.5226, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:08<06:55,  2.93s/it, loss=3.5229, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:10<06:26,  2.74s/it, loss=3.5235, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:16<08:23,  3.60s/it, loss=3.5199, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:19<07:42,  3.33s/it, loss=3.5213, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:21<07:03,  3.07s/it, loss=3.5202, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:28<09:35,  4.20s/it, loss=3.5214, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:30<08:15,  3.64s/it, loss=3.5207, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:33<07:46,  3.45s/it, loss=3.5206, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:38<08:25,  3.78s/it, loss=3.5194, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:44<09:54,  4.47s/it, loss=3.5202, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:50<11:14,  5.11s/it, loss=3.5210, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:54<09:55,  4.54s/it, loss=3.5226, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:56<08:39,  4.00s/it, loss=3.5219, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:59<07:50,  3.65s/it, loss=3.5231, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [07:03<07:46,  3.64s/it, loss=3.5251, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [07:06<07:37,  3.61s/it, loss=3.5250, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:08<06:31,  3.11s/it, loss=3.5225, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:13<07:30,  3.61s/it, loss=3.5219, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:17<07:55,  3.83s/it, loss=3.5214, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:19<06:35,  3.22s/it, loss=3.5198, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:23<06:42,  3.30s/it, loss=3.5189, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:25<05:52,  2.91s/it, loss=3.5194, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:27<05:27,  2.73s/it, loss=3.5192, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:31<06:13,  3.14s/it, loss=3.5208, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:33<05:27,  2.78s/it, loss=3.5185, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:35<04:50,  2.48s/it, loss=3.5172, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:41<06:47,  3.52s/it, loss=3.5167, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:44<06:44,  3.52s/it, loss=3.5152, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:48<06:37,  3.49s/it, loss=3.5144, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:50<05:45,  3.06s/it, loss=3.5133, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:53<05:49,  3.12s/it, loss=3.5115, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:55<05:12,  2.82s/it, loss=3.5109, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:58<05:09,  2.82s/it, loss=3.5108, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [08:03<06:08,  3.38s/it, loss=3.5092, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [08:05<05:19,  2.96s/it, loss=3.5094, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [08:09<06:14,  3.50s/it, loss=3.5093, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [08:12<05:44,  3.25s/it, loss=3.5077, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:17<06:32,  3.74s/it, loss=3.5076, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:20<06:20,  3.66s/it, loss=3.5069, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:22<05:27,  3.17s/it, loss=3.5054, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:25<05:00,  2.94s/it, loss=3.5062, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:28<05:11,  3.08s/it, loss=3.5059, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:34<06:12,  3.73s/it, loss=3.5058, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:37<06:15,  3.80s/it, loss=3.5060, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:45<07:57,  4.87s/it, loss=3.5056, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:47<06:40,  4.13s/it, loss=3.5047, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:53<07:10,  4.48s/it, loss=3.5046, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:54<05:52,  3.71s/it, loss=3.5033, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:58<05:36,  3.58s/it, loss=3.5040, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [09:01<05:14,  3.38s/it, loss=3.5034, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [09:04<05:00,  3.27s/it, loss=3.5052, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [09:09<05:45,  3.80s/it, loss=3.5054, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [09:13<05:51,  3.91s/it, loss=3.5065, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [09:19<06:38,  4.48s/it, loss=3.5073, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [09:22<06:05,  4.15s/it, loss=3.5089, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [09:24<05:00,  3.45s/it, loss=3.5074, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [09:26<04:26,  3.10s/it, loss=3.5069, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:30<04:43,  3.33s/it, loss=3.5076, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:36<05:51,  4.18s/it, loss=3.5077, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:39<05:22,  3.89s/it, loss=3.5075, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:44<05:27,  3.99s/it, loss=3.5086, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:45<04:30,  3.34s/it, loss=3.5075, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:50<04:44,  3.56s/it, loss=3.5077, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:58<06:27,  4.90s/it, loss=3.5064, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [10:00<05:32,  4.26s/it, loss=3.5058, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [10:04<05:08,  4.01s/it, loss=3.5068, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [10:06<04:23,  3.47s/it, loss=3.5074, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [10:08<03:44,  3.00s/it, loss=3.5087, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [10:12<04:09,  3.38s/it, loss=3.5090, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [10:14<03:42,  3.04s/it, loss=3.5101, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [10:17<03:19,  2.77s/it, loss=3.5099, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [10:20<03:27,  2.93s/it, loss=3.5112, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [10:22<03:14,  2.78s/it, loss=3.5118, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [10:24<02:54,  2.53s/it, loss=3.5102, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [10:31<04:18,  3.81s/it, loss=3.5090, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [10:33<03:46,  3.39s/it, loss=3.5092, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [10:36<03:20,  3.05s/it, loss=3.5088, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:39<03:17,  3.04s/it, loss=3.5091, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:43<03:47,  3.55s/it, loss=3.5086, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:45<03:08,  2.99s/it, loss=3.5075, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:48<03:00,  2.90s/it, loss=3.5071, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:51<02:54,  2.86s/it, loss=3.5066, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:58<04:20,  4.33s/it, loss=3.5059, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [11:01<03:51,  3.93s/it, loss=3.5056, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [11:04<03:24,  3.53s/it, loss=3.5044, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [11:06<03:03,  3.22s/it, loss=3.5040, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [11:10<02:58,  3.19s/it, loss=3.5040, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [11:12<02:40,  2.92s/it, loss=3.5034, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [11:15<02:46,  3.08s/it, loss=3.5026, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [11:18<02:33,  2.89s/it, loss=3.5015, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [11:22<02:53,  3.33s/it, loss=3.5002, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [11:26<02:54,  3.42s/it, loss=3.4995, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [11:30<03:06,  3.73s/it, loss=3.4985, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [11:37<03:42,  4.55s/it, loss=3.4989, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:44<04:25,  5.53s/it, loss=3.4995, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:47<03:42,  4.73s/it, loss=3.4990, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:50<03:07,  4.07s/it, loss=3.4989, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:54<02:57,  3.96s/it, loss=3.4999, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:56<02:41,  3.66s/it, loss=3.5000, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [12:00<02:31,  3.52s/it, loss=3.5007, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [12:03<02:23,  3.42s/it, loss=3.5009, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [12:05<02:08,  3.14s/it, loss=3.5009, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [12:10<02:22,  3.57s/it, loss=3.5002, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [12:14<02:28,  3.81s/it, loss=3.4989, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [12:18<02:22,  3.75s/it, loss=3.4978, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [12:20<02:00,  3.26s/it, loss=3.4974, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [12:24<02:03,  3.43s/it, loss=3.4972, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [12:26<01:42,  2.92s/it, loss=3.4960, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [12:28<01:34,  2.79s/it, loss=3.4957, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [12:33<01:48,  3.30s/it, loss=3.4955, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [12:36<01:42,  3.20s/it, loss=3.4951, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [12:38<01:32,  3.00s/it, loss=3.4947, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [12:42<01:41,  3.40s/it, loss=3.4958, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [12:47<01:47,  3.71s/it, loss=3.4953, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [12:49<01:33,  3.33s/it, loss=3.4940, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [12:52<01:26,  3.20s/it, loss=3.4937, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:56<01:26,  3.33s/it, loss=3.4948, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:58<01:14,  2.98s/it, loss=3.4941, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [13:01<01:11,  3.00s/it, loss=3.4919, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [13:03<00:59,  2.60s/it, loss=3.4904, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [13:05<00:52,  2.38s/it, loss=3.4893, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [13:08<00:59,  2.82s/it, loss=3.4891, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [13:10<00:50,  2.52s/it, loss=3.4881, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [13:15<00:58,  3.07s/it, loss=3.4879, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [13:22<01:17,  4.31s/it, loss=3.4874, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [13:24<01:01,  3.61s/it, loss=3.4862, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [13:29<01:07,  4.23s/it, loss=3.4856, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [13:32<00:55,  3.67s/it, loss=3.4860, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [13:38<01:02,  4.46s/it, loss=3.4874, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [13:41<00:50,  3.88s/it, loss=3.4869, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [13:43<00:41,  3.45s/it, loss=3.4862, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [13:49<00:45,  4.11s/it, loss=3.4868, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [13:51<00:36,  3.64s/it, loss=3.4866, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [13:54<00:29,  3.24s/it, loss=3.4865, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [13:56<00:24,  3.11s/it, loss=3.4875, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:59<00:20,  2.88s/it, loss=3.4870, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [14:01<00:16,  2.82s/it, loss=3.4865, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [14:04<00:14,  2.85s/it, loss=3.4860, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [14:08<00:12,  3.07s/it, loss=3.4856, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [14:12<00:10,  3.44s/it, loss=3.4867, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [14:15<00:06,  3.11s/it, loss=3.4867, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [14:17<00:02,  2.87s/it, loss=3.4874, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [14:34<00:00,  3.50s/it, loss=3.4885, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:40,  7.10s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:52,  3.76s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:09<01:17,  2.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:10<00:55,  1.97s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:12<00:52,  1.95s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:13<00:43,  1.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:14<00:35,  1.44s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:15<00:30,  1.27s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:17<00:29,  1.28s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:17<00:25,  1.14s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:18<00:23,  1.13s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:19<00:19,  1.00it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:21<00:23,  1.26s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:22<00:19,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:23<00:17,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:23<00:15,  1.05it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:24<00:14,  1.01it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:25<00:13,  1.06it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:26<00:12,  1.07it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:27<00:10,  1.10it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:28<00:09,  1.20it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:29<00:08,  1.16it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:31<00:10,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:32<00:09,  1.19s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:33<00:07,  1.12s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:34<00:07,  1.19s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:35<00:05,  1.11s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:49<00:00,  1.54s/it]\n",
      "Saved checkpoint to checkpoints/model_epoch_8.pt\n",
      "Saved training results to training_results_20250511_182258.csv\n",
      "Train Loss: 3.4885\n",
      "Val Loss: 4.6554\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: ?\n",
      "\n",
      "Epoch 9/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:10<42:37, 10.27s/it, loss=3.3027, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:13<25:34,  6.19s/it, loss=3.4483, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:16<19:58,  4.85s/it, loss=3.4299, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:22<21:17,  5.19s/it, loss=3.3974, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<16:23,  4.01s/it, loss=3.4004, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:27<14:58,  3.68s/it, loss=3.3980, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:29<12:47,  3.16s/it, loss=3.3783, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:31<11:43,  2.91s/it, loss=3.3691, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:36<14:12,  3.54s/it, loss=3.3552, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:40<13:40,  3.42s/it, loss=3.3373, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:44<14:23,  3.61s/it, loss=3.3369, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:47<14:13,  3.58s/it, loss=3.3496, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:50<12:48,  3.24s/it, loss=3.3584, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:51<11:02,  2.81s/it, loss=3.3632, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:54<10:41,  2.73s/it, loss=3.3405, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [00:56<09:27,  2.43s/it, loss=3.3312, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [00:58<09:31,  2.45s/it, loss=3.3382, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:00<09:06,  2.35s/it, loss=3.3382, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:05<11:31,  2.99s/it, loss=3.3385, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:11<14:59,  3.91s/it, loss=3.3364, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:14<14:06,  3.70s/it, loss=3.3324, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:18<13:59,  3.68s/it, loss=3.3474, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:20<12:14,  3.23s/it, loss=3.3424, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:23<11:43,  3.11s/it, loss=3.3417, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:27<13:29,  3.60s/it, loss=3.3483, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:34<17:01,  4.56s/it, loss=3.3561, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:38<15:40,  4.22s/it, loss=3.3555, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:44<18:09,  4.91s/it, loss=3.3703, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:47<15:58,  4.34s/it, loss=3.3696, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:49<13:37,  3.72s/it, loss=3.3717, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [01:52<12:35,  3.45s/it, loss=3.3774, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [01:59<16:34,  4.56s/it, loss=3.3747, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [02:03<15:47,  4.37s/it, loss=3.3762, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:06<13:41,  3.80s/it, loss=3.3782, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:10<13:49,  3.86s/it, loss=3.3786, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:12<12:13,  3.43s/it, loss=3.3799, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:15<11:52,  3.34s/it, loss=3.3812, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:18<10:39,  3.02s/it, loss=3.3803, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:21<10:55,  3.11s/it, loss=3.3804, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:25<11:21,  3.25s/it, loss=3.3766, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:27<10:43,  3.08s/it, loss=3.3726, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:32<12:28,  3.60s/it, loss=3.3727, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:38<15:01,  4.36s/it, loss=3.3756, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:41<13:52,  4.04s/it, loss=3.3713, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:47<15:37,  4.57s/it, loss=3.3750, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:50<13:51,  4.07s/it, loss=3.3770, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:54<13:36,  4.02s/it, loss=3.3795, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [03:00<15:04,  4.48s/it, loss=3.3757, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [03:02<13:14,  3.95s/it, loss=3.3780, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [03:05<12:01,  3.61s/it, loss=3.3831, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [03:11<13:50,  4.17s/it, loss=3.3886, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:13<12:01,  3.65s/it, loss=3.3892, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:16<11:05,  3.38s/it, loss=3.3866, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:21<12:58,  3.97s/it, loss=3.3903, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:24<12:11,  3.75s/it, loss=3.3883, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:28<11:48,  3.65s/it, loss=3.3836, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:30<10:17,  3.20s/it, loss=3.3787, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:35<11:31,  3.60s/it, loss=3.3814, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:37<09:56,  3.12s/it, loss=3.3814, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:40<09:50,  3.11s/it, loss=3.3792, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:43<09:37,  3.06s/it, loss=3.3778, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:45<09:04,  2.90s/it, loss=3.3725, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:47<08:24,  2.70s/it, loss=3.3687, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [03:52<09:54,  3.19s/it, loss=3.3698, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [03:54<09:14,  3.00s/it, loss=3.3647, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [03:58<09:52,  3.22s/it, loss=3.3621, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:00<08:45,  2.87s/it, loss=3.3612, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:02<08:15,  2.72s/it, loss=3.3594, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:05<08:11,  2.72s/it, loss=3.3605, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:07<07:46,  2.59s/it, loss=3.3629, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:10<08:10,  2.74s/it, loss=3.3629, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:13<07:42,  2.60s/it, loss=3.3635, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:15<07:05,  2.40s/it, loss=3.3616, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:16<06:33,  2.23s/it, loss=3.3637, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:19<06:25,  2.20s/it, loss=3.3604, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:22<07:12,  2.48s/it, loss=3.3609, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:24<07:16,  2.52s/it, loss=3.3621, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:29<08:54,  3.11s/it, loss=3.3637, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:31<08:18,  2.92s/it, loss=3.3658, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:34<08:01,  2.84s/it, loss=3.3670, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:36<07:22,  2.62s/it, loss=3.3648, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [04:39<07:43,  2.76s/it, loss=3.3626, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [04:42<07:50,  2.82s/it, loss=3.3627, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [04:45<08:04,  2.92s/it, loss=3.3607, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [04:48<08:12,  2.98s/it, loss=3.3591, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [04:51<07:43,  2.83s/it, loss=3.3546, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [04:53<06:49,  2.51s/it, loss=3.3527, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [04:55<06:51,  2.54s/it, loss=3.3510, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:02<10:07,  3.77s/it, loss=3.3548, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:04<08:31,  3.20s/it, loss=3.3539, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:07<08:14,  3.11s/it, loss=3.3540, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:09<07:55,  3.01s/it, loss=3.3526, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:13<08:11,  3.13s/it, loss=3.3532, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:14<06:55,  2.66s/it, loss=3.3483, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:17<06:56,  2.69s/it, loss=3.3509, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:20<07:08,  2.79s/it, loss=3.3467, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:23<07:08,  2.80s/it, loss=3.3462, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:28<08:26,  3.34s/it, loss=3.3495, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:30<07:37,  3.03s/it, loss=3.3468, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [05:37<10:28,  4.19s/it, loss=3.3495, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [05:39<08:43,  3.51s/it, loss=3.3475, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [05:41<07:44,  3.14s/it, loss=3.3442, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [05:45<08:08,  3.32s/it, loss=3.3415, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [05:48<08:00,  3.29s/it, loss=3.3422, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [05:50<07:13,  2.99s/it, loss=3.3413, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [05:55<08:10,  3.41s/it, loss=3.3394, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [05:58<07:45,  3.26s/it, loss=3.3393, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:00<07:08,  3.02s/it, loss=3.3401, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:03<07:14,  3.08s/it, loss=3.3408, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:09<09:02,  3.88s/it, loss=3.3380, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:12<08:25,  3.63s/it, loss=3.3394, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:15<07:34,  3.30s/it, loss=3.3380, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:22<10:14,  4.48s/it, loss=3.3398, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:24<08:41,  3.83s/it, loss=3.3396, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:27<08:04,  3.59s/it, loss=3.3400, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:31<08:22,  3.75s/it, loss=3.3395, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [06:38<09:58,  4.50s/it, loss=3.3401, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [06:44<11:14,  5.11s/it, loss=3.3411, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [06:47<09:47,  4.49s/it, loss=3.3431, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [06:49<07:56,  3.66s/it, loss=3.3420, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [06:52<07:44,  3.60s/it, loss=3.3433, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [06:55<07:05,  3.32s/it, loss=3.3455, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [06:58<06:48,  3.21s/it, loss=3.3449, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:00<05:55,  2.82s/it, loss=3.3422, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:03<05:53,  2.82s/it, loss=3.3416, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:05<05:47,  2.81s/it, loss=3.3418, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:08<05:34,  2.72s/it, loss=3.3392, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:11<05:39,  2.78s/it, loss=3.3383, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:13<05:11,  2.58s/it, loss=3.3391, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:15<04:58,  2.49s/it, loss=3.3390, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:18<05:01,  2.53s/it, loss=3.3404, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:20<04:36,  2.34s/it, loss=3.3382, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:22<04:43,  2.43s/it, loss=3.3369, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:28<06:27,  3.34s/it, loss=3.3359, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:31<06:24,  3.34s/it, loss=3.3344, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:34<05:51,  3.09s/it, loss=3.3336, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [07:36<05:14,  2.78s/it, loss=3.3325, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [07:39<05:31,  2.96s/it, loss=3.3302, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [07:41<04:57,  2.68s/it, loss=3.3294, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [07:43<04:37,  2.52s/it, loss=3.3295, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [07:46<04:46,  2.63s/it, loss=3.3278, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [07:48<04:21,  2.42s/it, loss=3.3273, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [07:51<04:46,  2.67s/it, loss=3.3267, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [07:55<05:03,  2.86s/it, loss=3.3251, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:00<06:07,  3.50s/it, loss=3.3247, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:02<05:31,  3.19s/it, loss=3.3238, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:04<04:49,  2.81s/it, loss=3.3225, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:06<04:23,  2.59s/it, loss=3.3234, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:10<04:54,  2.91s/it, loss=3.3229, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:15<06:02,  3.63s/it, loss=3.3225, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:18<05:32,  3.36s/it, loss=3.3230, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:21<05:34,  3.41s/it, loss=3.3228, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:24<04:55,  3.05s/it, loss=3.3218, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:27<04:59,  3.12s/it, loss=3.3219, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:30<04:44,  3.00s/it, loss=3.3210, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [08:33<04:41,  2.99s/it, loss=3.3221, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [08:35<04:32,  2.93s/it, loss=3.3210, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [08:38<04:28,  2.92s/it, loss=3.3228, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [08:41<04:25,  2.92s/it, loss=3.3228, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [08:45<04:39,  3.11s/it, loss=3.3240, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [08:50<05:39,  3.82s/it, loss=3.3247, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [08:53<05:13,  3.56s/it, loss=3.3262, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [08:55<04:24,  3.04s/it, loss=3.3249, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [08:57<04:01,  2.81s/it, loss=3.3243, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:01<04:11,  2.96s/it, loss=3.3250, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:05<04:56,  3.53s/it, loss=3.3254, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:08<04:31,  3.28s/it, loss=3.3249, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:12<04:43,  3.46s/it, loss=3.3261, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:14<03:58,  2.95s/it, loss=3.3249, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:17<03:53,  2.92s/it, loss=3.3254, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:23<05:16,  4.01s/it, loss=3.3239, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [09:26<04:42,  3.62s/it, loss=3.3233, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [09:29<04:20,  3.39s/it, loss=3.3242, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [09:31<03:46,  2.98s/it, loss=3.3247, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [09:33<03:18,  2.65s/it, loss=3.3261, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [09:36<03:24,  2.76s/it, loss=3.3259, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [09:38<03:14,  2.66s/it, loss=3.3265, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [09:41<03:14,  2.70s/it, loss=3.3260, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [09:43<03:06,  2.63s/it, loss=3.3272, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [09:46<03:01,  2.59s/it, loss=3.3274, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [09:48<02:45,  2.39s/it, loss=3.3255, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [09:54<04:05,  3.61s/it, loss=3.3245, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [09:57<03:37,  3.25s/it, loss=3.3244, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [10:00<03:31,  3.20s/it, loss=3.3239, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:02<03:07,  2.88s/it, loss=3.3243, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:06<03:23,  3.17s/it, loss=3.3242, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:07<02:51,  2.73s/it, loss=3.3230, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:10<02:53,  2.80s/it, loss=3.3228, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:14<03:06,  3.06s/it, loss=3.3220, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [10:21<04:13,  4.22s/it, loss=3.3214, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [10:24<03:45,  3.83s/it, loss=3.3209, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [10:26<03:19,  3.44s/it, loss=3.3195, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [10:29<02:59,  3.14s/it, loss=3.3192, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [10:32<02:51,  3.06s/it, loss=3.3187, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [10:35<02:50,  3.10s/it, loss=3.3180, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [10:37<02:38,  2.94s/it, loss=3.3175, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [10:40<02:26,  2.76s/it, loss=3.3168, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [10:44<02:40,  3.09s/it, loss=3.3154, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [10:49<03:13,  3.79s/it, loss=3.3147, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [10:54<03:29,  4.19s/it, loss=3.3138, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [11:00<03:54,  4.78s/it, loss=3.3143, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:07<04:11,  5.25s/it, loss=3.3149, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:10<03:32,  4.52s/it, loss=3.3147, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:14<03:20,  4.36s/it, loss=3.3142, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:18<03:23,  4.52s/it, loss=3.3149, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [11:22<03:00,  4.11s/it, loss=3.3151, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [11:25<02:43,  3.81s/it, loss=3.3158, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [11:27<02:21,  3.37s/it, loss=3.3159, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [11:29<02:06,  3.08s/it, loss=3.3162, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [11:34<02:21,  3.53s/it, loss=3.3157, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [11:39<02:33,  3.94s/it, loss=3.3146, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [11:42<02:22,  3.76s/it, loss=3.3132, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [11:44<02:00,  3.25s/it, loss=3.3127, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [11:47<01:49,  3.03s/it, loss=3.3124, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [11:49<01:32,  2.65s/it, loss=3.3113, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [11:52<01:33,  2.75s/it, loss=3.3112, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [11:56<01:45,  3.20s/it, loss=3.3111, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [11:59<01:46,  3.34s/it, loss=3.3110, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [12:02<01:35,  3.09s/it, loss=3.3108, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [12:05<01:28,  2.95s/it, loss=3.3119, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [12:08<01:26,  3.00s/it, loss=3.3110, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [12:11<01:26,  3.09s/it, loss=3.3095, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [12:15<01:27,  3.25s/it, loss=3.3091, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [12:19<01:36,  3.71s/it, loss=3.3101, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [12:22<01:21,  3.25s/it, loss=3.3093, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [12:24<01:09,  2.91s/it, loss=3.3071, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [12:25<00:58,  2.52s/it, loss=3.3058, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [12:28<00:56,  2.56s/it, loss=3.3051, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [12:32<01:01,  2.95s/it, loss=3.3049, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [12:34<00:52,  2.60s/it, loss=3.3036, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [12:38<01:00,  3.17s/it, loss=3.3032, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [12:44<01:09,  3.87s/it, loss=3.3023, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [12:46<00:55,  3.28s/it, loss=3.3014, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [12:52<01:09,  4.36s/it, loss=3.3005, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [12:55<00:56,  3.79s/it, loss=3.3009, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [13:01<01:02,  4.47s/it, loss=3.3023, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [13:04<00:50,  3.92s/it, loss=3.3016, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [13:06<00:43,  3.59s/it, loss=3.3010, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [13:13<00:48,  4.37s/it, loss=3.3016, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [13:15<00:38,  3.82s/it, loss=3.3012, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [13:17<00:29,  3.33s/it, loss=3.3012, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [13:20<00:25,  3.21s/it, loss=3.3024, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [13:23<00:20,  2.99s/it, loss=3.3016, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [13:25<00:17,  2.90s/it, loss=3.3012, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [13:28<00:14,  2.91s/it, loss=3.3004, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [13:33<00:13,  3.35s/it, loss=3.2998, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [13:37<00:10,  3.64s/it, loss=3.3010, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [13:39<00:06,  3.26s/it, loss=3.3011, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [13:42<00:02,  2.99s/it, loss=3.3014, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [13:58<00:00,  3.36s/it, loss=3.3025, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:39,  7.07s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:08<01:52,  3.74s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:10<01:19,  2.74s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:11<01:04,  2.31s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:12<00:51,  1.92s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:14<00:43,  1.66s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:14<00:35,  1.40s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:15<00:29,  1.25s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:17<00:29,  1.28s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:18<00:24,  1.14s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:19<00:23,  1.12s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:20<00:24,  1.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:21<00:23,  1.23s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:22<00:20,  1.15s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:23<00:18,  1.11s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:24<00:16,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:25<00:16,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:26<00:14,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:28<00:16,  1.27s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:29<00:13,  1.16s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:30<00:11,  1.02s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:31<00:10,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:32<00:10,  1.13s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:33<00:09,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:34<00:07,  1.13s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:36<00:08,  1.42s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:37<00:06,  1.27s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:51<00:00,  1.60s/it]\n",
      "Saved checkpoint to checkpoints/model_epoch_9.pt\n",
      "Saved training results to training_results_20250511_183750.csv\n",
      "Train Loss: 3.3025\n",
      "Val Loss: 4.7082\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "\n",
      "Epoch 10/10\n",
      "Training:   0%|          | 0/250 [00:00<?, ?it/s]                               Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   0%|          | 1/250 [00:09<39:56,  9.63s/it, loss=3.1629, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 2/250 [00:14<27:42,  6.70s/it, loss=3.2885, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   1%|          | 3/250 [00:17<20:41,  5.03s/it, loss=3.2585, tokens/s=Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 4/250 [00:23<21:44,  5.30s/it, loss=3.2065, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 5/250 [00:24<16:38,  4.08s/it, loss=3.2074, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   2%|▏         | 6/250 [00:29<16:36,  4.09s/it, loss=3.2051, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 7/250 [00:31<13:54,  3.43s/it, loss=3.1809, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   3%|▎         | 8/250 [00:35<15:25,  3.83s/it, loss=3.1717, tokens/s=Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▎         | 9/250 [00:41<17:36,  4.38s/it, loss=3.1656, tokens/s=Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 10/250 [00:43<14:38,  3.66s/it, loss=3.1497, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   4%|▍         | 11/250 [00:47<14:45,  3.71s/it, loss=3.1392, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▍         | 12/250 [00:51<15:29,  3.90s/it, loss=3.1483, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   5%|▌         | 13/250 [00:56<16:28,  4.17s/it, loss=3.1527, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 14/250 [00:58<13:36,  3.46s/it, loss=3.1563, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▌         | 15/250 [00:59<11:24,  2.91s/it, loss=3.1326, tokens/sMax input_id: 31969\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   6%|▋         | 16/250 [01:01<10:05,  2.59s/it, loss=3.1293, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 17/250 [01:04<09:55,  2.56s/it, loss=3.1434, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   7%|▋         | 18/250 [01:06<09:16,  2.40s/it, loss=3.1412, tokens/sMax input_id: 31977\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 19/250 [01:12<13:14,  3.44s/it, loss=3.1408, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 20/250 [01:18<16:55,  4.42s/it, loss=3.1417, tokens/sMax input_id: 31978\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   8%|▊         | 21/250 [01:21<14:28,  3.79s/it, loss=3.1374, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 22/250 [01:24<13:51,  3.65s/it, loss=3.1549, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:   9%|▉         | 23/250 [01:26<12:09,  3.21s/it, loss=3.1496, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|▉         | 24/250 [01:30<12:24,  3.29s/it, loss=3.1478, tokens/sMax input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 25/250 [01:35<14:57,  3.99s/it, loss=3.1548, tokens/sMax input_id: 31978\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  10%|█         | 26/250 [01:42<18:28,  4.95s/it, loss=3.1651, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 27/250 [01:45<15:38,  4.21s/it, loss=3.1640, tokens/sMax input_id: 31970\n",
      "Max target_input: 31988\n",
      "Max label: 31988\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  11%|█         | 28/250 [01:51<17:35,  4.75s/it, loss=3.1796, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 29/250 [01:54<15:42,  4.26s/it, loss=3.1798, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 30/250 [01:57<14:18,  3.90s/it, loss=3.1829, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  12%|█▏        | 31/250 [02:01<13:58,  3.83s/it, loss=3.1889, tokens/sMax input_id: 31958\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 32/250 [02:08<17:34,  4.84s/it, loss=3.1842, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  13%|█▎        | 33/250 [02:11<15:26,  4.27s/it, loss=3.1867, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▎        | 34/250 [02:13<13:27,  3.74s/it, loss=3.1896, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 35/250 [02:19<15:42,  4.38s/it, loss=3.1889, tokens/sMax input_id: 31957\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  14%|█▍        | 36/250 [02:22<13:38,  3.83s/it, loss=3.1912, tokens/sMax input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▍        | 37/250 [02:25<12:52,  3.63s/it, loss=3.1916, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  15%|█▌        | 38/250 [02:27<11:17,  3.20s/it, loss=3.1912, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 39/250 [02:31<11:58,  3.40s/it, loss=3.1918, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▌        | 40/250 [02:34<11:03,  3.16s/it, loss=3.1882, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  16%|█▋        | 41/250 [02:37<11:35,  3.33s/it, loss=3.1835, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 42/250 [02:41<12:15,  3.54s/it, loss=3.1843, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  17%|█▋        | 43/250 [02:46<13:17,  3.85s/it, loss=3.1874, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 44/250 [02:48<11:40,  3.40s/it, loss=3.1825, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 45/250 [02:51<11:12,  3.28s/it, loss=3.1861, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  18%|█▊        | 46/250 [02:54<10:10,  2.99s/it, loss=3.1869, tokens/sMax input_id: 31963\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 47/250 [02:58<11:29,  3.40s/it, loss=3.1907, tokens/sMax input_id: 31969\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  19%|█▉        | 48/250 [03:04<13:56,  4.14s/it, loss=3.1867, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|█▉        | 49/250 [03:07<12:27,  3.72s/it, loss=3.1888, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 50/250 [03:10<12:02,  3.61s/it, loss=3.1935, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  20%|██        | 51/250 [03:16<14:17,  4.31s/it, loss=3.1993, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 52/250 [03:19<13:23,  4.06s/it, loss=3.1998, tokens/sMax input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  21%|██        | 53/250 [03:24<13:25,  4.09s/it, loss=3.1977, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 54/250 [03:30<15:22,  4.71s/it, loss=3.2017, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 55/250 [03:32<13:25,  4.13s/it, loss=3.2004, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  22%|██▏       | 56/250 [03:36<12:25,  3.84s/it, loss=3.1964, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 57/250 [03:38<10:46,  3.35s/it, loss=3.1933, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  23%|██▎       | 58/250 [03:44<13:15,  4.15s/it, loss=3.1956, tokens/sMax input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▎       | 59/250 [03:46<11:24,  3.58s/it, loss=3.1960, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 60/250 [03:50<11:32,  3.65s/it, loss=3.1947, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  24%|██▍       | 61/250 [03:53<11:11,  3.55s/it, loss=3.1939, tokens/sMax input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▍       | 62/250 [03:56<10:10,  3.25s/it, loss=3.1887, tokens/sMax input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  25%|██▌       | 63/250 [03:58<09:08,  2.93s/it, loss=3.1842, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 64/250 [04:05<13:10,  4.25s/it, loss=3.1849, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▌       | 65/250 [04:08<11:41,  3.79s/it, loss=3.1787, tokens/sMax input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  26%|██▋       | 66/250 [04:12<11:36,  3.78s/it, loss=3.1759, tokens/sMax input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 67/250 [04:14<09:59,  3.27s/it, loss=3.1764, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  27%|██▋       | 68/250 [04:16<09:06,  3.00s/it, loss=3.1742, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 69/250 [04:20<09:29,  3.15s/it, loss=3.1748, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 70/250 [04:22<08:44,  2.91s/it, loss=3.1770, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  28%|██▊       | 71/250 [04:27<10:05,  3.39s/it, loss=3.1764, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 72/250 [04:29<09:06,  3.07s/it, loss=3.1761, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  29%|██▉       | 73/250 [04:31<08:08,  2.76s/it, loss=3.1753, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|██▉       | 74/250 [04:34<07:58,  2.72s/it, loss=3.1778, tokens/sMax input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 75/250 [04:35<07:13,  2.48s/it, loss=3.1743, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  30%|███       | 76/250 [04:39<07:58,  2.75s/it, loss=3.1745, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 77/250 [04:41<07:49,  2.71s/it, loss=3.1757, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  31%|███       | 78/250 [04:46<09:28,  3.31s/it, loss=3.1778, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 79/250 [04:50<09:31,  3.34s/it, loss=3.1802, tokens/sMax input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 80/250 [04:54<10:01,  3.54s/it, loss=3.1811, tokens/sMax input_id: 31959\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  32%|███▏      | 81/250 [04:56<08:52,  3.15s/it, loss=3.1787, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 82/250 [05:00<09:29,  3.39s/it, loss=3.1762, tokens/sMax input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  33%|███▎      | 83/250 [05:03<09:13,  3.31s/it, loss=3.1762, tokens/sMax input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▎      | 84/250 [05:09<11:24,  4.12s/it, loss=3.1745, tokens/sMax input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 85/250 [05:12<10:21,  3.77s/it, loss=3.1732, tokens/sMax input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  34%|███▍      | 86/250 [05:14<09:16,  3.40s/it, loss=3.1685, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▍      | 87/250 [05:16<07:55,  2.92s/it, loss=3.1665, tokens/sMax input_id: 31969\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  35%|███▌      | 88/250 [05:19<07:27,  2.76s/it, loss=3.1652, tokens/sMax input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 89/250 [05:25<10:40,  3.98s/it, loss=3.1692, tokens/sMax input_id: 31958\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▌      | 90/250 [05:28<09:38,  3.62s/it, loss=3.1680, tokens/sMax input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  36%|███▋      | 91/250 [05:32<09:50,  3.71s/it, loss=3.1677, tokens/sMax input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 92/250 [05:35<09:31,  3.61s/it, loss=3.1660, tokens/sMax input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  37%|███▋      | 93/250 [05:39<09:08,  3.50s/it, loss=3.1665, tokens/sMax input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 94/250 [05:40<07:37,  2.93s/it, loss=3.1616, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 95/250 [05:43<07:09,  2.77s/it, loss=3.1638, tokens/sMax input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  38%|███▊      | 96/250 [05:46<07:09,  2.79s/it, loss=3.1594, tokens/sMax input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 97/250 [05:48<06:36,  2.59s/it, loss=3.1578, tokens/sMax input_id: 31968\n",
      "Max target_input: 31967\n",
      "Max label: 31967\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  39%|███▉      | 98/250 [05:52<08:11,  3.23s/it, loss=3.1617, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|███▉      | 99/250 [05:55<07:35,  3.02s/it, loss=3.1596, tokens/sMax input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 100/250 [06:02<10:34,  4.23s/it, loss=3.1625, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  40%|████      | 101/250 [06:05<09:29,  3.82s/it, loss=3.1599, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 102/250 [06:07<08:26,  3.42s/it, loss=3.1562, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  41%|████      | 103/250 [06:10<08:05,  3.30s/it, loss=3.1535, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 104/250 [06:13<07:48,  3.21s/it, loss=3.1549, tokens/Max input_id: 31938\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 105/250 [06:16<07:04,  2.93s/it, loss=3.1546, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  42%|████▏     | 106/250 [06:21<08:45,  3.65s/it, loss=3.1526, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 107/250 [06:24<08:16,  3.47s/it, loss=3.1521, tokens/Max input_id: 31963\n",
      "Max target_input: 31998\n",
      "Max label: 31998\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  43%|████▎     | 108/250 [06:26<07:29,  3.17s/it, loss=3.1529, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▎     | 109/250 [06:29<06:49,  2.91s/it, loss=3.1545, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 110/250 [06:33<07:43,  3.31s/it, loss=3.1515, tokens/Max input_id: 31970\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  44%|████▍     | 111/250 [06:36<07:19,  3.16s/it, loss=3.1526, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▍     | 112/250 [06:39<07:22,  3.21s/it, loss=3.1512, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  45%|████▌     | 113/250 [06:46<10:00,  4.38s/it, loss=3.1529, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 114/250 [06:49<08:32,  3.77s/it, loss=3.1527, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▌     | 115/250 [06:51<07:19,  3.26s/it, loss=3.1532, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  46%|████▋     | 116/250 [06:54<07:12,  3.23s/it, loss=3.1524, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 117/250 [07:02<10:07,  4.57s/it, loss=3.1531, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  47%|████▋     | 118/250 [07:08<11:20,  5.16s/it, loss=3.1543, tokens/Max input_id: 31959\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 119/250 [07:11<09:55,  4.55s/it, loss=3.1570, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 120/250 [07:13<08:02,  3.71s/it, loss=3.1563, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  48%|████▊     | 121/250 [07:15<07:05,  3.30s/it, loss=3.1584, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 122/250 [07:18<06:34,  3.08s/it, loss=3.1608, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  49%|████▉     | 123/250 [07:23<07:35,  3.59s/it, loss=3.1602, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|████▉     | 124/250 [07:25<06:28,  3.08s/it, loss=3.1579, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 125/250 [07:28<06:39,  3.19s/it, loss=3.1577, tokens/Max input_id: 31958\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  50%|█████     | 126/250 [07:31<06:21,  3.07s/it, loss=3.1585, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 127/250 [07:33<05:32,  2.71s/it, loss=3.1557, tokens/Max input_id: 31959\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  51%|█████     | 128/250 [07:36<06:04,  2.99s/it, loss=3.1552, tokens/Max input_id: 31970\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 129/250 [07:38<05:28,  2.71s/it, loss=3.1559, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 130/250 [07:41<05:13,  2.62s/it, loss=3.1561, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  52%|█████▏    | 131/250 [07:43<05:14,  2.64s/it, loss=3.1574, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 132/250 [07:45<04:44,  2.41s/it, loss=3.1550, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  53%|█████▎    | 133/250 [07:47<04:20,  2.22s/it, loss=3.1532, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▎    | 134/250 [07:54<06:49,  3.53s/it, loss=3.1521, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 135/250 [07:57<06:32,  3.41s/it, loss=3.1517, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  54%|█████▍    | 136/250 [07:59<05:56,  3.13s/it, loss=3.1508, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▍    | 137/250 [08:01<05:18,  2.82s/it, loss=3.1498, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  55%|█████▌    | 138/250 [08:04<05:06,  2.74s/it, loss=3.1473, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 139/250 [08:06<04:41,  2.54s/it, loss=3.1457, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▌    | 140/250 [08:09<04:54,  2.68s/it, loss=3.1452, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  56%|█████▋    | 141/250 [08:12<05:15,  2.89s/it, loss=3.1436, tokens/Max input_id: 31969\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 142/250 [08:14<04:41,  2.61s/it, loss=3.1425, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  57%|█████▋    | 143/250 [08:19<05:31,  3.09s/it, loss=3.1413, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 144/250 [08:21<05:14,  2.97s/it, loss=3.1399, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 145/250 [08:27<06:54,  3.94s/it, loss=3.1398, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  58%|█████▊    | 146/250 [08:30<06:00,  3.47s/it, loss=3.1397, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 147/250 [08:32<05:09,  3.00s/it, loss=3.1379, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  59%|█████▉    | 148/250 [08:34<04:37,  2.72s/it, loss=3.1389, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|█████▉    | 149/250 [08:37<04:34,  2.72s/it, loss=3.1377, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 150/250 [08:41<05:28,  3.28s/it, loss=3.1379, tokens/Max input_id: 31977\n",
      "Max target_input: 31990\n",
      "Max label: 31990\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  60%|██████    | 151/250 [08:45<05:34,  3.38s/it, loss=3.1382, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 152/250 [08:50<06:31,  4.00s/it, loss=3.1384, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  61%|██████    | 153/250 [08:52<05:36,  3.47s/it, loss=3.1380, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 154/250 [08:57<06:02,  3.78s/it, loss=3.1384, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 155/250 [08:59<05:06,  3.23s/it, loss=3.1372, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  62%|██████▏   | 156/250 [09:02<05:12,  3.32s/it, loss=3.1378, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 157/250 [09:06<05:06,  3.29s/it, loss=3.1366, tokens/Max input_id: 31963\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  63%|██████▎   | 158/250 [09:09<05:06,  3.33s/it, loss=3.1385, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▎   | 159/250 [09:13<05:07,  3.38s/it, loss=3.1382, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 160/250 [09:15<04:50,  3.23s/it, loss=3.1391, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  64%|██████▍   | 161/250 [09:21<05:46,  3.89s/it, loss=3.1397, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▍   | 162/250 [09:26<06:10,  4.21s/it, loss=3.1414, tokens/Max input_id: 31963\n",
      "Max target_input: 31977\n",
      "Max label: 31977\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  65%|██████▌   | 163/250 [09:28<05:04,  3.50s/it, loss=3.1403, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 164/250 [09:30<04:29,  3.13s/it, loss=3.1398, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▌   | 165/250 [09:35<05:03,  3.57s/it, loss=3.1405, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  66%|██████▋   | 166/250 [09:38<05:00,  3.57s/it, loss=3.1404, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 167/250 [09:42<04:53,  3.53s/it, loss=3.1398, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  67%|██████▋   | 168/250 [09:46<05:15,  3.85s/it, loss=3.1409, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 169/250 [09:48<04:21,  3.22s/it, loss=3.1396, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 170/250 [09:52<04:32,  3.41s/it, loss=3.1397, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  68%|██████▊   | 171/250 [09:59<05:50,  4.43s/it, loss=3.1385, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 172/250 [10:01<05:05,  3.92s/it, loss=3.1379, tokens/Max input_id: 31959\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  69%|██████▉   | 173/250 [10:04<04:38,  3.61s/it, loss=3.1382, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|██████▉   | 174/250 [10:06<03:59,  3.15s/it, loss=3.1386, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 175/250 [10:08<03:27,  2.76s/it, loss=3.1400, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  70%|███████   | 176/250 [10:12<03:48,  3.09s/it, loss=3.1396, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 177/250 [10:14<03:31,  2.89s/it, loss=3.1406, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  71%|███████   | 178/250 [10:17<03:28,  2.90s/it, loss=3.1397, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 179/250 [10:20<03:19,  2.81s/it, loss=3.1406, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 180/250 [10:22<03:09,  2.70s/it, loss=3.1412, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  72%|███████▏  | 181/250 [10:24<02:51,  2.48s/it, loss=3.1393, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 182/250 [10:31<04:22,  3.86s/it, loss=3.1383, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  73%|███████▎  | 183/250 [10:35<04:12,  3.76s/it, loss=3.1384, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▎  | 184/250 [10:37<03:39,  3.32s/it, loss=3.1376, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 185/250 [10:40<03:15,  3.01s/it, loss=3.1378, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  74%|███████▍  | 186/250 [10:44<03:41,  3.46s/it, loss=3.1375, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▍  | 187/250 [10:46<03:04,  2.93s/it, loss=3.1357, tokens/Max input_id: 31977\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  75%|███████▌  | 188/250 [10:50<03:23,  3.28s/it, loss=3.1355, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 189/250 [10:53<03:21,  3.31s/it, loss=3.1349, tokens/Max input_id: 31964\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▌  | 190/250 [11:00<04:27,  4.46s/it, loss=3.1345, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  76%|███████▋  | 191/250 [11:03<03:54,  3.97s/it, loss=3.1339, tokens/Max input_id: 31970\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 192/250 [11:07<03:40,  3.81s/it, loss=3.1324, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  77%|███████▋  | 193/250 [11:09<03:17,  3.47s/it, loss=3.1320, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 194/250 [11:14<03:33,  3.81s/it, loss=3.1318, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 195/250 [11:16<03:04,  3.35s/it, loss=3.1313, tokens/Max input_id: 31970\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  78%|███████▊  | 196/250 [11:19<02:49,  3.13s/it, loss=3.1309, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 197/250 [11:21<02:33,  2.89s/it, loss=3.1299, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  79%|███████▉  | 198/250 [11:26<03:06,  3.59s/it, loss=3.1285, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|███████▉  | 199/250 [11:31<03:21,  3.96s/it, loss=3.1277, tokens/Max input_id: 31977\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 200/250 [11:36<03:34,  4.28s/it, loss=3.1264, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  80%|████████  | 201/250 [11:42<03:59,  4.89s/it, loss=3.1270, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 202/250 [11:50<04:37,  5.78s/it, loss=3.1278, tokens/Max input_id: 31963\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  81%|████████  | 203/250 [11:53<03:50,  4.89s/it, loss=3.1274, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 204/250 [11:56<03:11,  4.17s/it, loss=3.1268, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 205/250 [11:59<03:02,  4.05s/it, loss=3.1274, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  82%|████████▏ | 206/250 [12:02<02:44,  3.73s/it, loss=3.1275, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 207/250 [12:08<03:07,  4.35s/it, loss=3.1287, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  83%|████████▎ | 208/250 [12:11<02:38,  3.77s/it, loss=3.1289, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▎ | 209/250 [12:13<02:21,  3.46s/it, loss=3.1296, tokens/Max input_id: 31958\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 210/250 [12:17<02:26,  3.66s/it, loss=3.1288, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  84%|████████▍ | 211/250 [12:22<02:31,  3.90s/it, loss=3.1276, tokens/Max input_id: 31958\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▍ | 212/250 [12:27<02:44,  4.34s/it, loss=3.1263, tokens/Max input_id: 31977\n",
      "Max target_input: 31953\n",
      "Max label: 31953\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  85%|████████▌ | 213/250 [12:29<02:16,  3.68s/it, loss=3.1259, tokens/Max input_id: 31977\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 214/250 [12:32<02:01,  3.36s/it, loss=3.1259, tokens/Max input_id: 31963\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▌ | 215/250 [12:34<01:40,  2.87s/it, loss=3.1248, tokens/Max input_id: 31970\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  86%|████████▋ | 216/250 [12:36<01:30,  2.66s/it, loss=3.1250, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 217/250 [12:41<01:54,  3.48s/it, loss=3.1250, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  87%|████████▋ | 218/250 [12:45<01:54,  3.58s/it, loss=3.1249, tokens/Max input_id: 31959\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 219/250 [12:48<01:40,  3.25s/it, loss=3.1245, tokens/Max input_id: 31957\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 220/250 [12:50<01:31,  3.04s/it, loss=3.1257, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  88%|████████▊ | 221/250 [12:54<01:31,  3.16s/it, loss=3.1249, tokens/Max input_id: 31977\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 222/250 [12:57<01:29,  3.20s/it, loss=3.1234, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  89%|████████▉ | 223/250 [13:01<01:31,  3.37s/it, loss=3.1233, tokens/Max input_id: 31970\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|████████▉ | 224/250 [13:06<01:39,  3.82s/it, loss=3.1249, tokens/Max input_id: 31963\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 225/250 [13:08<01:22,  3.31s/it, loss=3.1241, tokens/Max input_id: 31977\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  90%|█████████ | 226/250 [13:10<01:10,  2.94s/it, loss=3.1219, tokens/Max input_id: 31977\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 227/250 [13:12<00:59,  2.59s/it, loss=3.1207, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  91%|█████████ | 228/250 [13:14<00:58,  2.64s/it, loss=3.1201, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 229/250 [13:18<01:03,  3.03s/it, loss=3.1199, tokens/Max input_id: 31963\n",
      "Max target_input: 31957\n",
      "Max label: 31957\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 230/250 [13:20<00:53,  2.67s/it, loss=3.1185, tokens/Max input_id: 31963\n",
      "Max target_input: 31959\n",
      "Max label: 31959\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  92%|█████████▏| 231/250 [13:25<01:01,  3.24s/it, loss=3.1184, tokens/Max input_id: 31970\n",
      "Max target_input: 31962\n",
      "Max label: 31962\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 232/250 [13:32<01:19,  4.43s/it, loss=3.1181, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  93%|█████████▎| 233/250 [13:34<01:02,  3.66s/it, loss=3.1172, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▎| 234/250 [13:39<01:08,  4.26s/it, loss=3.1164, tokens/Max input_id: 31958\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 235/250 [13:42<00:55,  3.71s/it, loss=3.1168, tokens/Max input_id: 31978\n",
      "Max target_input: 31978\n",
      "Max label: 31978\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  94%|█████████▍| 236/250 [13:48<01:02,  4.43s/it, loss=3.1184, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▍| 237/250 [13:51<00:53,  4.15s/it, loss=3.1178, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  95%|█████████▌| 238/250 [13:54<00:44,  3.73s/it, loss=3.1171, tokens/Max input_id: 31970\n",
      "Max target_input: 31969\n",
      "Max label: 31969\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 239/250 [13:59<00:44,  4.01s/it, loss=3.1179, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▌| 240/250 [14:01<00:35,  3.54s/it, loss=3.1177, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  96%|█████████▋| 241/250 [14:03<00:28,  3.14s/it, loss=3.1175, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 242/250 [14:08<00:29,  3.63s/it, loss=3.1187, tokens/Max input_id: 31963\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  97%|█████████▋| 243/250 [14:11<00:23,  3.33s/it, loss=3.1179, tokens/Max input_id: 31963\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 244/250 [14:14<00:19,  3.31s/it, loss=3.1172, tokens/Max input_id: 31963\n",
      "Max target_input: 31975\n",
      "Max label: 31975\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 245/250 [14:16<00:14,  2.98s/it, loss=3.1167, tokens/Max input_id: 31970\n",
      "Max target_input: 31958\n",
      "Max label: 31958\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Training:  98%|█████████▊| 246/250 [14:20<00:12,  3.20s/it, loss=3.1160, tokens/Max input_id: 31963\n",
      "Max target_input: 31986\n",
      "Max label: 31986\n",
      "Training:  99%|█████████▉| 247/250 [14:26<00:11,  3.92s/it, loss=3.1175, tokens/Max input_id: 31963\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training:  99%|█████████▉| 248/250 [14:28<00:07,  3.55s/it, loss=3.1175, tokens/Max input_id: 31970\n",
      "Max target_input: 31970\n",
      "Max label: 31970\n",
      "Training: 100%|█████████▉| 249/250 [14:31<00:03,  3.19s/it, loss=3.1178, tokens/Max input_id: 31970\n",
      "Max target_input: 31963\n",
      "Max label: 31963\n",
      "Training: 100%|██████████| 250/250 [14:48<00:00,  3.55s/it, loss=3.1191, tokens/\n",
      "Evaluating:   0%|                                        | 0/32 [00:00<?, ?it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size:Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      " 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   3%|█                               | 1/32 [00:07<03:57,  7.68s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   6%|██                              | 2/32 [00:09<01:59,  3.98s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:   9%|███                             | 3/32 [00:10<01:20,  2.78s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  12%|████                            | 4/32 [00:11<00:56,  2.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  16%|█████                           | 5/32 [00:12<00:46,  1.73s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  19%|██████                          | 6/32 [00:14<00:46,  1.78s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  22%|███████                         | 7/32 [00:15<00:37,  1.48s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  25%|████████                        | 8/32 [00:16<00:31,  1.32s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  28%|█████████                       | 9/32 [00:17<00:30,  1.34s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  31%|█████████▋                     | 10/32 [00:18<00:25,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  34%|██████████▋                    | 11/32 [00:19<00:24,  1.16s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  38%|███████████▋                   | 12/32 [00:20<00:20,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  41%|████████████▌                  | 13/32 [00:22<00:24,  1.29s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  44%|█████████████▌                 | 14/32 [00:22<00:20,  1.14s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  47%|██████████████▌                | 15/32 [00:23<00:18,  1.09s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  50%|███████████████▌               | 16/32 [00:24<00:16,  1.01s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  53%|████████████████▍              | 17/32 [00:25<00:15,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  56%|█████████████████▍             | 18/32 [00:26<00:13,  1.00it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  59%|██████████████████▍            | 19/32 [00:27<00:12,  1.00it/s]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  62%|███████████████████▍           | 20/32 [00:29<00:14,  1.18s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  66%|████████████████████▎          | 21/32 [00:30<00:11,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  69%|█████████████████████▎         | 22/32 [00:31<00:10,  1.03s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  72%|██████████████████████▎        | 23/32 [00:32<00:09,  1.07s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  75%|███████████████████████▎       | 24/32 [00:33<00:08,  1.12s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  78%|████████████████████████▏      | 25/32 [00:34<00:07,  1.08s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  81%|█████████████████████████▏     | 26/32 [00:36<00:08,  1.37s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating:  84%|██████████████████████████▏    | 27/32 [00:37<00:06,  1.22s/it]Tokenizer vocab size: 32000\n",
      "Special tokens:\n",
      "  [PAD] ID: 0\n",
      "  [UNK] ID: 1\n",
      "  [BOS] ID: 2\n",
      "  [EOS] ID: 3\n",
      "  [DE] ID: 4\n",
      "  [EN] ID: 5\n",
      "Evaluating: 100%|███████████████████████████████| 32/32 [00:50<00:00,  1.58s/it]\n",
      "Removed old checkpoint: model_epoch_10.pt\n",
      "Saved checkpoint to checkpoints/model_epoch_10.pt\n",
      "Saved training results to training_results_20250511_185336.csv\n",
      "Train Loss: 3.1191\n",
      "Val Loss: 4.7265\n",
      "Learning Rate: 0.000100\n",
      "[DE] ID: 4\n",
      "[EN] ID: 5\n",
      "\n",
      "Sample translation:\n",
      "German: Hallo, wie geht es dir?\n",
      "English: ?\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-en-translator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
